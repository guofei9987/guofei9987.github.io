
# hive
## 下面这个不推荐了
原因：SQLcontext不支持udf

### Hive to Spark
```py
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("test").enableHiveSupport().getOrCreate()
df_data = spark.sql('select ...') #返回spark下的DataFrame类型

# 另一种做法（更推荐这种）
from pyspark.sql import HiveContext
hiveCtx=HiveContext(sc)
df_data=hiveCtx.sql('select ...')
```
### Spark to Hive
```py
df_data.toDF('col_name1','col_name2').registerTempTable("table3")
spark.sql("create table tmp.tmp_test_pyspark2hive as select * from table3")
# table3可以作为一个表用了，因此还有众多可以写sql的用法
```

## Spark与pandas的交互
### spark to pandas

```py
df_data.toPandas() #返回DataFrame类型
```
### pandas to spark
```py
spark.createDataFrame(<pd.DataFrame>)
```


-------------------------------------------



# docker
## 其它
上面列出的是一些linux风格或git风格的命令，其中一些命令也有其它表达形式，虽然不太好记  

```bash
# 列出正在运行的容器
docker ps

# 列出所有容器（包括Exited）
docker ps -a

# 删除指定镜像（删除镜像前须先停止并删除容器）
docker rmi <image id>


# 删除指定容器
docker rm <container id>
# 删除所有容器
docker rm $(docker ps -aq)
# 停止并删除容器
docker stop $(docker ps -q) & docker rm $(docker ps -aq)

```

------------------------------------
win7版本的安装讲解  
https://blog.csdn.net/u013810234/article/details/78483055


docs.docker.com/engine/userguide
---------------------------------

## 案例
docker jekyll

挂载目录
- win7比较麻烦  
step1：先在 virtual Box 中“共享文件夹”下配置挂载  
step2：重启
step3：  
```bash
docker run --rm --label=jekyll --volume /abc:/abc -it -p 127.0.0.1:4006:4000 jekyll/jekyll bash
```


-------------------------------------------------


# hadoop

HDFS的优点
- 高容错性（数据多副本，自动恢复）
- 适合批处理
    - 移动计算而非数据
    - 数据位置暴露给计算框架
- 适合大数据处理
    - GB，TB，PB级数据
    - 百万规模以上
    - 10K+ 节点
- 可构建在廉价机器上（容错机制）


HDFS的缺点
- 延迟高
- 小文件存储
    - 占用 NameNode 大量内存
    - 寻道时间超过读取时间
- 并发写入，文件随机修改
    - 一个文件只能有一个 writer
    - 仅支持 append（2.0 有这个功能，但生产环境一般不开放）


--------------------------------------------------


HDFS 数据存储单元（block）
- 每个 block 默认3个副本（例如，一个服务器断电，会自动复制一个，通电后会一直保持4个）
- block 默认64M
- 所占磁盘空间按照实际来
