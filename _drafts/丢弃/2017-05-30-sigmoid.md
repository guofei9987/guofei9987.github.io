---
layout: post
title: 神经网络中的若干问题
categories:
tags: 
keywords:
description:
order: 450
---

## sigmoid作为激活函数的原因
### 从exponential family角度看
关于exponential family, 参见我的另一篇文章<a href='/2017/05/26/distribution1.html#title12'>常见统计分布(2).</a>  

Bernoulli distribution的概率分布函数可以写为：  
$f(x)=p^x(1-p)^{1-x}$  
进一步写为，  
$f(x)=exp[ln\dfrac{p}{1-p} x+ln(1-p)]$  

比较exponential family的定义：  
$P(y;\eta)=b(y)exp(\eta^T T(y)-a(\eta))$    

得到,
$\eta=\dfrac{p}{1-p}$


注：从normal distribution推导exponential family，对应的是线性回归。

### 从logistic regression角度看
用sigmoid function后，logistic regression的参数，MLE方法等价于OLS方法  

### 从BP神经网络角度看
误差反向传播过程中，sigmoid function有以下好处：
- 光滑：处处可导，导数连续
- 压缩：把R压缩到[-1,1],并且接近0更敏感。这是BP的需求。
- 导数容易求。y'=y(1-y)$.
误差反向传播算法中，大大减少了运算量（误差反向传播的详细介绍在我的博客中(www.guofei.site)）


## 交叉熵作为 Cost Function 的原因
残差平方和作为 Cost Function ，误差前向传播算法中，最后一项开始便有$\sigma'(v)$这一项，神经元的输入值较为极端时，$\sigma'(v)=\sigma(v) (1-\sigma(v))$ 接近0.  
用交叉熵作为 Cost Function，最后一层不存在这个问题  
公式自己推导，很简单。  


## softmax 作为最后一层的原因
1. 非负、和总是1，符合对概率的定义  
2. softmax regression 的二元情况与 logistics regression 等价
3. 与最大熵模型等价


## 过拟合的应对
### 1. 增加数据量往往可以有效降低过拟合的可能性
效果显著，但成本巨大
### 2. 人为扩展训练数据。  
第一条说了，增加数据量往往可以有效降低过拟合的可能性，但获取真实数据的成本往往很高，这里考虑在原数据上进行一些扩展。  
例如，MNIST数据库中，将图像进行少量旋转。

### 3. validation

看一些资料，感觉与 early stopping 一样。都是如果发现在 validation datasets 上误差连续上升，就停止迭代。

### 4. L1, L2 规范化


$C=C_0+\dfrac{\lambda}{2n}\sum w^2$（其中n是数据集的大小）  
偏导数就变成这种形式：  
1. $\dfrac{\partial C}{\partial w}=\dfrac{\partial C_0}{\partial w}+\dfrac{\lambda}{n}w$
2. $\dfrac{\partial C}{\partial b}=\dfrac{\partial C_0}{\partial b}$


训练算法变成这样：
1. $w\to w-\eta\dfrac{\partial C_0}{\partial b}-\dfrac{\eta\lambda}{n}w$
2. $b\to b-\eta\dfrac{\partial C_0}{\partial b}$


对于L1正则化，$\dfrac{\partial C}{\partial w}=\dfrac{\partial C_0}{\partial w}+\dfrac{\lambda}{n}  \mathcal{N}w$


以下引用 [Michael Nielsen](http://michaelnielsen.org/) 的说法  
规范化能够减轻过拟合，但背后的原因还不得而知。通常认为是因为小的权重意味着更低的复杂性。  
例如，一个线性模型和一个9阶多项式都可以回归一组点，有两种可能：
1. 9次多项式是正确的
2. 线性模型是正确的，噪音是因为存在测量误差


科学中，很难说明哪种原则正确，“奥卡姆剃刀”也不是一个一般的科学原理。例如，爱因斯坦的理论更能预测天体的微小偏差，但复杂很多。  
归根结底，你可以把规范化看成某种整合的技术，尽管其效果不错，但我们并没有一套完整的理解，仅仅当作不完备的启发规则或经验。规范化是一种帮助神经网络泛化的魔法，但不会带来原理上理解的指导。


### 5. Dropout
随机、临时关闭某些神经元。实际预测时，全部神经元激活，为了补偿这个，把权重按比例缩小。  
Dropout 类似于某种投票机制，有些类似训练不同的神经网络  



## 权重初始化
### 1/n
举例说明，某个3层神经元，输入层有1000个神经元，其中500个输出1,500个输出0  
用$N(0,1)$初始化隐含层的 weight 和 bias  
那么，对于某个隐含层的神经元，其输入是$\sum w_i^{l-1} v_i^{l-1}+b \sim N(0,501)$  
这是一个非常宽的正态分布，导致其值较大和较小的概率都不低，这样会导致训练缓慢。  


可以用$w\sim N(0,1/n),b\sim N(0,1)$来初始化 weight 和 bias，这样某个隐含层的输入是$\sum w_i^{l-1} v_i^{l-1}+b \sim N(0,1.5)$  


实际上，两种方案训练很多次，准确率趋向于一致

### 非0
初始化时，不能用全0初始化，因为这样的话，每一层内部的神经元都一样，无论迭代多少次，值也都一样，也就失去了神经网络的意义。  

实际上，可以设定w=0.01*np.random.randn,b=0  
当然，w不能太大，容易落在sigmoid函数的尾部，导致迭代很慢。  

## batch_size
理论上，只要权重更新足够频繁，就可以训练到最优，这样看来在线学习（batch_size=1）似乎更好。  
但是，
1. 神经网络每 batch_size 个样本，更新一次权重。在线学习更新太过频繁。
2. 现有的硬件支持矩阵运算，所以 batch_size 不为1时，每多一个样本，花费时间并不太增多。
3. batch_size 是一个相对独立的超参数（独立于网络整体架构之外）


## momentum
用 Hessian 矩阵做最优化有巨大的优势，关于 Hessian 矩阵，出门左转[非线性无约束最优化-牛顿法](http://www.guofei.site/2018/05/26/nonlinearprogramming.html#title6)  
问题是 Hessian 矩阵太大了，加入有n个权重，Hessian矩阵就是$n\times n$大小的  
作为改进，使用 momentum  


momentum 从物理中引入了两个概念，
1. 速度，梯度类似力，只改变加速度
2. 摩擦力，用来逐步减少速度


$v\to v'=\mu v-\eta \nabla C$  
$w\to w'=w+v'$  
$1-\mu$是摩擦力，$0\leq\mu\leq1$  


其它最优化算法：共和梯度法、BFGS方法(limited memory BFGS,L-BFGS)

## why deep?
Informally:Thera are functions you can compute with a "small" L-layer deep neural network that shallower networks require exponentially more hidden units to comput.  

这里有一个比喻性的说明：  
你的真实函数是$y=X_1 \mathbf{XOR} X_2 \mathbf{XOR} X_3 \mathbf{XOR} ... \mathbf{XOR} X_n$  
- 深度方案：类似一个二叉树，深度为$\log_2 n+1$，节点个数为$2n-1$（可以自己画一下）  
- 浅层方案：如果限定最多2层，那么hidden layer 需要 $2^{n-1}$ 个节点。


## 神经网络可以拟合任意函数

神经网络可以拟合任意函数，意味着神经网络有某种普遍性。  
例如，你可以把图像识别看成是某个函数，把翻译看成是某个函数。  

这里做一些说明，而非证明。  
1. 拟合指的是某种 **近似**，通过增加隐藏神经元的数目，提高精度  
$\forall \varepsilon>0$（$\varepsilon$是精度）,可以使用足够多的神经元，使得神经元的输出$g(x)$满足$\lvert g(x)-f(x)\rvert <\varepsilon$,  
2. 对于跳跃函数不能拟合，这是因为神经网络的输出必然是连续的，但是仍然可以用连续函数去近似拟合。


具体过程见于[神经网络与深度学习-chapter4](http://neuralnetworksanddeeplearning.com/chap4.html)

### 1. 分析单输入单输出
1. 单个sigmoid函数可以模拟阶跃函数
2. 多个阶跃函数叠加，可以模拟 if-else 结构
3. 两个阶跃函数可以模拟一个凸起（相反的权重，下一层求和）
4. 多个凸起


### 2. 分析多输入多输出
以2个输入为例，类似地，可以组合一个“塔型凸起”。  
多个凸起可以近似任意函数


### 讨论激活函数
把激活函数改成与sigmoid相似、但中间有很多波动，整个推理仍然成立（仍然可以拟合任意函数）  
思考一下激活函数改成 ReLU 的情况，这种情况仍然可以拟合任意函数  
思考一下激活函数改成线性的情况，这种情况不可以拟合任意函数  
