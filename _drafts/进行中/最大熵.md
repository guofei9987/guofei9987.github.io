
知识准备
概率测度
条件概率
Lagrange法
信息熵


## 简介
Multinomial logistic regression is known by a variety of other names, including polytomous LR, multiclass LR, **softmax regression** , multinomial logit, **maximum entropy** (MaxEnt) classifier, conditional maximum entropy model.[^wikipedia]

## 模型

给定数据集$$\Gamma=\{ (x_1,y_1),(x_2,y_2),...(x_N,y_N)\}$$  
$x_i \in \mathcal{X}, y_i \in \mathcal{Y}$  
$(x_i,  y_i) \in \mathcal{X, Y}$  

目标：分类器  

按照最大熵原理，应当优先找到满足所有的已知约束。  
如何提取这些约束呢？  
思路是：从数据集$\Gamma$中提取特征。当然要求这些特征在$\Gamma$熵的关于经验分布$\tilde p (x,y)$的数学期望，等于模型中关于$p(x,y)$的数学期望。  

### feature function

$$f(x,y)=\left \{ \begin{array}{ccc}
1&x,y\space subject\space to \space some\space fact\\
0&other wise\end{array}\right.$$

### 约束

要求这些特征在$\Gamma$熵的关于经验分布$\tilde p (x,y)$的数学期望，等于模型中关于$p(x,y)$的数学期望。
也就是说$E_{\tilde p}(f)=E_p(f)$  

当然，可以有多个feature function，  
$E_{\tilde p}(f_i)=E_p(f_i),i=1,2,...n$  

这两个期望怎么计算呢？  
$E_{\tilde p}(f_i)=\sum\limits_{x,y}\tilde p(x,y)f_i(x,y)$  
$E_p(f_i)=\sum\limits_{x,y}p(x,y)f_i(x,y)$  

经验概率怎么计算呢？
就是我们通常的计算方法:  
$\tilde p(x,y)=\dfrac{count(x,y)}{N}$  
$\tilde p(x)=\dfrac{count(x)}{N}$  

所以：  
$\sum\limits_{x,y}\tilde p(x,y)f_i(x,y)=\sum\limits_{x,y}p(x,y)f_i(x,y)$  
$p(x,y)$是未知的，而建模的目标是生成$p(y\mid x)$  
因此，只好使用一个 **近似**  $p(x)\thickapprox \tilde p(x)$  

最后，约束变为：  
$\sum\limits_{x,y}\tilde p(x,y)f_i(x,y)=\sum\limits_{x,y}\tilde p(x)p(x \mid y)f_i(x,y)$  

### 最大熵

$H(p(y\mid x))=-\sum\limits_{x,y} p(x)p(y\mid x) \log p(y\mid x)$  
(用定义很容易证明，请自己推导一遍)[^myblog]  

这里仍然使用这个近似 $p(x)\thickapprox \tilde p(x)$  

最大熵模型就是求解:  
$\arg \max\limits_p H(p) =\sum\limits_{x,y}(\tilde p(x) p(y\mid x) \log p(y\mid x))$  
s.t.$\sum\limits_y p(y\mid x)=1$  

再加上上一部分的约束条件，求最优化就可以得到最终结果了。  





## 参考资料

[^wikipedia]: [wikipedia] (https://en.wikipedia.org/wiki/Multinomial_logistic_regression)

[peghoty]: [最大熵学习笔记](http://blog.csdn.net/itplus/article/details/26550597)
[^myblog]: 我的另一篇blog[信息熵](http://www.guofei.site/2017/05/23/entropy.html)
