---
layout: post
title: 信息熵
categories: 模型
tags: 机器学习
keywords: entropy, conditional entropy
description:
---
## 信息熵的定义与性质

信息熵
: 如果X是离散分布，$P(X=x_i)=p_i$，那么信息熵定义为$H(X)=-\sum\limits_{i=1}^n p_i \log p_i$

(定义 $0\log0=0$)

性质  
1. 单位是bit或nat
1. 信息熵用来表示随机变量的不确定性，不确定性越大，信息熵越大
2. $0 \leq H(X) \leq \log n$

## 条件熵

条件熵(conditional entropy)
: 已知X的条件下，Y的不确定性。X对Y的条件分布的熵，对X的数学期望$H(Y \mid X)=\sum\limits_{i=1}^n P(X=x_i)H(Y \mid X=x_i)$

性质:  
1. $H(Y \mid X)=H(X,Y)-H(X)$(用条件熵定义和条件概率定义容易证明)
2. $H(Y \mid X) \leq H(Y)$(???不会证)

## empirical

大部分时候，概率是不知道的，是从数据中估计出来的，用估计出来的概率计算熵时，得到的结果是经验熵或条件经验熵  

经验熵(empirical entropy)
: 计算熵时，用的概率是从数据估计出来的。

条件经验熵(empirical entropy)
: 计算条件熵时，用的概率是从数据估计出来的

## 信息益增(information gain)

信息益增(information gain)
: 得知特征X的信息而使得Y的信息不确定度减少的程度

计算方法：  
A分类方法下，数据集D的经验熵变化  
$g(D,A)=H(D)-H(D \mid A)$  

## 信息益增比(information gain ratio)

$g_R(D,A)=\dfrac{g(D,A)}{H(D)}$
