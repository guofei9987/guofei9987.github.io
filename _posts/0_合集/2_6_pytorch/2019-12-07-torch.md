---
layout: post
title: ğŸ”¥ã€pytorchã€‘åŸºæœ¬è¯­æ³•
categories: torch
tags: 
keywords:
description:
order: 261
---



## å®‰è£…ä¸é…ç½®

çœ‹CUDAç‰ˆæœ¬
```
nvidia-smi
```

[å®˜ç½‘ä¸Šçš„å®‰è£…æ•™ç¨‹](https://pytorch.org/get-started/locally/)


2.xçš„é‡å¤§æ›´æ–°
```python
# ç¼–è¯‘ï¼Œæå¤§æå‡æ€§èƒ½
model = torch.compile(model)

# torch.distributedã€FSDPã€DTensor ç­‰èƒ½åŠ›æŒç»­æ”¹è¿›

# Transformer å¸¸ç”¨ç®—å­ï¼ˆå¦‚ attention ç›¸å…³ï¼‰å’Œç®—å­å®ç°ä¸ŠæŒç»­ä¼˜åŒ–
```

**GPU**
```python
import torch
from torch import nn

torch.cuda.is_available() # è¿”å› True/False è¡¨ç¤ºGPUæ˜¯å¦å¯ç”¨
torch.cuda.device_count() # å¯ç”¨çš„GPUæ•°é‡
```

**ä½¿ç”¨GPU**
```python
mytensor = my_tensor.to(device) # tensor
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
# 'cuda' ç­‰åŒäº 'cuda:X',å…¶ä¸­Xæ˜¯torch.cuda.current_device()
model = model.to(device) # è¿”å›çš„ model å’Œè¾“å…¥çš„ model æ˜¯åŒä¸€ä¸ªå¯¹è±¡ï¼Œè¿™ä¸ªè¡Œä¸ºä¸ä¸‹é¢çš„ tensor ä¸åŒ

print(mytensor.device)
# çœ‹æ¨¡å‹åœ¨å“ªä¸ªè®¾å¤‡ä¸Šï¼š(åŸç†æ˜¯çœ‹ç¬¬ä¸€ä¸ªå‚æ•°æ‰€åœ¨çš„ä½ç½®)
print(next(model.parameters()).device)
```

### å¹¶è¡Œ

```python
# torch é»˜è®¤ä¸ä¼šåšå¤šæ˜¾å¡è®¡ç®—ï¼Œç”¨è¿™ä¸ª
model = nn.DataParallel(model)
# ç›®å‰æ›¿æ¢ï¼šnn.parallel.DistributedDataParallel

# è®¾å®šè¿™ä¸ªä¹‹åï¼ŒCPUå ç”¨ä¼šæå¤§æé«˜
torch.set_num_threads(num_physical_cores/num_workers)

# åŠ è½½å¹¶è¡Œ
DataLoader(..., num_workers=args.nThreads)
```

- [æ€§èƒ½ç›¸å…³çš„å…¶å®ƒèµ„æ–™] https://zhuanlan.zhihu.com/p/69250939



## Tensor

**æ–°å»º**
```python
torch.empty(5, 3) 
torch.ones(3, 3)
torch.ones_like(...)
torch.zeros(5, 3, dtype=torch.long)
torch.eye(5)

torch.arange(start=0, end=8, step=2) # å«å¤´ä¸å«å°¾
torch.linspace(start=0, end=9, steps=5) # å‡åŒ€çš„å–5ä¸ªå€¼ï¼Œå«å¤´å«å°¾


# éšæœºç”Ÿæˆ
torch.manual_seed(2)         # è®¾ç½®ç§å­
print(torch.initial_seed())  # æŸ¥çœ‹ç§å­
torch.rand(5, 3)             # å‡åŒ€åˆ†å¸ƒ 0ï½1
torch.randn(5, 3)            # æ ‡å‡†æ­£æ€åˆ†å¸ƒ
torch.randn_like(x, dtype=torch.float)


# ä»å…¶å®ƒæ•°æ®æ–°å»º
torch.tensor([[5.5, 3],[2,3]], dtype=torch.float32)
torch.from_numpy(np.ones((5,5))) 

# è¯´æ˜ï¼š
# 1. å…¨éƒ¨å¯ä»¥æœ‰å…¥å‚ device=deviceï¼Œæˆ–è€… device='cuda:0'
# 2. å…¨éƒ¨å¯ä»¥ç”¨ dtype=torch.float32 æŒ‡å®šæ•°æ®ç±»å‹ï¼Œæ•°æ®ç±»å‹å¦‚ä¸‹
# 3. è¦åœ¨åŒä¸€ä¸ª device ä¸Šæ‰å¯ä»¥è¿è¡Œ
```

**device**

```python
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
# device è¿˜å¯ä»¥æ˜¯å­—ç¬¦ä¸²: device = 'cpu' # device = 'cuda'

# åœ¨ CPU ä¸Šç”Ÿæˆï¼Œç„¶åè½¬åˆ° GPU
x = torch.randn(4, 4).to(device)
# åœ¨ GPU ç”Ÿæˆï¼Œç„¶åè½¬åˆ° CPU
x = torch.randn(4, 4, device=device).cpu()
# ä¹Ÿå¯ä»¥ç”¨ to(device) è½¬ CPU

# è¯´æ˜ï¼šè½¬æ¢è®¾å¤‡æ—¶å‘ç”Ÿæ•°æ®å¤åˆ¶
```



**æ•°æ®ç±»å‹è½¬æ¢**
```python
x.int()
x.long()
x.float()
x.bool()
x.char() # int8 ç±»å‹
x.double()

# æœ‰è¿™äº›ç±»å‹ï¼ˆè¿˜æœ‰å¾ˆå¤šï¼‰
torch.bool
torch.int
torch.short
torch.uint8 # è¿™ä¸ªæ˜¯ ByteTensor
torch.int
torch.int8
torch.int16
torch.int32
torch.long
torch.float
torch.float16
torch.float32
torch.float64
torch.double
torch.complex32
torch.complex64
```


### åŸºæœ¬è¿ç®—
```python
# å››èˆäº”å…¥
a.round()
a.fix()
a.floor()


a + b
a * b    # çŸ©é˜µå…ƒç´ ç§¯
a @ b    # çŸ©é˜µç§¯
a += 1

# å¯ä»¥æŒ‡å®šè¾“å‡ºåˆ°å“ªä¸ªå˜é‡
result = torch.empty(5, 3)
torch.add(a, b, out=result)


# å¤§å¤šæ•°è¿ç®—ç¬¦åé¢å¯ä»¥åŠ ä¸ªä¸‹åˆ’çº¿ï¼Œè¡¨ç¤ºæ›¿æ¢è¿ç®—
# æ›¿æ¢åŠ 
y.add_(x) # è¿™ä¸ªä¼šæŠŠåŠ æ³•çš„ç»“æœèµ‹å€¼ç»™y
# å¦‚ x.copy_(y), x.t_()

# è¿™äº›è¿ç®—ä¹Ÿå¯ä»¥ä¸ç”¨ç¬¦å·ï¼Œè€Œæ˜¯ç”¨å‡½æ•°è¡¨ç¤º:
a.matmul(b)
```



å–æ•°
```python
a.size() # torch.Size([5, 3])
a.numel() # å…±å¤šå°‘ä¸ªå…ƒç´ 

# index å’Œ Numpy ä¸€æ ·
a[:, 1]

# è·å–æŸä¸€è¡Œ
a.select(dim=1,index=2) # è·å–æŒ‰ç…§ dim è®¡ç®—çš„ ç¬¬ index ç»„ã€‚
# ä¾‹å­çš„ dim=1 è¡¨ç¤ºè·å–çš„æ˜¯åˆ—ï¼Œindex=2 è¡¨ç¤ºè·å–ç¬¬ 2 åˆ—


# tensor è½¬å…¶å®ƒæ ¼å¼
a.numpy() # è½¬ np.array
a.tolist() # è½¬ list

# è½¬ Python æ•°å­—ï¼Œåªæœ‰å•ä¸ªå…ƒç´ çš„æ—¶å€™å¯ä»¥ç”¨
a[0][0].item()

# æ³¨æ„ä¸€ä¸ªç‰¹æ€§: å…±äº«å†…å­˜
x = torch.ones(2)
y = x.numpy()
x += 1
print(x, y)
# æ‰“å°ï¼štensor([2., 2.]) [2. 2.]

# å€¼å¾—ä¸€æï¼Œåè¿‡æ¥ä¹Ÿæ˜¯è¿™ä¸ªç‰¹æ€§
a = np.ones(5)
b = torch.from_numpy(a)
a += 1
print(a, b)
# [2. 2.] tensor([2., 2.])
```

reshape
```python
# reshape ä¹Ÿå¯ä»¥åšä¸‹é¢è¿™äº›äº‹ï¼Œä½†ä¸èƒ½reshapeåˆ°1ç»´ï¼ˆï¼Ÿä¸çŸ¥é“ä¸ºä»€ä¹ˆè¦è¿™ä¹ˆè®¾è®¡ï¼‰

a = torch.randn(4, 4)
a.reshape(2, -1)
a.reshape(-1)
```



### æ•°å­¦è¿ç®—

```python
a.sqrt()
a.square()
a.exp()

a.cos()
a.cosh()
a.acos()
a.acosh()
a.arccos()
a.arccosh()
```


**linalg**
```python
from torch import linalg

a = torch.rand(5, 5)


U, S, Vh = linalg.svd(a)
# full_matrices=Falseï¼Œå¯ä»¥æé«˜æ€§èƒ½

# åªè¿”å›ç‰¹å¾å€¼ï¼š
linalg.svdvals(a)

e_val, e_vec = linalg.eig(a)
linalg.eigvals(a)


# linalg è¿˜æœ‰å¾ˆå¤šæ–¹æ³•ï¼Œä¾‹å¦‚ï¼š
linalg.norm
linalg.qr
```



çŸ©é˜µæ“ä½œ
```py
d = a.diagonal()           # å–å‡ºå¯¹è§’çº¿
a1 = torch.diag_embed(d)   # è¿˜åŸä¸ºå¯¹è§’çŸ©é˜µ

a.flip(dims=(0,)) # æŒ‰ç…§ dims ç¡®å®šçš„ç»´åº¦ç¿»è½¬

a.t() # è½¬ç§©


a.tril(k=0) # ä¸‹ä¸‰è§’çŸ©é˜µ
a.triu(k=0) # ä¸Šä¸‰è§’çŸ©é˜µ

# cat
a = torch.rand(3, 2)
b = torch.rand(3, 2)

torch.cat([a, b], dim=0)

# åˆ†å‰²
a = torch.arange(0,12,step=1).reshape(2,6)
a.chunk(chunks=3, dim=1) # å°½é‡å‡åŒ€åˆ†ä¸ºä¸‰ä»½
# åˆ†ä¸º3ä»½ï¼Œå¤§å°åˆ†åˆ«æ˜¯ 1ï¼Œ3ï¼Œ2
torch.split(x, split_size_or_sections=(1, 3, 2), dim=1)
# æŒ‰ dim=1 åˆ†ä¸º3ä»½ï¼Œå…¶å¤§å°åˆ†åˆ«ä¸º 1, 3, 2
```

where

```python
torch.where(x1 > 0.5, x1, x2)
torch.clip(x1, min=0.4, max=0.6)
```



**æŒ‰ä½è¿ç®—**
```python
# dtype å¿…é¡»æ˜¯ int ç±»å‹ï¼Œæœ€å¥½æ˜¯ uint8
x1 = torch.tensor([1, 2, 3], dtype=torch.uint8)
x2 = torch.tensor([1, 1, 1], dtype=torch.uint8)


x1 & x2  # æŒ‰ä½ä¸
x1 | x2  # æŒ‰ä½æˆ–
~x1  # æŒ‰ä½é
x1 ^ x2  # æŒ‰ä½å¼‚æˆ–
# ä»¥ä¸Šå¯¹åº”çš„è¿ç®—ç¬¦ä¸ºï¼šx1.bitwise_or(x2) ç­‰ç±»ä¼¼çš„ä¸œè¥¿

x1 << 1  # ç§»ä½è¿ç®—
# x1.bitwise_left_shift(1)
```

**é€»è¾‘è¿ç®—**
```python
# 0 è½¬ä¸º Falseï¼Œåˆ«çš„æ•°å­—éƒ½è½¬ä¸º True
x1 = torch.tensor([-0.9, 0, True, False], dtype=torch.bool)

# >ã€<ã€==ã€ >=ã€ <= éƒ½å¯ä»¥
x2 = torch.rand(size=(4,)) < 0.5

# é€»è¾‘ä¸ã€æˆ–ã€å¼‚æˆ–ã€é
x1 & x2
x1 | x2
x1 ^ x2
~x1

# å…¶å®ƒæ–¹å¼ï¼š
# x1.logical_and(x2)
# x1.logical_or(x2)
# x1.logical_xor(x2)
# x1.logical_xor_(x2)
# x1.logical_not()
# x1.logical_not_()
```



### ç»Ÿè®¡ç±»è¿ç®—

```py
a.mean()
a.mean(dim=1,keepdim=True)

a.max()
values, indices = a.max(dim=1, keepdim=True)

a.min()
a.mode()

values, indices = a.sort(dim=1, descending=False)

a.argmin()
a.argsort()
a.argmax(dim=1, keepdim=True)

a.histc
a.histogram

a.std()

```




## æ¿€æ´»å‡½æ•°

```python
import torch.nn as nn
import torch.nn.functional as F
# å¾ˆå¤šæ¿€æ´»å‡½æ•°ï¼Œåœ¨ä¸Šé¢ä¸¤ä¸ªæ¨¡å—ä¸­æ˜¯ç­‰ä»·çš„ï¼Œä½¿ç”¨ä¸Šçš„åŒºåˆ«ï¼š
# nn æä¾›çš„æ˜¯æ¨¡å—å½¢å¼ï¼Œé€‚åˆæ”¾è¿› nn.Sequential
# F æä¾›çš„æ˜¯å‡½æ•°å½¢å¼ï¼Œæ›´çµæ´»ï¼Œé€‚åˆæ¡ä»¶è°ƒç”¨ã€çµæ´»çš„åœºæ™¯


# Module æ–¹å¼
act = nn.ReLU(inplace=False)
y = act(x)

# Functional æ–¹å¼
y = F.relu(x, inplace=False)
```


### æ¿€æ´»å‡½æ•°
- `nn.ReLU` ReLU(Rectifier Linear Unit), $\max(0,x)$ 
- `nn.ReLU6` æ˜¯ hard-sigmoid çš„å˜ç§ $\min(\max(0,x),6)$ï¼Œç§»åŠ¨ç«¯/é‡åŒ–å‹å¥½
- `nn.Sigmoid`  $1/(1+\exp(-x))$ï¼Œä¼˜ç‚¹æ˜¯ä¸å®¹æ˜“å‡ºç°æç«¯åªï¼Œä½†å› ä¸ºæ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œå·²ç»å¾ˆå°‘ç”¨äº†
- `nn.Tanh` $\dfrac{e^{2x}-1}{e^{2x}+1}$
- `nn.Softsign` æ˜¯ç¬¦å·å‡½æ•°çš„è¿ç»­ä¼°è®¡$x/(abs(x)+1)$
- `nn.Softplus` æ˜¯ReLUçš„å¹³æ»‘ç‰ˆ $\log(\exp(x)+1)$
- `nn.ELU` ELU(Exponential Linear Unit)  $$\begin{cases} x, & x>0 \\ \alpha\left(e^{x}-1\right), & x\le 0 \end{cases}$$





```python
nn.Softmax(dim=dim)
torch.softmax(x, dim=1)    # ç”¨äºå›¾ç‰‡ï¼Œå¯¹ channel åš softmax
nn.LogSoftmax()  # å¯¹ softmax å–å¯¹æ•°ï¼Œå¯¹åº”çš„æŸå¤±å‡½æ•°æ˜¯ NLLLoss

# æ›´ç°ä»£çš„å†™æ³•
nn.CrossEntropyLoss
# ç­‰ä»·äº nn.LogSoftmax + nn.NLLLoss
```






2019å¹´5æœˆ22æ—¥æ›´æ–°ï¼ˆæ¥è‡ªå´æ©è¾¾çš„ DeepLearning.ai è¯¾ç¨‹ï¼‰ï¼š  
- sigmoid: never use, except output layer  
- tanh: pretty much strictly superior then sigmoid  
- ReLU: if you do not know which to choose, always choose ReLU  
- Leaky ReLUï¼š you may try this $max(0.01z,z)$

**æ¿€æ´»å‡½æ•°çš„å¾®åˆ†** 
- sigmoid:$g(x)=\dfrac{1}{1+e^{-z}},g'(z)=g(z)(1-g(z))$  
- tanh:$g(x)=\dfrac{e^z-e^{-z}}{e^z+e^{-z}},g'(z)=1-(g(z))^2$  
- ReLU/Leaky RelUï¼š åˆ†æ®µå‡½æ•°ï¼Œæ³¨æ„0ç‚¹çš„æƒ…å†µï¼ˆä½†0ç‚¹ä¸æ˜¯å¾ˆé‡è¦ï¼‰


### å·ç§¯ç›¸å…³

```py
# nn.Conv1d, nn.Conv2d, nn.Conv3d, nn.Linear
nn.Conv2d(in_channels=1
                , out_channels=16, kernel_size=(5, 5), stride=(1, 1)
                , padding='same' # æˆ–è€… padding=2
)

nn.MaxPool2d(kernel_size=2)
```

## å»ºç«‹æ¨¡å‹

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from collections import OrderedDict


class MyNet(nn.Module):
    def __init__(self):
        super(MyNet, self).__init__()

        # æ–¹æ³•1ï¼šæœ€å¸¸ç”¨
        self.fc1 = nn.Linear(10, 10)
        # æ–¹æ³•2ï¼šåŠ¨æ€å‘½åï¼Œå°½é‡åˆ«ç”¨
        self.add_module("fc2", nn.Linear(10, 10))
        # æ–¹æ³•3ï¼š
        self.block3 = nn.Sequential(OrderedDict([
            ("conv1", torch.nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(5, 5))),
            ("relu1", nn.ReLU())
        ]))
        # æ–¹æ³•4ï¼š
        self.add_module("block4", nn.Sequential(OrderedDict([
            ("conv1", torch.nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(5, 5))),
            ("relu1", nn.ReLU())
        ])))

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        # forward ä¸­ä¸åŠ å…¥å°±ä¸èµ·ä½œç”¨
        # x = self.block3(x)
        # x = self.block4(x)
        return x


my_net = MyNet()
print(my_net)
```

å¦‚ä½•ä½¿ç”¨æ¨¡å‹
```python
x = torch.randn(8, 10)

y = my_net(x)
```


### ä¿å­˜å’Œè½½å…¥æ¨¡å‹

```python
# å­˜
torch.save(my_net.state_dict(),'./model.pkh')

my_net1 = MyNet().to(device)
# è¯»
my_net1.load_state_dict(torch.load('./model.pkh', weights_only=True))
# weights_only åªåŠ è½½æƒé‡ï¼Œæ›´å®‰å…¨

# å…³äº device çš„è¯´æ˜ï¼š
# 1. state_dict æ—¶ï¼Œä¼šè¿åŒæ‰€åœ¨è®¾å¤‡å·ä¸€èµ·ä¿å­˜ï¼›åœ¨ load æ—¶ï¼Œä¼š load åˆ°å¯¹åº”çš„è®¾å¤‡ä¸Š
# 2. ä½¿ç”¨ map_location å¯ä»¥æŒ‡å®š load åˆ°å“ªä¸ªè®¾å¤‡ä¸Š
# 3. my_net1.load_state_dict(xxx) æ—¶ï¼Œä¼šåŠ è½½åˆ° my_net1 æ‰€åœ¨çš„è®¾å¤‡ä¸Šã€‚æ— è®º load æ—¶è¿™äº› tensor åœ¨å“ªä¸ªè®¾å¤‡ä¸Š

# æŒ‡å®šå‚æ•° load åˆ°å“ªä¸ªè®¾å¤‡ä¸Šï¼š
device3 = torch.device("cuda:3")
torch.load('model.pkh', map_location=device3, weights_only=True)

# æˆ–è€…
troch.load('model.pkh', map_location={"cuda:1":"cuda:0"})
```

è¿åŒè®­ç»ƒçŠ¶æ€ä¸€èµ·ä¿å­˜ï¼Œä»¥ä¾¿æ–­ç‚¹ç»­è®­

```python
# ä¿å­˜
ckpt = {
    "epoch": epoch,
    "model_state": model.state_dict(),
    "optim_state": optimizer.state_dict(),
    # å¯é€‰ï¼š
    # "sched_state": scheduler.state_dict(),
    # "scaler_state": scaler.state_dict(),   # AMP ç”¨
}
torch.save(ckpt, "ckpt.pth")

# åŠ è½½
model = MyNet().to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

ckpt = torch.load("ckpt.pth", map_location=device)
model.load_state_dict(ckpt["model_state"])
optimizer.load_state_dict(ckpt["optim_state"])

start_epoch = ckpt["epoch"] + 1
model.train()
for epoch in range(start_epoch, num_epochs):
    ...
```


safetensors: æ›´å¿«ã€æ›´å®‰å…¨ã€æ›´æ ‡å‡†çš„ tensor ä¿å­˜æ ¼å¼

```python
# pip install safetensors

# ä¿å­˜
from safetensors.torch import save_file

state = my_net.state_dict()
save_file(state, "model.safetensors")

# è¯»å–
from safetensors.torch import load_file

state = load_file("model.safetensors", device='cuda:1')  # é»˜è®¤åœ¨ CPUï¼Œå¯ä»¥æŒ‡å®šè®¾å¤‡
my_net1 = MyNet()
my_net1.load_state_dict(state)
```










## æ¡ˆä¾‹
```python
model = Model(input_size, output_size)
if torch.cuda.device_count() > 1:
  print("Let's use", torch.cuda.device_count(), "GPUs!")
  # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs
  model = nn.DataParallel(model)

model.to(device)
```

## å…¶å®ƒ

```python

torch.get_default_dtype()
torch.set_default_dtype(torch.float16)


torch.ByteTensor([1.1, 2, 3]) # å¯¹äºå°æ•°ï¼Œä¼šå–æ•´ã€‚å¯¹äºæº¢å‡ºçš„æ•°ï¼ˆå¤§äº255æˆ–è´Ÿçš„ï¼‰ï¼Œä¼šèˆå¼ƒæº¢å‡ºä½æ•°
# ä½†æ˜¯ï¼Œå¦‚æœè¾“å…¥çš„æ˜¯Tensorï¼Œä¼šå¡æ­»ï¼Œè¿™ä¹ˆè§£å†³ï¼š
torch.tensor([1, 2, 3]).type(torch.int8)
```



