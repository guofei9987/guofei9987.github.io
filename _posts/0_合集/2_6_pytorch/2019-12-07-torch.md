---
layout: post
title: ğŸ”¥ã€pytorchã€‘åŸºæœ¬è¯­æ³•
categories: torch
tags: 
keywords:
description:
order: 261
---

å¯ä»¥å‚è€ƒ [ã€pytorchã€‘å»ºç«‹æ¨¡å‹](https://www.guofei.site/2019/12/14/torch_model.html)ï¼Œæä¾›äº†ä¸€ä¸ªå®Œæ•´çš„æœ€å°ç¥ç»ç½‘ç»œæ„å»ºè¿‡ç¨‹ï¼Œæœ¬æ–‡ä¼šå¯¹æ¯ä¸ªæ­¥éª¤åšæ‹†è§£å’Œè¯¦ç»†æè¿°ï¼š

1. å®‰è£…ä¸é…ç½®ã€‚ä»‹ç»å®‰è£…ã€ä½¿ç”¨GPUç­‰çŸ¥è¯†
2. Tensor çš„å„ç§ç”¨æ³•ç¤ºä¾‹ï¼ŒåŒ…æ‹¬å„ç§å‡½æ•°ã€çŸ©é˜µã€ç»Ÿè®¡è¿ç®—
3. å»ºç«‹æ¨¡å‹ï¼ŒåŒ…æ‹¬
    - ç½‘ç»œå±‚ã€æ¿€æ´»å‡½æ•°ã€å·ç§¯å±‚ ç­‰
    - æ¨¡å‹çš„ save/load
    - æ­£åˆ™åŒ–
4. å®šä¹‰æŸå¤±å‡½æ•°
5. å®šä¹‰ä¼˜åŒ–å™¨
6. è®­ç»ƒ



## å®‰è£…ä¸é…ç½®

çœ‹CUDAç‰ˆæœ¬
```
nvidia-smi
```

[å®˜ç½‘ä¸Šçš„å®‰è£…æ•™ç¨‹](https://pytorch.org/get-started/locally/)


2.xçš„é‡å¤§æ›´æ–°
```python
# ç¼–è¯‘ï¼Œæå¤§æå‡æ€§èƒ½
model = torch.compile(model)

# torch.distributedã€FSDPã€DTensor ç­‰èƒ½åŠ›æŒç»­æ”¹è¿›

# Transformer å¸¸ç”¨ç®—å­ï¼ˆå¦‚ attention ç›¸å…³ï¼‰å’Œç®—å­å®ç°ä¸ŠæŒç»­ä¼˜åŒ–
```

**GPU**
```python
import torch
from torch import nn

torch.cuda.is_available() # è¿”å› True/False è¡¨ç¤ºGPUæ˜¯å¦å¯ç”¨
torch.cuda.device_count() # å¯ç”¨çš„GPUæ•°é‡
```

**ä½¿ç”¨GPU**
```python
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
# 'cuda' ç­‰åŒäº 'cuda:X',å…¶ä¸­Xæ˜¯torch.cuda.current_device()
mytensor = my_tensor.to(device) # tensor
model = model.to(device) # è¿”å›çš„ model å’Œè¾“å…¥çš„ model æ˜¯åŒä¸€ä¸ªå¯¹è±¡ï¼Œè¿™ä¸ªè¡Œä¸ºä¸ä¸‹é¢çš„ tensor ä¸åŒ

print(mytensor.device)
# çœ‹æ¨¡å‹åœ¨å“ªä¸ªè®¾å¤‡ä¸Šï¼š(åŸç†æ˜¯çœ‹ç¬¬ä¸€ä¸ªå‚æ•°æ‰€åœ¨çš„ä½ç½®)
print(next(model.parameters()).device)
```

### å¹¶è¡Œ

```python
# torch é»˜è®¤ä¸ä¼šåšå¤šæ˜¾å¡è®¡ç®—ï¼Œç”¨è¿™ä¸ª
model = nn.DataParallel(model)
# ç›®å‰æ›¿æ¢ï¼šnn.parallel.DistributedDataParallel

# è®¾å®šè¿™ä¸ªä¹‹åï¼ŒCPUå ç”¨ä¼šæå¤§æé«˜
torch.set_num_threads(num_physical_cores/num_workers)

# åŠ è½½å¹¶è¡Œ
DataLoader(..., num_workers=args.nThreads)
```

- [æ€§èƒ½ç›¸å…³çš„å…¶å®ƒèµ„æ–™] https://zhuanlan.zhihu.com/p/69250939



## Tensor

**æ–°å»º**
```python
torch.empty(5, 3) 
torch.ones(3, 3)
torch.ones_like(...)
torch.zeros(5, 3, dtype=torch.long)
torch.eye(5)

torch.arange(start=0, end=8, step=2) # å«å¤´ä¸å«å°¾
torch.linspace(start=0, end=9, steps=5) # å‡åŒ€çš„å–5ä¸ªå€¼ï¼Œå«å¤´å«å°¾


# éšæœºç”Ÿæˆ
torch.manual_seed(2)         # è®¾ç½®ç§å­
print(torch.initial_seed())  # æŸ¥çœ‹ç§å­
torch.rand(5, 3)             # å‡åŒ€åˆ†å¸ƒ 0ï½1
torch.randn(5, 3)            # æ ‡å‡†æ­£æ€åˆ†å¸ƒ
torch.randn_like(x, dtype=torch.float)


# ä»å…¶å®ƒæ•°æ®æ–°å»º
torch.tensor([[5.5, 3],[2,3]], dtype=torch.float32)
torch.from_numpy(np.ones((5,5))) 

# è¯´æ˜ï¼š
# 1. å…¨éƒ¨å¯ä»¥æœ‰å…¥å‚ device=deviceï¼Œæˆ–è€… device='cuda:0'
# 2. å…¨éƒ¨å¯ä»¥ç”¨ dtype=torch.float32 æŒ‡å®šæ•°æ®ç±»å‹ï¼Œæ•°æ®ç±»å‹å¦‚ä¸‹
# 3. è¦åœ¨åŒä¸€ä¸ª device ä¸Šæ‰å¯ä»¥è¿è¡Œ
```

**device**

```python
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
# device è¿˜å¯ä»¥æ˜¯å­—ç¬¦ä¸²: device = 'cpu' # device = 'cuda'

# åœ¨ CPU ä¸Šç”Ÿæˆï¼Œç„¶åè½¬åˆ° GPU
x = torch.randn(4, 4).to(device)
# åœ¨ GPU ç”Ÿæˆï¼Œç„¶åè½¬åˆ° CPU
x = torch.randn(4, 4, device=device).cpu()
# ä¹Ÿå¯ä»¥ç”¨ to(device) è½¬ CPU

# è¯´æ˜ï¼šè½¬æ¢è®¾å¤‡æ—¶å‘ç”Ÿæ•°æ®å¤åˆ¶
```



**æ•°æ®ç±»å‹è½¬æ¢**
```python
x.int()
x.long()
x.float()
x.bool()
x.char() # int8 ç±»å‹
x.double()

# æœ‰è¿™äº›ç±»å‹ï¼ˆè¿˜æœ‰å¾ˆå¤šï¼‰
torch.bool
torch.int
torch.short
torch.uint8 # è¿™ä¸ªæ˜¯ ByteTensor
torch.int
torch.int8
torch.int16
torch.int32
torch.long
torch.float
torch.float16
torch.float32
torch.float64
torch.double
torch.complex32
torch.complex64
```


### åŸºæœ¬è¿ç®—
```python
# å››èˆäº”å…¥
a.round()
a.fix()
a.floor()


a + b
a * b    # çŸ©é˜µå…ƒç´ ç§¯
a @ b    # çŸ©é˜µç§¯
a += 1

# å¯ä»¥æŒ‡å®šè¾“å‡ºåˆ°å“ªä¸ªå˜é‡
result = torch.empty(5, 3)
torch.add(a, b, out=result)


# å¤§å¤šæ•°è¿ç®—ç¬¦åé¢å¯ä»¥åŠ ä¸ªä¸‹åˆ’çº¿ï¼Œè¡¨ç¤ºæ›¿æ¢è¿ç®—
# æ›¿æ¢åŠ 
y.add_(x) # è¿™ä¸ªä¼šæŠŠåŠ æ³•çš„ç»“æœèµ‹å€¼ç»™y
# å¦‚ x.copy_(y), x.t_()

# è¿™äº›è¿ç®—ä¹Ÿå¯ä»¥ä¸ç”¨ç¬¦å·ï¼Œè€Œæ˜¯ç”¨å‡½æ•°è¡¨ç¤º:
a.matmul(b)
```



å–æ•°
```python
a.size() # torch.Size([5, 3])
a.numel() # å…±å¤šå°‘ä¸ªå…ƒç´ 

# index å’Œ Numpy ä¸€æ ·
a[:, 1]

# è·å–æŸä¸€è¡Œ
a.select(dim=1,index=2) # è·å–æŒ‰ç…§ dim è®¡ç®—çš„ ç¬¬ index ç»„ã€‚
# ä¾‹å­çš„ dim=1 è¡¨ç¤ºè·å–çš„æ˜¯åˆ—ï¼Œindex=2 è¡¨ç¤ºè·å–ç¬¬ 2 åˆ—


# tensor è½¬å…¶å®ƒæ ¼å¼
a.numpy() # è½¬ np.array
a.tolist() # è½¬ list

# è½¬ Python æ•°å­—ï¼Œåªæœ‰å•ä¸ªå…ƒç´ çš„æ—¶å€™å¯ä»¥ç”¨
a[0][0].item()

# æ³¨æ„ä¸€ä¸ªç‰¹æ€§: å…±äº«å†…å­˜
x = torch.ones(2)
y = x.numpy()
x += 1
print(x, y)
# æ‰“å°ï¼štensor([2., 2.]) [2. 2.]

# å€¼å¾—ä¸€æï¼Œåè¿‡æ¥ä¹Ÿæ˜¯è¿™ä¸ªç‰¹æ€§
a = np.ones(5)
b = torch.from_numpy(a)
a += 1
print(a, b)
# [2. 2.] tensor([2., 2.])
```

reshape
```python
# reshape ä¹Ÿå¯ä»¥åšä¸‹é¢è¿™äº›äº‹ï¼Œä½†ä¸èƒ½reshapeåˆ°1ç»´ï¼ˆï¼Ÿä¸çŸ¥é“ä¸ºä»€ä¹ˆè¦è¿™ä¹ˆè®¾è®¡ï¼‰

a = torch.randn(4, 4)
a.reshape(2, -1)
a.reshape(-1)
```



### æ•°å­¦è¿ç®—

```python
a.sqrt()
a.square()
a.exp()

# æŒ‡æ•°è¿ç®—
a.pow(-1.0)
a.pow(2.0)

a.cos()
a.cosh()
a.acos()
a.acosh()
a.arccos()
a.arccosh()
```


**linalg**
```python
from torch import linalg

a = torch.rand(5, 5)


U, S, Vh = linalg.svd(a)
# full_matrices=Falseï¼Œå¯ä»¥æé«˜æ€§èƒ½

# åªè¿”å›ç‰¹å¾å€¼ï¼š
linalg.svdvals(a)

e_val, e_vec = linalg.eig(a)
linalg.eigvals(a)


# linalg è¿˜æœ‰å¾ˆå¤šæ–¹æ³•ï¼Œä¾‹å¦‚ï¼š
linalg.norm
linalg.qr
```



çŸ©é˜µæ“ä½œ
```py
d = a.diagonal()           # å–å‡ºå¯¹è§’çº¿
a1 = torch.diag_embed(d)   # è¿˜åŸä¸ºå¯¹è§’çŸ©é˜µ

a.flip(dims=(0,)) # æŒ‰ç…§ dims ç¡®å®šçš„ç»´åº¦ç¿»è½¬

a.t() # è½¬ç§©


a.tril(k=0) # ä¸‹ä¸‰è§’çŸ©é˜µ
a.triu(k=0) # ä¸Šä¸‰è§’çŸ©é˜µ

# cat
a = torch.rand(3, 2)
b = torch.rand(3, 2)

torch.cat([a, b], dim=0)

# åˆ†å‰²
a = torch.arange(0,12,step=1).reshape(2,6)
a.chunk(chunks=3, dim=1) # å°½é‡å‡åŒ€åˆ†ä¸ºä¸‰ä»½
# åˆ†ä¸º3ä»½ï¼Œå¤§å°åˆ†åˆ«æ˜¯ 1ï¼Œ3ï¼Œ2
torch.split(x, split_size_or_sections=(1, 3, 2), dim=1)
# æŒ‰ dim=1 åˆ†ä¸º3ä»½ï¼Œå…¶å¤§å°åˆ†åˆ«ä¸º 1, 3, 2
```

where

```python
torch.where(x1 > 0.5, x1, x2)
torch.clip(x1, min=0.4, max=0.6)
```



**æŒ‰ä½è¿ç®—**
```python
# dtype å¿…é¡»æ˜¯ int ç±»å‹ï¼Œæœ€å¥½æ˜¯ uint8
x1 = torch.tensor([1, 2, 3], dtype=torch.uint8)
x2 = torch.tensor([1, 1, 1], dtype=torch.uint8)


x1 & x2  # æŒ‰ä½ä¸
x1 | x2  # æŒ‰ä½æˆ–
~x1  # æŒ‰ä½é
x1 ^ x2  # æŒ‰ä½å¼‚æˆ–
# ä»¥ä¸Šå¯¹åº”çš„è¿ç®—ç¬¦ä¸ºï¼šx1.bitwise_or(x2) ç­‰ç±»ä¼¼çš„ä¸œè¥¿

x1 << 1  # ç§»ä½è¿ç®—
# x1.bitwise_left_shift(1)
```

**é€»è¾‘è¿ç®—**
```python
# 0 è½¬ä¸º Falseï¼Œåˆ«çš„æ•°å­—éƒ½è½¬ä¸º True
x1 = torch.tensor([-0.9, 0, True, False], dtype=torch.bool)

# >ã€<ã€==ã€ >=ã€ <= éƒ½å¯ä»¥
x2 = torch.rand(size=(4,)) < 0.5

# é€»è¾‘ä¸ã€æˆ–ã€å¼‚æˆ–ã€é
x1 & x2
x1 | x2
x1 ^ x2
~x1

# å…¶å®ƒæ–¹å¼ï¼š
# x1.logical_and(x2)
# x1.logical_or(x2)
# x1.logical_xor(x2)
# x1.logical_xor_(x2)
# x1.logical_not()
# x1.logical_not_()
```



### ç»Ÿè®¡ç±»è¿ç®—

```py
a.mean()
a.mean(dim=1,keepdim=True)

a.sum()
a.sum(dim=1)

a.max()
values, indices = a.max(dim=1, keepdim=True)

a.min()
a.mode()

values, indices = a.sort(dim=1, descending=False)

a.argmin()
a.argsort()
a.argmax(dim=1, keepdim=True)

a.histc
a.histogram

a.std()

```




## ç½‘ç»œå±‚

```
nn.Linear(16 * 5 * 5, 120)
```


### æ¿€æ´»å‡½æ•°

```python
import torch.nn as nn
import torch.nn.functional as F
# å¾ˆå¤šæ¿€æ´»å‡½æ•°ï¼Œåœ¨ä¸Šé¢ä¸¤ä¸ªæ¨¡å—ä¸­æ˜¯ç­‰ä»·çš„ï¼Œä½¿ç”¨ä¸Šçš„åŒºåˆ«ï¼š
# nn æä¾›çš„æ˜¯æ¨¡å—å½¢å¼ï¼Œé€‚åˆæ”¾è¿› nn.Sequential
# F æä¾›çš„æ˜¯å‡½æ•°å½¢å¼ï¼Œæ›´çµæ´»ï¼Œé€‚åˆæ¡ä»¶è°ƒç”¨ã€çµæ´»çš„åœºæ™¯


# Module æ–¹å¼
act = nn.ReLU(inplace=False)
y = act(x)

# Functional æ–¹å¼
y = F.relu(x, inplace=False)
```


ä¸»è¦çš„æ¿€æ´»å‡½æ•°ï¼š
- `nn.ReLU` ReLU(Rectifier Linear Unit), $\max(0,x)$ 
- `nn.ReLU6` æ˜¯ hard-sigmoid çš„å˜ç§ $\min(\max(0,x),6)$ï¼Œç§»åŠ¨ç«¯/é‡åŒ–å‹å¥½
- `nn.Sigmoid`  $1/(1+\exp(-x))$ï¼Œä¼˜ç‚¹æ˜¯ä¸å®¹æ˜“å‡ºç°æç«¯åªï¼Œä½†å› ä¸ºæ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œå·²ç»å¾ˆå°‘ç”¨äº†
- `nn.Tanh` $\dfrac{e^{2x}-1}{e^{2x}+1}$
- `nn.Softsign` æ˜¯ç¬¦å·å‡½æ•°çš„è¿ç»­ä¼°è®¡$x/(abs(x)+1)$
- `nn.Softplus` æ˜¯ReLUçš„å¹³æ»‘ç‰ˆ $\log(\exp(x)+1)$
- `nn.ELU` ELU(Exponential Linear Unit)  $$\begin{cases} x, & x>0 \\ \alpha\left(e^{x}-1\right), & x\le 0 \end{cases}$$





```python
nn.Softmax(dim=dim)
torch.softmax(x, dim=1)    # ç”¨äºå›¾ç‰‡ï¼Œå¯¹ channel åš softmax
nn.LogSoftmax()  # å¯¹ softmax å–å¯¹æ•°ï¼Œå¯¹åº”çš„æŸå¤±å‡½æ•°æ˜¯ NLLLoss

# æ›´ç°ä»£çš„å†™æ³•
nn.CrossEntropyLoss
# ç­‰ä»·äº nn.LogSoftmax + nn.NLLLoss
```






2019å¹´5æœˆ22æ—¥æ›´æ–°ï¼ˆæ¥è‡ªå´æ©è¾¾çš„ DeepLearning.ai è¯¾ç¨‹ï¼‰ï¼š  
- sigmoid: never use, except output layer  
- tanh: pretty much strictly superior then sigmoid  
- ReLU: if you do not know which to choose, always choose ReLU  
- Leaky ReLUï¼š you may try this $max(0.01z,z)$

**æ¿€æ´»å‡½æ•°çš„å¾®åˆ†** 
- sigmoid:$g(x)=\dfrac{1}{1+e^{-z}},g'(z)=g(z)(1-g(z))$  
- tanh:$g(x)=\dfrac{e^z-e^{-z}}{e^z+e^{-z}},g'(z)=1-(g(z))^2$  
- ReLU/Leaky RelUï¼š åˆ†æ®µå‡½æ•°ï¼Œæ³¨æ„0ç‚¹çš„æƒ…å†µï¼ˆä½†0ç‚¹ä¸æ˜¯å¾ˆé‡è¦ï¼‰


### å·ç§¯ç›¸å…³

```py
import numpy as np
import torch
from torch import nn

conv_1 = nn.Conv2d(
    in_channels=1
    , out_channels=16
    , kernel_size=(5, 5)
    , stride=(1, 1)
    , padding='same'  # æˆ–è€… padding=2
)

# æ˜¾ç¤ºå·ç§¯æ ¸çš„å€¼
conv_1.weight

# å¯ä»¥ç»™å®ƒèµ‹å€¼
filter_1 = torch.from_numpy(np.ones((16, 1, 5, 5)))
conv_1.weight = torch.nn.Parameter(filter_1)

```

Pool

```python
nn.MaxPool2d(kernel_size=2)


# è¿˜æœ‰ï¼š
# nn.Conv1d, nn.Conv2d, nn.Conv3d
# nn.MaxPool, nn.MaxPool2d, nn.MaxPool3d
```










## å»ºç«‹æ¨¡å‹

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from collections import OrderedDict


class MyNet(nn.Module):
    def __init__(self):
        super(MyNet, self).__init__()

        # æ–¹æ³•1ï¼šæœ€å¸¸ç”¨
        self.fc1 = nn.Linear(10, 10)
        # æ–¹æ³•2ï¼šåŠ¨æ€å‘½åï¼Œå°½é‡åˆ«ç”¨
        self.add_module("fc2", nn.Linear(10, 10))
        # æ–¹æ³•3ï¼š
        self.block3 = nn.Sequential(OrderedDict([
            ("conv1", torch.nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(5, 5))),
            ("relu1", nn.ReLU())
        ]))
        # æ–¹æ³•4ï¼š
        self.add_module("block4", nn.Sequential(OrderedDict([
            ("conv1", torch.nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(5, 5))),
            ("relu1", nn.ReLU())
        ])))

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        # forward ä¸­ä¸åŠ å…¥å°±ä¸èµ·ä½œç”¨
        # x = self.block3(x)
        # x = self.block4(x)
        return x


my_net = MyNet()
print(my_net)
```

å¦‚ä½•ä½¿ç”¨æ¨¡å‹
```python
x = torch.randn(8, 10)

y = my_net(x)
```


### ä¿å­˜å’Œè½½å…¥æ¨¡å‹

```python
# å­˜
torch.save(my_net.state_dict(),'./model.pkh')

my_net1 = MyNet().to(device)
# è¯»
my_net1.load_state_dict(torch.load('./model.pkh', weights_only=True))
# weights_only åªåŠ è½½æƒé‡ï¼Œæ›´å®‰å…¨

# å…³äº device çš„è¯´æ˜ï¼š
# 1. state_dict æ—¶ï¼Œä¼šè¿åŒæ‰€åœ¨è®¾å¤‡å·ä¸€èµ·ä¿å­˜ï¼›åœ¨ load æ—¶ï¼Œä¼š load åˆ°å¯¹åº”çš„è®¾å¤‡ä¸Š
# 2. ä½¿ç”¨ map_location å¯ä»¥æŒ‡å®š load åˆ°å“ªä¸ªè®¾å¤‡ä¸Š
# 3. my_net1.load_state_dict(xxx) æ—¶ï¼Œä¼šåŠ è½½åˆ° my_net1 æ‰€åœ¨çš„è®¾å¤‡ä¸Šã€‚æ— è®º load æ—¶è¿™äº› tensor åœ¨å“ªä¸ªè®¾å¤‡ä¸Š

# æŒ‡å®šå‚æ•° load åˆ°å“ªä¸ªè®¾å¤‡ä¸Šï¼š
device3 = torch.device("cuda:3")
torch.load('model.pkh', map_location=device3, weights_only=True)

# æˆ–è€…
troch.load('model.pkh', map_location={"cuda:1":"cuda:0"})
```

è¿åŒè®­ç»ƒçŠ¶æ€ä¸€èµ·ä¿å­˜ï¼Œä»¥ä¾¿æ–­ç‚¹ç»­è®­

```python
# ä¿å­˜
ckpt = {
    "epoch": epoch,
    "model_state": model.state_dict(),
    "optim_state": optimizer.state_dict(),
    # å¯é€‰ï¼š
    # "sched_state": scheduler.state_dict(),
    # "scaler_state": scaler.state_dict(),   # AMP ç”¨
}
torch.save(ckpt, "ckpt.pth")

# åŠ è½½
model = MyNet().to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

ckpt = torch.load("ckpt.pth", map_location=device)
model.load_state_dict(ckpt["model_state"])
optimizer.load_state_dict(ckpt["optim_state"])

start_epoch = ckpt["epoch"] + 1
model.train()
for epoch in range(start_epoch, num_epochs):
    ...
```


safetensors: æ›´å¿«ã€æ›´å®‰å…¨ã€æ›´æ ‡å‡†çš„ tensor ä¿å­˜æ ¼å¼

```python
# pip install safetensors

# ä¿å­˜
from safetensors.torch import save_file

state = my_net.state_dict()
save_file(state, "model.safetensors")

# è¯»å–
from safetensors.torch import load_file

state = load_file("model.safetensors", device='cuda:1')  # é»˜è®¤åœ¨ CPUï¼Œå¯ä»¥æŒ‡å®šè®¾å¤‡
my_net1 = MyNet()
my_net1.load_state_dict(state)
```



### è®­ç»ƒé…ç½®çš„å¼€å¯å’Œæ§åˆ¶

```python
# åˆ‡æ¢åˆ°è®­ç»ƒæ¨¡å¼ï¼šå¼€å¯ Dropoutã€BatchNorm ç­‰
model.train()

# åˆ‡æ¢åˆ°è¯„ä¼°/æ¨ç†æ¨¡å¼ï¼šå…³é—­ Dropoutï¼ŒBatchNorm ç”¨è®­ç»ƒæ—¶è®°å½•çš„
model.eval()

# ä¸è®°å½•è®¡ç®—å›¾ã€ä¸è®¡ç®—æ¢¯åº¦ï¼Œç”¨äºè¯„ä¼°/æ¨ç†
with torch.no_grad():
    ...
```



å…³äº `with torch.no_grad():`
```python
x = torch.randn(3, requires_grad=True)
y = x * x
print(y.requires_grad) # True
with torch.no_grad():
    print(y.requires_grad) # True
    y2 = x * x
    print(y2.requires_grad) # False

print(y.requires_grad) # True
print(y2.requires_grad) # False
```
æˆ–è€…ä½¿ç”¨ `y = x.detach()` ç”Ÿæˆçš„æ˜¯æ•°æ®å…±äº«å†…å­˜ï¼Œä½†æŒ‡å®šä¸å¾®åˆ†çš„æ–° Tensor


### AUTOGRAD

**requires_grad** ç”¨æ¥æ§åˆ¶å…¶æ˜¯å¦äº§ç”Ÿæ¢¯åº¦
```python
# æ–¹æ³•1
x = torch.ones(2, 2, requires_grad=True)
# requires_grad é»˜è®¤ä¸º False

# æ–¹æ³•2
x = torch.ones(2, 2)
x.requires_grad = True
```


```python
y = x + 2
# y.requires_grad ä¸º True
# y.grad_fn éç©º
```


æ¡ˆä¾‹ï¼š

```py
y = x + 2
# åªè¦ x.requires_grad == Trueï¼Œé‚£ä¹ˆå¯¹ x çš„å¤§å¤šæ•°è¿ç®—éƒ½ç¬¦åˆï¼š
# y.requires_grad ä¸º True
# y.grad_fn éç©º
z = y * y * 3
k = z.mean()
# åŒä¸Šï¼Œz.requires_grad å’Œ k.requires_grad éƒ½æ˜¯ True
```

- `x.requires_grad` å¦‚æœè®¾å®šä¸º Tureï¼Œåé¢ç”¨åˆ°yçš„å˜é‡éƒ½ä¼šè®¡ç®—æ¢¯åº¦
- `y.grad_fn` æ˜¯ä¸æ¢¯åº¦è®¡ç®—æœ‰å…³çš„å‡½æ•°


```python
# æ±‚å¯¼æ•°
k.backward() # å¼€å§‹è®¡ç®—æ¢¯åº¦
print(x.grad) # è¿”å›æ¢¯åº¦å€¼
# 1. è¿”å› dk/dx_ijï¼Œå½¢å¼æ˜¯çŸ©é˜µ
# 2. åªä¼šç»™å‡ºå¶å­çš„å¯¼æ•°ã€‚æ‰€ä»¥ z.grad æ²¡æœ‰å€¼ã€‚å› ä¸ºè®¡ç®—è¿™ä¸ªå€¼å¾€å¾€æ„ä¹‰ä¸å¤§
# 3. å¦‚æœç¡®å®éœ€è¦ z.gradï¼Œæå‰å£°æ˜ z.retain_grad()

x.is_leaf, y.is_leaf # (True, False)
```

è¦ç‚¹ï¼š
- `k.backward()` åªèƒ½è¿è¡Œä¸€æ¬¡
- `k.backward(retain_graph=True)` å¯ä»¥è¿è¡Œå¤šæ¬¡ï¼Œä½†ç»“æœä¸ºä¸Šä¸€æ¬¡çš„ç´¯åŠ ï¼Œï¼ˆä¼šå¯¼è‡´å†…å­˜å ç”¨è¾ƒå¤šï¼Ÿï¼‰

å¼€å…³æ¢¯åº¦è®¡ç®—

```python
x = torch.ones(2, 2, requires_grad=True)

# æ–¹æ³•1. ä½¿ç”¨ with
with torch.no_grad():
    y = x + 2

print(y.requires_grad)  # False

# æ–¹æ³•2. ä½¿ç”¨è£…é¥°å™¨
@torch.no_grad()
def func(x):
    return x + 2


y = func(x)
print(y.requires_grad)  # False

# ç›¸åçš„è¿ç®—æ˜¯ torch.enable_grad()ï¼Œå¯ä»¥åµŒå…¥åˆ° no_grad() ä»£ç å—é‡Œé¢
# å¦‚æœ x æœ¬èº«æ²¡æœ‰è®¾ç½® requires_grad=Trueï¼Œé‚£ä¹ˆå³ä½¿ enable_grad()ï¼Œå®ƒä¹Ÿä¸ç”Ÿæ•ˆ

# æ–¹æ³•3. å…¨å±€æ‰“å¼€/å…³é—­
torch.set_grad_enabled(True)
torch.set_grad_enabled(False)
```


```python
# ???
# å¦‚æœ out æ˜¯ä¸€ä¸ª tensor æ•°æ®ç±»å‹çš„çŸ¢é‡ï¼Œé‚£ä¹ˆè¿™æ ·
v = torch.tensor([1.0, 2, 3])
y.backward(v)
# ï¼Ÿï¼Ÿï¼Ÿä½†æˆ‘æ²¡ææ¸…æ¥šè¿™ä¸ªå¯¹åº”å“ªä¸ªæ•°å­¦å…¬å¼
```



### æ­£åˆ™åŒ–


**l2æ­£åˆ™åŒ–**

```python
# l2 æ­£åˆ™åŒ–é€‚åˆç”¨ AdamWï¼Œå¹¶ä¸” weight_decay å°±æ˜¯ l2 æ­£åˆ™åŒ–çš„å‚æ•°
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)


# ç¬¬äºŒç§å†™æ³•ï¼ˆå¦‚æœæƒ³è¦æ›´ç²¾ç»†æ§åˆ¶ï¼‰ï¼š
# l2 æ­£åˆ™åŒ–å»ºè®®ä½œç”¨äº biasï¼Œå› æ­¤åˆ†ä¸¤ç»„
weight_p, bias_p = [], []
for name, p in my_net.named_parameters():
    if 'bias' in name:
        bias_p += [p]
    else:
        weight_p += [p]

optimizer = torch.optim.SGD([{'params': weight_p, 'weight_decay': 1e-5},
                             {'params': bias_p, 'weight_decay': 0}],
                            lr=1e-2,
                            momentum=0.9)

# l1 æ­£åˆ™åŒ–ä¹Ÿéœ€è¦æ‰‹åŠ¨å†™
```



**Dropout** ç”¨æ¥å‡è½» overfitting
```py
nn.Dropout(p=0.5) # p æ˜¯ä¸¢å¼ƒæ¦‚ç‡


# ä»¥ä¸‹ä¼šæŒ‰ channel ä¸¢å¼ƒæ•´æ¡ channelï¼š

# è¾“å…¥ (N, C, L) ï¼ŒéšæœºæŠŠæŸäº› æ•´æ¡é€šé“ C å…¨éƒ¨ç½® 0 ï¼ˆè¯¥é€šé“ä¸Šçš„æ‰€æœ‰ L ä½ç½®ä¸º0ï¼‰
nn.Dropout1d

# è¾“å…¥(N, C, H, W)ï¼ŒéšæœºæŠŠæŸäº› æ•´æ¡é€šé“ C å…¨éƒ¨ç½® 0 ï¼ˆè¯¥é€šé“çš„ HÃ—W å…¨éƒ¨ä¸º 0ï¼‰ã€‚å…¸å‹ç”¨åœ¨ï¼š2D CNNï¼ˆå›¾åƒï¼‰
nn.Dropout2d

# è¾“å…¥ (N, C, D, H, W)ï¼šéšæœºæŠŠæŸäº› æ•´å—é€šé“ C ç½® 0ï¼ˆè¯¥é€šé“çš„ DÃ—HÃ—W å…¨ä¸º 0ï¼‰ã€‚å…¸å‹ç”¨åœ¨ï¼š3D CNNï¼ˆè§†é¢‘ã€åŒ»å­¦å½±åƒä½“æ•°æ®ï¼‰
nn.Dropout3d
```




**Batch Normalization**

```py
nn.BatchNorm1d # é’ˆå¯¹ 2 ç»´æˆ–è€… 3 ç»´ï¼Œè¾“å…¥æ˜¯ (N, C), æˆ–è€… (N, C, L)
nn.BatchNorm2d # é’ˆå¯¹ 2 ç»´ï¼Œè¾“å…¥ (N, C, H, W)
nn.BatchNorm3d # é’ˆå¯¹ 3 ç»´ï¼Œè¾“å…¥ (N, C, D, H, W)
# å…¶ä¸­ï¼ŒN æ˜¯ batch size


torch.nn.BatchNorm1d(num_features=5, eps=1e-5
                     , momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None)
# num_featuresï¼šäºŒç»´(N, D) ä¸­çš„ Dï¼ŒBatchNorm2d ä¸­çš„ C
# epsï¼šé˜²æ²»é™¤0é”™è¯¯
# momentum åŠ¨æ€å‡å€¼å’ŒåŠ¨æ€æ–¹å·®æ‰€ç”¨çš„åŠ¨é‡
# affine è‡ªé€‚åº”è°ƒæ•´ gamma/beta å€¼ï¼Œè‹¥ä¸º False åˆ™ä¸ç”¨å®ƒä»¬
# track_running_stats=True ä¸ä½†è¿½è¸ªå½“æœŸçš„å‡å€¼å’Œæ–¹å·®ï¼Œè¿˜æ ¹æ®ä¹‹å‰æ‰¹æ¬¡åšè°ƒæ•´

```

$y = \dfrac{x - \mathrm{E}[x]}{\sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta$

BNä¸èƒ½ç´§è·Ÿç€dropoutï¼Œå¦åˆ™ä¼šæŠ–åŠ¨ä¸¥é‡


## æŸå¤±å‡½æ•°


1. äº¤å‰ç†µï¼Œé€‚ç”¨äºåˆ†ç±»ï¼Œ$H(p,q)=-\sum p(x)\log q(x)$
```py
criterion = nn.CrossEntropyLoss()
nn.CrossEntropyLoss(weight)  # åŠ æƒäº¤å‰ç†µï¼Œé’ˆå¯¹ç±»åˆ«æ•°é‡ä¸å‡è¡¡çš„æƒ…å†µ
```
2. MSEï¼Œé€‚ç”¨äºå›å½’ï¼Œ$MSE(y,\hat y)=1/n \sum (y-\hat y)^2$
```py
criterion = nn.MSELoss()
criterion = torch.nn.L1Loss() # è¿™ä¸ªå¯¹åº”çš„æ˜¯ MAEï¼Œå¹³å‡ç»å¯¹è¯¯å·®
```
3. è‡ªå®šä¹‰ã€‚ä¾‹å¦‚ï¼Œé¢„æµ‹é”€é‡æ—¶ï¼Œå¤šé¢„æµ‹ä¸€ä¸ªæŸå¤±1å…ƒï¼Œå°‘é¢„æµ‹1ä¸ªæŸå¤±10å…ƒã€‚  
$Loss(y,y')=\sum f(y_i,y_i')$,  
å…¶ä¸­ï¼Œ$$f(x,y)=\left\{\begin{array}{ccc}a(x-y)&x>y\\
b(y-x)&x\leq y\end{array}\right.$$
```py
def asym_l1_loss(y_pred, y_true, a=1.0, b=10.0):
    # over: y_pred > y_true
    over = (y_pred - y_true).clamp_min(0.0)
    under = (y_true - y_pred).clamp_min(0.0)
    return (a * over + b * under).mean()
```
4. å¤šæ ‡ç­¾äºŒåˆ†ç±»ä»»åŠ¡ä¸­çš„æŸå¤±å‡½æ•°ï¼Œæ¯ä¸ªé¢„æµ‹å€¼æœ‰ n ä¸ª 0/1
```python
nn.BCELoss
nn.BCEWithLogitsLoss
```



## ä¼˜åŒ–å™¨


![optimization1](/pictures_for_blog/optimization/optimization1.gif)

![optimization2](/pictures_for_blog/optimization/optimization2.jpg)


### torchä¸­çš„ä¼˜åŒ–å™¨
```python
optimizer = torch.optim.SGD(my_model.parameters(), lr=learning_rate)
optimizer = torch.optim.Adam(my_model.parameters(),weight_decay =1e-3) # weight_decay æ˜¯ L2 penalty
```

é€‚ç”¨æ€§
- SGD å¯¹å°ºåº¦å¾ˆæ•æ„Ÿï¼Œå› æ­¤å¿…é¡»åšæ ‡å‡†åŒ–



### å­¦ä¹ ç‡

å­¦ä¹ ç‡è¡°å‡
```python
# ç­‰é—´éš”è°ƒæ•´
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)
# æŒ‰æ­¥è°ƒæ•´
torch.optim.lr_scheduler.MultiStepLR

# æŒ‡æ•°è¡°å‡ lr * (gamma ** step)
torch.optim.lr_scheduler.ExponentialLR

# ä½™å¼¦é€€ç«ï¼Œå­¦ä¹ ç‡æŒ‰ä½™å¼¦è¡°å‡
torch.optim.lr_scheduler.CosineAnnealingLR

# æŒ‡æ ‡åœ¨è¿‘å‡ æ¬¡æ²¡æœ‰å˜åŒ–æ—¶ï¼Œè°ƒæ•´å­¦ä¹ ç‡ï¼Œæœ€å¸¸ç”¨
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau \
    (optimizer=optimizer
     , mode='min'
     , factor=0.5  # gamma
     , patience=5  # ç›‘æ§ä¸å†å‡å°‘/å¢åŠ çš„æ¬¡æ•°
     , verbose=True  # è§¦å‘è§„åˆ™åæ‰“å°
     , threshold=1e-4  # è§¦å‘è§„åˆ™çš„é˜ˆå€¼
     , threshold_mode='abs'  # è§¦å‘è§„åˆ™çš„è®¡ç®—æ–¹æ³•
     , cooldown=0  # è§¦å‘è§„åˆ™ååœæ­¢ç›‘æ§è¿™ä¹ˆå¤šæ¬¡
     , min_lr=0  # lræœ€å°æ˜¯è¿™ä¹ˆå¤š
     , eps=1e-8
     )

# è‡ªå®šä¹‰ å­¦ä¹ ç‡è¡°å‡
torch.optim.lr_scheduler.LambdaLR
```

å¦‚ä½•ä½¿ç”¨ï¼Ÿ
```python
# è®­ç»ƒé˜¶æ®µï¼Œå‰é¢ä¸€å †ä»£ç 
optimizer.step()
scheduler.step()
# åé¢ä¸€å †ä»£ç 
```

ç”»ä¸€ä¸ªå­¦ä¹ ç‡è¡°å‡å›¾

```python
optimizer = torch.optim.Adam(my_net.parameters(), lr=0.1)
# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)
scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.96)

lr_history = []
for i in range(100):
    scheduler.step()
    lr_history.append(optimizer.state_dict()['param_groups'][0]['lr'])
```




![learning_rate1](https://github.com/guofei9987/StatisticsBlog/blob/master/%E9%99%84%E4%BB%B6/tf/learning_rate1.png?raw=true)




## å…¶å®ƒå¸¸è§é—®é¢˜

```python

torch.get_default_dtype()
torch.set_default_dtype(torch.float16)


torch.ByteTensor([1.1, 2, 3]) # å¯¹äºå°æ•°ï¼Œä¼šå–æ•´ã€‚å¯¹äºæº¢å‡ºçš„æ•°ï¼ˆå¤§äº255æˆ–è´Ÿçš„ï¼‰ï¼Œä¼šèˆå¼ƒæº¢å‡ºä½æ•°
# ä½†æ˜¯ï¼Œå¦‚æœè¾“å…¥çš„æ˜¯Tensorï¼Œä¼šå¡æ­»ï¼Œè¿™ä¹ˆè§£å†³ï¼š
torch.tensor([1, 2, 3]).type(torch.int8)
```



**hook**

```python
# æ­£å‘æ—¶ä¼šè§¦å‘çš„hook
def func1(model,input,output):pass
my_net.register_forward_hook(func1)
# åå‘æ—¶ä¼šè§¦å‘çš„hook
def func2(model,grad_input,grad_output):pass
my_net.register_backward_hook(func2)
```

### æ˜¾å­˜ä¸å¤Ÿ

å¦‚æœæ¨¡å‹å¤ªå¤§ï¼Œä¸€ä¸ªbatchæœªå¿…èƒ½æ”¾è¿›å»æ˜¾å­˜ã€‚è§£å†³ï¼šæŠŠä¸€ä¸ª batch åˆ†å‰²è¿è¡Œï¼ŒæŠŠæ¢¯åº¦ç´¯ç§¯èµ·æ¥ï¼Œnæ¬¡åæ›´æ–°ä¸€æ¬¡

```python
accumulation_steps = 5  # ç´¯ç§¯5æ¬¡ï¼Œç„¶åæ›´æ–°ä¸€æ¬¡æƒé‡

for i in range(1000):
    loss = loss_func(pred, y)
    loss = loss / accumulation_steps
    loss.backward() # è®¡ç®— loss
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()  # å‚æ•°æ›´æ–°
        optimizer.zero_grad()  # æ¸…ç©ºæ¢¯åº¦
```

å¦ä¸€ç§æƒ…å†µï¼Œå¦‚æœæœ‰ä¸¤ä¸ªç‹¬ç«‹çš„å­ä»»åŠ¡ï¼Œå°½é‡ç‹¬è‡ªå‰å‘ä¼ æ’­
```py
# ä¸è¦è¿™æ ·ï¼š
loss = loss1 + loss2
loss.backward()

# è€Œæ˜¯è¿™æ ·ï¼š
loss = loss1 + loss2
loss1.backward()
loss2.backward()
```


