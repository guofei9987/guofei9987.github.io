---
layout: post
title: 文本分词并画词云.
categories: 趣文
tags:
keywords:
description:
---

<a href='https://github.com/guofei9987/Corpus_Python_Demo'><i class="fa fa-github fa-lg" style="color:#428BCA;">代码库</i></a>

```
$git clone git@github.com:guofei9987/Corpus_Python_Demo.git
```

## step1:用jieba分词

<a href='https://github.com/fxsjy/jieba'><i class="fa fa-github fa-lg" style="color:#428BCA;">jieba</i></a>


```Python
url='人民的名义 周梅森.txt'
content=open(url,'rb').read()
import jieba.posseg as pseg
words = pseg.cut(content)#<generator>
```

## step2:做一个简单的词频统计

```Python
word_dict={}
taboo_list=['不要','还要']#禁止统计列表
for word, flag in words:
    if flag=='x':#剔除虚词
        continue
    if len(word) in taboo_list:#剔除禁止列表中的词
        continue
    if len(word)==1:#剔除长度为1的词
        continue
    if word in word_dict.keys():
        word_dict[word]+=1
    else:
        word_dict[word]=1

#按照词频从大到小排序
dict1= sorted(word_dict.items(), key=lambda d:d[1], reverse = True)
```

## step3:用wordcloud画图

<a href='https://github.com/amueller/word_cloud'><i class="fa fa-github fa-lg" style="color:#428BCA;">wordcloud</i></a>


```
#下面用wordcloud这个包，画词云图
from wordcloud import WordCloud
wc=WordCloud(font_path='a.ttf')#,max_font_size=40)#这里网上下载一个中文字体，就可以支持中文了
wordcloud=wc.fit_words(dict(dict1))
```

图片可以保存到本地：
```
wc.to_file('hh.png')
```

也可以在屏幕上显示出来：
```
import matplotlib.pyplot as plt
plt.figure()
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()
```

## 结果

![k2](http://i.imgur.com/CDq5sDU.png)


## 进一步分析
### TF-IDF权重策略
有些常见词，比如“有的”，在所有文章中出现概率都比较高。  
TF-IDF策略是调整这种偏差的方案    

1. TF(Term Frequency)(词频)
$$TF_{ij}=\dfrac{n_{ij}}{\sum\limits_k n_{kj}}$$
- i：第i个Term
- j:第j个文本文件
分子是出第i个词在第j个文本文件中出现的次数。  
坟墓是第j个文本文件中，所有词的个数  

2. IDF(Inverse Document Ferquency)(逆向文件频率)  
$$IDF_i=\log\dfrac{\mid D\mid}{\mid\{ j:t_i \in d_j\}\mid}$$
- 分母是语料库中文件总数
- 分子是包含第i个词语的文件总数

$$TFIDF_{ij}=TF_{ij} * IDF_{ij}$$
这就排除了高频词  
