---
layout: post
title: 【EM算法】理论篇
categories:
tags: 2-1-有监督学习
keywords:
description:
order: 280
---



## 简介
EM算法(Expectation Maximization),是一种求解极大似然估计参数的算法。  
很多的机器学习或统计模型中，要用到最大似然估计。  
而最大似然估计的求参数过程牵涉到最优化问题。  
EM算法就是可以求解这一类问题的算法。  
可以应用于GMM，HMM等模型  


EM算法的每次迭代由两步组成：E步求期望，M步求极大。  


## EM算法
输入：观测变量数据Y, 隐含变量数据Z, 联合分布$P(Y,Z\mid \theta)$, 条件分布$P(Z\mid Y, \theta)$  
输出：模型参数$\theta$  


**step1** : 选择初始值$\theta_0$,开始迭代  
**step2** ：E步$Q(\theta,\theta_i)=E_Z[\log P(Y,Z\mid \theta)\mid Y,\theta_i]=\sum\limits_Z \log P(Y,Z \mid \theta)P(Z\mid Y,\theta_i)$  
**step3** : M步,$\theta_{i+1}=\arg\max\limits_\theta Q(\theta,\theta_i)$  
**step4** : 回到step2，直到达到收敛条件。  
收敛条件一般是$\mid\mid \theta_{i+1}-\theta_i\mid\mid<\varepsilon$,或者$\mid\mid Q(\theta_{i+1},\theta_i)-Q(\theta_i,\theta_i)\mid\mid<\varepsilon$  


## 说明
1. 参数初值可以任意选择，但EM算法对初值敏感
2. EM算法不能保证找到全局最优[^lihang]


## 参考资料
[^lihang]: [李航：《统计学习方法》](https://www.weibo.com/u/2060750830?refer_flag=1005055013_)  
[^wangxiaochuan]: [王小川授课内容](https://weibo.com/hgsz2003)  
