---
layout: post
title: ğŸ”¥ æ•°æ®æ¸…æ´—ä¸ç‰¹å¾å·¥ç¨‹
categories:
tags: 0x12_ç‰¹å¾å·¥ç¨‹
keywords:
description:
order: 100
---

## åˆæ­¥æ¸…æ´—

**ç¼ºå¤±å€¼**
1. ç›´æ¥ä½¿ç”¨ã€‚æœ‰äº›æ¨¡å‹æ”¯æŒç¼ºå¤±å€¼ï¼Œå¦‚å†³ç­–æ ‘
2. åˆ é™¤ç‰¹å¾ã€‚å¦‚æœæŸä¸ªç‰¹å¾å¤§å¤šæ•°éƒ½æ˜¯ç¼ºå¤±å€¼ï¼Œé‚£ä¹ˆå¯ä»¥åˆ é™¤è¿™ä¸ªç‰¹å¾
3. è¡¥å…¨
    - å‡å€¼å¡«å……ã€‚ç¼ºç‚¹æ˜¯å¡«å……çš„å€¼éƒ½ä¸€æ ·
    - ä¼—æ•°å¡«å……
    - æ’å€¼æ³•å¡«å……
    - èšç±»ï¼Œç„¶ååŒç±»å‡å€¼æ’è¡¥
    - å»ºæ¨¡é¢„æµ‹ã€‚ç¼ºç‚¹æ˜¯ï¼šå¦‚æœç¼ºå¤±å±æ€§ä¸å…¶ä»–å±æ€§æ— å…³ï¼Œé‚£ä¹ˆé¢„æµ‹ç»“æœæ— æ„ä¹‰ã€‚å¦‚æœé«˜åº¦ç›¸å…³ï¼Œé‚£ä¹ˆå¯ä»¥åˆ é™¤ç‰¹å¾ã€‚  
    - é«˜ç»´æ˜ å°„ã€‚ä¼˜ç‚¹ï¼šæœ€å‡†ç¡®çš„åšæ³•ï¼Œå› ä¸ºå®Œå…¨ä¿ç•™äº†ä¿¡æ¯ï¼Œä¹Ÿä¸å¢åŠ ä»»ä½•ä¿¡æ¯ã€‚  
    - ç¼ºå¤±æ ‡è®°ï¼šé€‚ç”¨äºæ ·æœ¬é‡éå¸¸å¤§ã€ç¼ºå¤±çš„å˜é‡æ˜¯ç¦»æ•£çš„ï¼Œä¸”éå¸¸ç¨€ç–ã€‚ç¼ºå¤±å€¼ä¹Ÿå½“åšæŸç±»å–å€¼å¤„ç†ã€‚å®ƒç±»ä¼¼One-hot Encode



**å¼‚å¸¸å€¼**
- **æç«¯å€¼**æŒ‡çš„æ˜¯5 Sigmaä¹‹å¤–çš„å€¼
- **ç¦»ç¾¤å€¼**æŒ‡çš„æ˜¯3 Sigmaä¹‹å¤–çš„å€¼  
- ç¡®è®¤çš„æ–¹æ³•
    - ç”»boxå›¾æ£€æŸ¥
    - ç”¨5å€stdæ£€æŸ¥
- è§£å†³æ–¹æ³•
    - ç›–å¸½æ³•ã€‚æŠŠ3sigmaä¹‹å¤–çš„æ•°æ®å®šä¸ºsigma
    - **å–ln**ã€‚æœ‰äº›å¼‚å¸¸å€¼æ˜¯å› ä¸ºæ­¤å˜é‡æœä»å¯¹æ•°æ­£æ€åˆ†å¸ƒï¼Œç›´æ¥å–å¯¹æ•°å°±èƒ½è§£å†³ã€‚
        - ç°å®ä¸­çš„å¯¹æ•°æ­£æ€åˆ†å¸ƒä¹Ÿå¾ˆå¹¿æ³›ï¼ˆå¯èƒ½ä»…æ¬¡äºæ­£æ€åˆ†å¸ƒï¼‰
    - åˆ†ç±»å»ºæ¨¡ã€‚æŠŠå¹²æ‰°å˜é‡å˜æˆåˆ†ç±»å˜é‡ï¼ˆå¼‚å¸¸ä¸º1ï¼Œä¸å¼‚å¸¸ä¸º0ï¼‰  
    - ç¦»æ•£åŒ–ã€‚ä¾‹å¦‚åšæˆ é«˜ã€ä¸­ã€ä½ï¼Œä¸‰ç§å­—æ®µã€‚  
    - å‰”é™¤
        - å‰”é™¤æ•´è¡Œ
        - å‰”é™¤æ•´åˆ—

**å†—ä½™å€¼**
- drop_duplicates



## æè¿°æ€§ç»Ÿè®¡

| åç§° | ç‰¹ç‚¹ |
|------|-----|
| ä¼—æ•° | ä¸å—æç«¯å€¼å½±å“ã€ä¸å”¯ä¸€ã€å¯ç”¨äºåˆ†å¸ƒæœ‰åçš„æƒ…å†µ |
| ä¸­ä½æ•° | ä¸å—æç«¯å€¼å½±å“ã€å¯ç”¨äºåˆ†å¸ƒæœ‰åçš„æƒ…å†µ |
| å¹³å‡å€¼ | æ˜“å—æç«¯å€¼å½±å“ã€æ•°å­¦æ€§è´¨è¾ƒå¥½ã€é€‚ç”¨äºæ— åçš„æƒ…å†µ |

<br>


| æ•°æ®ç±»å‹ | é€‚ç”¨å“ªäº› |
|---------|---------|
| åˆ†ç±»æ•°æ® | **ä¼—æ•°** |
| é¡ºåºæ•°æ® | **ä¸­ä½æ•°**ã€å››åˆ†ä½æ•°ã€ä¼—æ•°|
| é—´éš”æ•°æ® | **å¹³å‡æ•°**ã€å››åˆ†ä½æ•°ã€ä¼—æ•°|
| è¿ç»­æ•°æ® | **å¹³å‡æ•°**ã€è°ƒå’Œå¹³å‡ã€å‡ ä½•å¹³å‡ã€ä¸­ä½æ•°ã€å››åˆ†ä½æ•°ã€ä¼—æ•°|

å…¶å®ƒ
- å…¨è·
- å››åˆ†ä½è·
- æ–¹å·®
- æ ‡å‡†å·®
- ååº¦
- å³°åº¦
- å˜å¼‚ç³»æ•°


### æè¿°æ€§ç»˜å›¾

**å•å˜é‡**

| æ•°æ®ç±»å‹ | é€‚ç”¨å“ªäº› |
|---------|---------|
| åˆ†ç±»æ•°æ® | **é¥¼å›¾** |
| é¡ºåºæ•°æ® | **æ¡å½¢å›¾** |
| é—´éš”æ•°æ® | **æ¡å½¢å›¾** |
| è¿ç»­æ•°æ® | **ç›´æ–¹å›¾**ã€**ç®±å›¾**|


**åŒå˜é‡X-Y**

|X\Y|å¤šåˆ†ç±»|è¿ç»­ |
|---|-----|----|
|å¤šåˆ†ç±»|çŸ©å½¢å›¾|ç®±å›¾|
|è¿ç»­|ç®±å›¾|scatter|


## æ¨¡å‹åé¦ˆ

1. æ•°æ®æ¸…æ´—æœ‰æ²¡æœ‰é—®é¢˜
2. æ•°æ®æŠ½æ ·æœ‰æ²¡æœ‰é—®é¢˜
3. æ•°æ®ç†è§£æœ‰æ²¡æœ‰é—®é¢˜
    - ä¸»æˆåˆ†åˆ†æçœ‹ä¸€ä¸‹
    - èšç±»çœ‹ä¸€ä¸‹
4. æ¨¡å‹é€‰æ‹©æœ‰æ²¡æœ‰é—®é¢˜
5. å‚æ•°è°ƒæ•´æœ‰æ²¡æœ‰é—®é¢˜




## æ•°æ®ä¸å‡è¡¡


**Unbalanced Data** èƒŒæ™¯å°±ä¸è¯´äº†ï¼Œæ•°æ®ä¸å‡è¡¡æ˜¯å¸¸æ€ï¼Œåšåˆ†ç±»æ¨¡å‹æ—¶ä¸å¾—ä¸è®¤çœŸå¤„ç†ã€‚  
åŸºæœ¬ä¸Šæœ‰è¿™å‡ ä¸ªç­–ç•¥ï¼š
1. å¢åŠ æ•°æ®ã€‚å¾ˆå¤šå…¶å®ƒé—®é¢˜ä¹Ÿéƒ½èƒ½ç”¨è¿™ä¸ªæ–¹æ³•è§£å†³ï¼Œä½†æˆæœ¬å¤ªé«˜ï¼Œä¸å¤šæäº†ã€‚
2. Oversamplingï¼Œæœ‰å¾ˆå¤šå˜ç§
3. Undersamplingï¼Œä¹Ÿæœ‰å¾ˆå¤šå˜ç§
4. åˆ’åˆ†è®­ç»ƒé›†
5. ç®—æ³•å±‚é¢
    - æ”¹å˜ cost function. sklearn å¾ˆå¤šæ–¹æ³•å¯ä»¥è¾“å…¥ weights å‚æ•°ï¼Œå®é™…ä¸Šå°±æ˜¯æ”¹å˜ cost functionï¼Œè®©ç¨€æœ‰ç±»çš„æƒé‡å¢åŠ ã€‚
    - ä½¿ç”¨å¯ä»¥å¤„ç†ä¸å¹³è¡¡é—®é¢˜çš„æ¨¡å‹ï¼Œå¦‚ æœ´ç´ è´å¶æ–¯ã€‚æœ‰ææ–™è¯´ å†³ç­–æ ‘ ä¹Ÿå¯ä»¥ï¼Ÿæˆ‘è¡¨ç¤ºæ€€ç–‘ï¼Œå›å¤´è¯•è¯•ã€‚
    - å´æ©è¾¾çš„ç¥ç»ç½‘ç»œè¯¾ç¨‹ä¸Šè®²è¿‡ä¸€ä¸ªæ–¹æ³•ã€‚å¯¹äºå¯ä»¥é‡å¤è®­ç»ƒçš„æ¨¡å‹ï¼ŒæŠŠå¤šæ•°ç±»åˆ†å‰²æˆå¤šä¸ªéƒ¨åˆ†ï¼Œæ¯éƒ¨åˆ†éƒ½å’Œç¨€æœ‰ç±»ç»“åˆèµ·æ¥å»è®­ç»ƒæ¨¡å‹ã€‚
    - ã€TODO: è¿™ä¸€éƒ¨åˆ†ä»¥åè¡¥å……ï¼Œå› ä¸ºå¾ˆæœ‰ç”¨ä¸”é‡è¦ã€‘è€ƒè™‘ä¸åŒè¯¯åˆ†ç±»æƒ…å†µä»£ä»·çš„å·®å¼‚æ€§å¯¹ç®—æ³•è¿›è¡Œä¼˜åŒ–ï¼Œä¸»è¦æ˜¯åŸºäºä»£ä»·æ•æ„Ÿå­¦ä¹ ç®—æ³•(Cost-Sensitive Learning)ï¼Œä»£è¡¨çš„ç®—æ³•æœ‰adacostï¼›
    - ä¸å¹³è¡¡æ•°æ®é›†çš„é—®é¢˜è€ƒè™‘ä¸ºä¸€åˆ†ç±»ï¼ˆOne Class Learningï¼‰æˆ–è€…å¼‚å¸¸æ£€æµ‹ï¼ˆNovelty Detectionï¼‰é—®é¢˜ï¼Œä»£è¡¨çš„ç®—æ³•æœ‰One-class SVMã€‚ `sklearn.svm.OneClassSVM`


### 2. over sampling
https://github.com/scikit-learn-contrib/imbalanced-learn  
https://pypi.org/project/imbalanced-learn/

#### æ•°æ®å‡†å¤‡
```python
from sklearn.datasets import make_classification
from collections import Counter

X, y = make_classification(n_samples=100, weights=[0.2, 0.8])
```

#### RandomOverSampler
ç®€å•çš„å¤åˆ¶
```python
from imblearn import over_sampling
osamp = over_sampling.RandomOverSampler(random_state=0)
X_resampled, y_resampled = osamp.fit_sample(X, y)
Counter(y_resampled)
```

- ä¼˜ç‚¹ï¼šç®€å•
- ç¼ºç‚¹ï¼šå°ä¼—æ ·æœ¬å¤åˆ¶å¤šä»½ï¼Œä¸€ä¸ªç‚¹ä¼šåœ¨é«˜ç»´ç©ºé—´ä¸­åå¤å‡ºç°ã€‚å¯¼è‡´è¿‡æ‹Ÿåˆï¼Œæˆ–è€…è¿æ°”å¥½å°±èƒ½åˆ†å¯¹å¾ˆå¤šç‚¹ï¼Œå¦åˆ™åˆ†é”™å¾ˆå¤šç‚¹ã€‚

#### SMOTE
```python
from imblearn import over_sampling
osamp = over_sampling.SMOTE(kind='regular',k_neighbors=2)
X_resampled, y_resampled = osamp.fit_sample(X, y)
Counter(y_resampled)
```
éšæœºé€‰å–å°‘æ•°ç±»çš„æ ·æœ¬ï¼Œåœ¨è¯¥æ ·æœ¬ä¸æœ€é‚»è¿‘çš„æ ·æœ¬çš„è¿çº¿ä¸Šéšæœºå–ç‚¹ï¼Œç”Ÿæˆæ— é‡å¤çš„æ–°çš„ç¨€æœ‰ç±»æ ·æœ¬ã€‚

- kind='regular'
- kind='borderline1', kind='borderline2'å…³æ³¨åœ¨æœ€ä¼˜åŒ–å†³ç­–å‡½æ•°è¾¹ç•Œçš„ä¸€äº›å±é™©æ ·æœ¬
- kind='svm' ä½¿ç”¨æ”¯æŒå‘é‡æœºåˆ†ç±»å™¨äº§ç”Ÿæ”¯æŒå‘é‡ç„¶åå†ç”Ÿæˆæ–°çš„å°‘æ•°ç±»æ ·æœ¬ã€‚


ç¼ºç‚¹ï¼š
- å¢åŠ äº†ç±»ä¹‹é—´é‡å çš„å¯èƒ½æ€§ï¼ˆç”±äºå¯¹æ¯ä¸ªå°‘æ•°ç±»æ ·æœ¬éƒ½ç”Ÿæˆæ–°æ ·æœ¬ï¼Œå› æ­¤å®¹æ˜“å‘ç”Ÿç”Ÿæˆæ ·æœ¬é‡å (Overlapping)çš„é—®é¢˜ï¼‰ï¼Œ
- ç”Ÿæˆä¸€äº›æ²¡æœ‰æä¾›æœ‰ç›Šä¿¡æ¯çš„æ ·æœ¬ã€‚


#### ADASYN

å…³æ³¨çš„æ˜¯åœ¨é‚£äº›åŸºäºKNNåˆ†ç±»å™¨è¢«é”™è¯¯åˆ†ç±»çš„åŸå§‹æ ·æœ¬é™„è¿‘ç”Ÿæˆæ–°çš„å°‘æ•°ç±»æ ·æœ¬

```python
from imblearn import over_sampling
osamp = over_sampling.ADASYN()
X_resampled, y_resampled = osamp.fit_sample(X, y)
Counter(y_resampled)
```

### 3. under sampling

```python
usap = under_sampling.RandomUnderSampler()
X_resampled, y_resampled = usap.fit_sample(X, y)
Counter(y_resampled)
```
åŠŸèƒ½æŒºå¤šï¼Œ`replacement = True` æœ‰æ”¾å›æŠ½æ ·ï¼Œç»“æœä¼šæœ‰é‡å¤ï¼Œé»˜è®¤ `replacement = False`

- å¯å‘å¼è§„åˆ™åšé™é‡‡æ ·  `under_sampling.NearMiss(version=1)` version = 1, 2, 3 æœ‰3ç§å¯å‘å¼è§„åˆ™
- ç”¨è¿‘é‚»ç®—æ³•è¿›è¡Œé™é‡‡æ · `under_sampling.EditedNearestNeighbours()` ç»å¤§å¤šæ•°(kind_sel='mode')æˆ–è€…å…¨éƒ¨(kind_sel='all')çš„è¿‘é‚»æ ·æœ¬éƒ½å±äºåŒä¸€ä¸ªç±»ï¼Œè¿™äº›æ ·æœ¬ä¼šè¢«ä¿ç•™åœ¨æ•°æ®é›†ä¸­
- åœ¨EditedNearestNeighboursåŸºç¡€ä¸Š, å»¶ä¼¸å‡ºäº†RepeatedEditedNearestNeighboursç®—æ³•å’ŒALLKNNç®—æ³•ï¼Œå‰è€…é‡å¤åŸºç¡€çš„EditedNearestNeighboursç®—æ³•å¤šæ¬¡ï¼Œåè€…åœ¨è¿›è¡Œæ¯æ¬¡è¿­ä»£çš„æ—¶å€™ï¼Œæœ€è¿‘é‚»çš„æ•°é‡éƒ½åœ¨å¢åŠ ã€‚
- `under_sampling.ClusterCentroids()` ä¸æ˜¯ä»åŸå§‹æ•°æ®é›†ä¸­ç›´æ¥æŠ½å–æ•°æ®ï¼Œæ¯ä¸€ä¸ªç±»åˆ«çš„æ ·æœ¬éƒ½ä¼šç”¨K-Meansç®—æ³•çš„ä¸­å¿ƒç‚¹æ¥è¿›è¡Œåˆæˆã€‚

### over Sampling + under sampling



æ•°æ®ä¸å‡è¡¡å‚è€ƒèµ„æ–™ï¼š
- https://www.cnblogs.com/kamekin/p/9824294.html
- https://www.jianshu.com/p/2149d94963cc  
- https://github.com/scikit-learn-contrib/imbalanced-learn



## æ ‡å‡†åŒ–

æ ‡å‡†åŒ–æ˜¯å¯¹åˆ—æ“ä½œ  

æ•°æ®å‡†å¤‡ï¼š
```py
import pandas as pd
import numpy as np
df = pd.DataFrame(np.random.uniform(size=(20, 3)), columns=list('abc'))
```

**StandardScaler**
```py
# ï¼ˆZ-Scoreï¼‰
# å…¬å¼ä¸ºï¼š(X-mean)/std è®¡ç®—æ—¶å¯¹æ¯ä¸ªå±æ€§/æ¯åˆ—åˆ†åˆ«è¿›è¡Œã€‚
from sklearn import preprocessing as preprocessing

standard_scaler = preprocessing.StandardScaler()
standard_scaler.fit_transform(df)
standard_scaler.fit(df)
standard_scaler.transform(df)
```



**MinMaxScaler** 0-1æ ‡å‡†åŒ–  
- ä½¿ç”¨è¿™ç§æ–¹æ³•çš„ç›®çš„åŒ…æ‹¬ï¼š  
    1. å¯¹äºæ–¹å·®éå¸¸å°çš„ feature å¯ä»¥å¢å¼ºå…¶ç¨³å®šæ€§ã€‚
    2. å¦‚æœæ˜¯æ­£çš„ç¨€ç–çŸ©é˜µï¼Œå¯ä»¥ç»´æŒä¸º0çš„æ¡ç›®ã€‚


```py
from sklearn import preprocessing
min_max_scaler = preprocessing.MinMaxScaler()
min_max_scaler.fit_transform(df)
min_max_scaler.fit(df)
min_max_scaler.transform(df)
```

**MaxAbsScaler** åˆ—é™¤ä»¥è¯¥åˆ—çš„æœ€å¤§å€¼ã€‚ä¼˜ç‚¹æ˜¯å¯ä»¥ä¿æŒç¨€ç–çŸ©é˜µä¸­çš„0ä¸å˜ã€‚
```py
from sklearn import preprocessing
max_abs_scaler = preprocessing.MaxAbsScaler()
max_abs_scaler.fit_transform(df)
max_abs_scaler.fit(df)
max_abs_scaler.transform(df)
```

**RobustScaler** ä¸€ç§å¯ä»¥é˜²æ­¢å¼‚å¸¸å€¼çš„Scalerã€‚ç®—æ³•æ˜¯å‡å»0.5åˆ†ä½æ•°ï¼Œç„¶åé™¤ä»¥0.75-0.25æå·®ã€‚
```py
from sklearn import preprocessing
robust_scaler=preprocessing.RobustScaler(with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0))
robust_scaler.fit_transform(df)
robust_scaler.fit(df)
robust_scaler.transform(df)
```

**å…¶å®ƒ**
- ç§©æ ‡å‡†åŒ–ï¼šæŠŠæ•°æ®æ’åºï¼Œç„¶åè®°ä¸‹å…¶åºå·ï¼Œç„¶åè½¬åŒ–åˆ° `[0,1]` çš„æ•°æ®
- å‡½æ•°æ ‡å‡†åŒ–ï¼šç”¨ $x'=1/x$ã€$x=log(x)$ ç­‰è½¬åŒ–ã€‚é€‚ç”¨äºä½ çŸ¥é“å…¶åˆ†å¸ƒ/æ•°æ®æƒ…å†µ
- 


## æ­£åˆ™åŒ–ï¼ˆNormalizationï¼‰
æ¯ä¸ªæ ·æœ¬å˜æˆå•ä½èŒƒæ•°

```py
from sklearn import preprocessing
normalizer = preprocessing.Normalizer(norm='l2') # l1, l2, max

normalizer.fit(df)
normalizer.transform(df)
```


## OneHotEncoder
```py
from sklearn import preprocessing
import pandas as pd
df = pd.DataFrame(np.random.randint(low=0, high=3, size=(10, 3)), columns=list('abc'))

onehotencoder = preprocessing.OneHotEncoder()
onehotencoder.fit(df)

onehotencoder.n_values_  # æ¯ä¸ªfeatureæœ‰å¤šå°‘ç§valueï¼Œä¾‹å¦‚ array([3, 3, 3])
onehotencoder.feature_indices_  # ç¬¬iä¸ªfeatureè¢«æ˜ å°„åˆ° feature_indices_[i] to feature_indices_[i+1]
onehotencoder.transform(df).toarray()  # OneHotEncoder çš„ç»“æœ
```

## PolynomialFeatures
åœ¨è¾“å…¥æ•°æ®å­˜åœ¨éçº¿æ€§ç‰¹å¾æ—¶ï¼Œè¿™ä¸€æ“ä½œå¯¹å¢åŠ æ¨¡å‹çš„å¤æ‚åº¦ååˆ†æœ‰ç”¨ã€‚  
ä¾‹å¦‚ï¼Œç‰¹å¾å‘é‡X=(X1, X2)ï¼Œæœ€é«˜é¡¹æ¬¡æ•°ä¸º2ï¼Œè¢«è½¬åŒ–ä¸º(1, X1, X2, X1^2, X1X2, X2^2)  
å¦‚æœæœ€é«˜ä¸ºnæ¬¡æ–¹ï¼Œæœ‰mä¸ªfeatureï¼Œé‚£ä¹ˆè½¬åŒ–åå¾—åˆ°comb(n+m,m)ä¸ªfeature
ï¼ˆå› ä¸º$(\sum_{i=1}^m x_i)^n$å±•å¼€åæœ‰comb(m+n+1,n)é¡¹ï¼‰  

```py
from sklearn import preprocessing

polynomial_features = preprocessing.PolynomialFeatures(degree=2) # æœ€é«˜é¡¹çš„æ¬¡æ•°ä¸º2
# interaction_only=True åªäº§ç”Ÿç›¸äº’ä½œç”¨é¡¹
polynomial_features.fit_transform(df)
polynomial_features.fit(df)
polynomial_features.transform(df)
```
## FunctionTransformer
è‡ªå®šä¹‰é¢„å¤„ç†å™¨
```py
from sklearn import preprocessing
function_transformer=preprocessing.FunctionTransformer(lambda x:np.log(x+100))
function_transformer.fit_transform(df)
function_transformer.fit(df)
function_transformer.transform(df)
```

## å‚è€ƒèµ„æ–™
- [æèˆªï¼šã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹](https://www.weibo.com/u/2060750830?refer_flag=1005055013_)  
- [å…³äºä½¿ç”¨sklearnè¿›è¡Œæ•°æ®é¢„å¤„ç†](http://www.cnblogs.com/chaosimple/p/4153167.html)  
