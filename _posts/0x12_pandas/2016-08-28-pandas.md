---
layout: post
title: 【pandas】语法速查
categories:
tags: 0x12_特征工程
keywords:
description:
order: 101
---

```python
import pandas as pd
```



**pd.set_option**

```py
pd.set_option('display.max_columns',5000)
pd.set_option('display.max_columns', None) # 显示所有的列
pd.set_option('display.width',100000)
pd.set_option('display.max_rows', None) # 显示所有行
pd.set_option('display.max_colwidth',100) # 有时候一个单元格里面的内容太长，超过上限会不显示并加上省略号
```



## 创建


- 按列创建
```python
df = pd.DataFrame({'col1': [1] * 9, 'col2': ['one', 'tow', 'three'] * 3}, index=range(9))
```
- 从数据集创建
```py
df = pd.DataFrame([[1, 2], [3, 4], [5, 6]], index=range(3), columns=['col1', 'col2'])
```
- 按行创建
```python
df = pd.DataFrame(
    data=[{'col1': 1, 'col2': 1}, {'col1': 2, 'col2': 2}, {'col1': 3, 'col2': 3}, {'col2': 4}]
    , index=['idx1', 'idx2', 'idx3', 'idx4'], columns=['col1', 'col2']
)
# 当字段名对不上时，会按照指定来，并最终保证最终的 df 与 columns 一致
```

## 与各种数据类型的交互

**Python 基本数据类型**
```python
df.to_dict(orient='list')
# {'col1': [数据], 'col2': [数据]}


df.to_dict(orient='records')
# [{'col1': 1, 'col2': 'one'},
# {'col1': 1, 'col2': 'tow'}]

# orient 可以是："dict", "list", "series", "split", "tight", "records", "index"
```

**Excel**
```python
df.to_excel('xlsx.xlsx', sheet_name='sheet1', index=False)
# index=False 不保存行索引
df1 = pd.read_excel('xlsx.xlsx')

# 写入多个 sheet：
with pd.ExcelWriter('xlsx.xlsx') as writer:
    df.to_excel(writer, sheet_name='sheet1', index=False)
    df.to_excel(writer, sheet_name='sheet2', index=False)
```

**CSV**
```python
df.to_csv("csv.csv", index=False)
df1 = pd.read_csv("csv.csv")
# 可以自定义 sep、names 等，还可以设置读取开始/结束位置，不常用，不记了
```

**Json**
```python
df.to_json('json.json', orient='records', force_ascii=False)
# [{'col1': 1, 'col2': 'one'},
# {'col1': 1, 'col2': 'tow'}]
df1 = pd.read_json('json.json', orient='records')

# 注：
# force_ascii=False 类似 json 中的 ensure_ascii
# orient 还可以是: "split", "records", "index", "table", "columns", "values"
```



**其它**
```py
# 剪贴板
read_clipboard
to_clipboard

to_panel
to_period # 把时间序列数据，变成频率数据
to_latex
to_html
to_string
to_pickle # 存到内存中
to_sql # 也挺有用，在另一篇博客里详解
```


## 按行筛选


```python
import pandas as pd

df = pd.DataFrame({'col1': [1, 2, 3, 4, 5, 6], 'col2': ['one', 'two', 'three'] * 2}, index=list('abcdef'))
```

**切片筛选数据**

- loc：指定 index 名和 columns 名，以选取行列（**含头又含尾**）
```python
df.loc['a':'b', 'col1']  # 选取1列的数据，返回Series
df.loc[:'b', ['col1']]  # 返回DataFrame
df.loc['a':'b', :]  # 选取ab两行数据，返回DataFrame
```
- iloc：基于序号选取行列（**含头不含尾**）
```python
df.iloc[1, 1]  # 从0开始计数，返回的是这个元素本身的类型
df.iloc[0:2, 1:2]  # 返回DataFrame
df.iloc[0:2, :]  # 返回DataFrame
df.iloc[:, 1]  # 选取所有记录的第一列的值，返回的为一个Series
df.iloc[1, :]  # 选取第一行数据，返回的为一个Series
```
- 想要一部分指定名字，另一部分指定序号，只好这样：
```python
df.iloc[:5, :].loc[:, ['col1']]
```

**按照条件筛选数据**

（方法太多了，记一个最通用的方法即可，更多的参见[这里](https://www.guofei.site/2017/10/15/pandas2.html)）


```python
def filter_func(x: pd.core.series.Series) -> bool:
    return x['col1'] > 2 and x['col2'] in {'one', 'three'}


# mask 是一个 Series，其内容是 bool 类型，表示每一行是否满足条件
mask = df.apply(filter_func, axis=1)

df[mask]
```

## 重复数据

```python
df.drop_duplicates()
df.drop_duplicates(subset=['col1'], keep='last')
# keep='first','last'
```

或者
```python
mask = df.duplicated(subset=['col1'], keep='last')
df[mask]
```

## 空数据

**删除空数据**
```python
dropna(how='any') # how='all'
```

**填充空数据**


可以向上填充/向下填充
```python
a=data.fillna(method='bfill',inplace=True)
# method :bfill,ffill,
```

也可以用值填充
```python
a=data.fillna(data.mean(),inplace=True)
```

值填充时，可以每列不一样
```py
df.fillna({'a':999,'b':888,'c':777,'d':666})
```

线性插值填充

```py
df1.interpolate()
```

线性插值填充：把index作为间隔

```py
df1.interpolate(method='index')
```

## apply


```python
def func(row: pd.core.series.Series):
    row['col1'] = float(row['col1'])
    row['col2_new'] = row['col2'] + '_' + str(row['col1'])
    return row


df1 = df.apply(func, axis=1)
```

## groupby

```python
df = pd.DataFrame({
    'col1': [1, 2, 3, 4, 5, 6]
    , 'col2': ['one', 'two', 'three'] * 2
    , 'col3': [1, 2] * 3})
```

groupby 的几种方式
```python
# groupby 一个字段
df.groupby('col1')

# groupby 多个字段
df.groupby(['col1', 'col2'])

# 可以是一个 list，其值表示分组号
df.groupby([[0, 0, 1, 1, 2, 2]])

# 可以是一个函数
df.groupby([lambda n: n % 3])

# 可以是它们的组合
df.groupby(['col2', lambda n: n % 3, [1, 2, 2, 2, 2, 2]])
```

**取出 group**
```python
# 1. len
len(df.groupby('col2'))

# 2. 迭代
for key, df_group in df.groupby(['col2', lambda n: n % 3]):
    print("key:", key)  # tuple，放置了此 group 的名字
    print(df_group)  # 此 group 的 DataFrame
    print("----")
```

**agg**

```python
def func(data: pd.core.series.Series):
    return data.max()


df.groupby(['col2']). \
    agg({'col1': ['mean', 'std', func]})
# 对 col1 列做 mean、std、自定义聚合


# 还可以对新列命名
df.groupby('col2'). \
    agg({
    'col1': [('新列名', 'mean'), ('std_of_col1', 'std'), ('max_of_col1', func)],
    'col3': [('mean_of_col3', 'count')]
})
```


## 多表合并
### 0. 数据准备

生成接下来要用到的几个表：  

```py
import pandas as pd
df1=pd.DataFrame({'A':['A0','A1','A2','A3'],
               'B':['B0','B1','B2','B3'],
               'C':['C0','C1','C2','C3'],
               'D':['D0','D1','D2','D3']},
               index=[0,1,2,3])

df2=pd.DataFrame({'A':['A4','A5','A6','A7'],
               'B':['B4','B5','B6','B7'],
               'C':['C4','C5','C6','C7'],
               'D':['D4','D5','D6','D7']},
               index=[4,5,6,7])

df3=pd.DataFrame({'A':['A8','A9','A10','A11'],
               'B':['B8','B9','B10','B11'],
               'C':['C8','C9','C10','C11'],
               'D':['D8','D9','D10','D11']},
               index=[8,9,10,11])
df4=pd.DataFrame({'B':['A8','A9','A10','A11'],
               'D':['B8','B9','B10','B11'],
               'F':['C8','C9','C10','C11']},
               index=[2,3,6,7])
```

### 1. pd.concat:以index或columns为合并条件

特点：匹配columns，匹配不到的的填入nan
```python
result=pd.concat([df1,df2,df3])
```

例子：  
```py
result=pd.concat([df1,df2,df3])
```

效果如下：  

![concat.jpg](/pictures_for_blog/postimg2/concat.jpg)


**1.1 axis：纵向合并还是横向合并**
```py
result = pd.concat([df1, df4], axis=1)
# axis='index'（默认）是纵向合并，（等价：axis=0）
# axis='columns' 是横向合并，（等价：axis=1）
```
效果如下：  

![concat3.jpg](/pictures_for_blog/postimg2/concat3.jpg)

**1.2 join**
join='outer'(默认),把所有未匹配到的也列出来，（上面这个案例）  
join='inner'，只列出左右两列都有的

```py
result = pd.concat([df1, df4], axis=1, join='inner')
```
效果如下：  

![concat4.jpg](/pictures_for_blog/postimg2/concat4.jpg)


**1.3 keys分组键**

要在相接的时候在加上一个层次的key来识别数据源自于哪张表，可以增加key参数  

```py
result = pd.concat([df1,df2,df3], keys=['x', 'y', 'z'])
#result=pd.concat({'x':df1,'y':df2,'z':df3})
#也可以用dict来做
```

效果如下：  

![concat2.jpg](/pictures_for_blog/postimg2/concat2.jpg)


**1.4 ignore_index**

如果两个表的index没什么实际含义，用ignore_index=True，使两个表对齐整理出一个新的index  

```py
result=pd.concat([df1,df4],ignore_index=True)
```

效果如下：  

![concat5.jpg](/pictures_for_blog/postimg2/concat5.jpg)

### 2. merge：以指定列为合并条件
除了简单合并外，有时需要匹配合并（类似SQL中的join命令）  
数据准备  
```py
import pandas as pd
left = pd.DataFrame({'key1': ['K0', 'K0', 'K1', 'K2'],
                    'key2': ['K0', 'K1', 'K0', 'K1'],
                    'A': ['A0', 'A1', 'A2', 'A3'],
                    'B': ['B0', 'B1', 'B2', 'B3']})


right = pd.DataFrame({'key1': ['K0', 'K1', 'K1', 'K2'],
                     'key2': ['K0', 'K0', 'K0', 'K0'],
                     'C': ['C0', 'C1', 'C2', 'C3'],
                     'D': ['D0', 'D1', 'D2', 'D3']})
```

**3.1 连接**
```pu
result = pd.merge(left, right, on=['key1', 'key2'])
```
![merge1.jpg](/pictures_for_blog/postimg2/merge1.jpg)

**3.2 how**

how='inner'(默认)  
how='left'  
how='right'  
how='outer'


```py
result = pd.merge(left, right, how='left', on=['key1', 'key2'])

```

![merge2.jpg](/pictures_for_blog/postimg2/merge2.jpg)


```py
result = pd.merge(left, right, how='right', on=['key1', 'key2'])
```
![merge3.jpg](/pictures_for_blog/postimg2/merge3.jpg)


```py
result = pd.merge(left, right, how='outer', on=['key1', 'key2'])
```
![merge4.jpg](/pictures_for_blog/postimg2/merge4.jpg)

**on**

有的时候，在左右表中，待匹配的列名不相同，分别指定左右表的列名就行了。    
- on
- left_on/right_on
- left_index/right_index: 用index作为左连接键/右链接键


```python
import pandas as pd
left = pd.DataFrame({'group': ['a', 'a', 'a', 'b','b', 'b', 'c', 'c','c'],'ounces': [4, 3, 12, 6, 7.5, 8, 3, 5, 6]})
right = pd.DataFrame({'label':['a','b','c'],'value':['alpha','beta','charlie']})
inner_joined = pd.merge(left, right, how='inner', left_on='group', right_on='label')
```                  




**suffix**

两个表的列名相同，但是意义不同。合并的时候想自动让他们重命名，然后保留下来。  

```
result = pd.merge(left, right, on='k', suffixes=['_l', '_r'])
```

![merge5.jpg](/pictures_for_blog/postimg2/merge5.jpg)


### 3. 加减乘除

```py
df1 + df2  
#+-*/
#//求商  %求余
```
不是合并。对应项相加减乘除。遇到index和column无法匹配的，填入NaN    

用函数功能更多
```py
df1.add(df2,fill_value=0)
#不再填入NaN，而是填入0
```

add,sub,div,mul  


## 后记

2017年记了 [极为详细的pandas使用笔记](https://www.guofei.site/pandas.html)  
回头看来，pandas 太杂乱，也太反直觉。因此这篇只保留最常用的。

另外，`DuckDB` 和 `polars` 都是比较好的替代品



