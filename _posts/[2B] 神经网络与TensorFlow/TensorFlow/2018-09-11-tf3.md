---
layout: post
title: 【TensorFlow3】激活函数
categories:
tags: 2-3-神经网络与TF
keywords:
description:
order: 283
---

## 激活函数
[更详细的内容见于TF官网](https://www.tensorflow.org/api_docs/python/tf/nn)  

### 激活函数
- ReLU  
ReLU(Rectifier Linear Unit),其函数为$max(0,x)$  
```py
tf.nn.relu
```
- ReLU6  
是 hard-sigmoid 的变种$min(max(0,x),6)$  
```py
tf.nn.relu6
```
- sigmoid
$1/(1+exp(-x))$
```py
tf.nn.sigmoid
```
- tanh
$\dfrac{1-e^{2x}}{1+e^{-2x}}$
```py
tf.nn.tanh
```
- softsign  
是符号函数的连续估计$x/(abs(x)+1)$  
```py
tf.nn.softsign
```
- softplus  
是ReLU的平滑版 $log(exp(x)+1)$
```py
tf.nn.softplus
```
- ELU(Exponential Linear Unit)  
类似于softplus  
```py
# 表达式为 (exp(x)+1) if x<0 else x
tf.nn.elu
tf.nn.softmax
```


|激励函数|优点|缺点|
|--|--|--|
|Sigmoid|不容易出现极端值|收敛速度慢
|ReLU|收敛速度快|容易出现极端值|


2019年5月22日更新（来自吴恩达的 DeepLearning.ai 课程）：  
sigmoid: never use, except output layer  
tanh: pretty much strictly superior then sigmoid  
ReLU: if you do not hnow which to choose, always choose ReLU  
Leaky ReLU： you may try this $max(0.01z,z)$

**About derivative**（自己推导一下）  
sigmoid:$g(x)=\dfrac{1}{1+e^{-z}},g'(z)=g(z)(1-g(z))$  
tanh:$g(x)=\dfrac{e^z-e^{-z}}{e^z+e^{-z}},g'(z)=1-(g(z))^2$  
ReLU/Leaky RelU： 分段函数，注意0点的情况 does not matter

### 卷积函数

```py
tf.nn.conv2d(input,filter,strides=[1,1,1,1],padding='SAME')
# 对四维数据进行二维卷积操作
# input: [batch, in_height, in_width, in_channels]
# filter: [filter_height, filter_width, in_channels, out_channels]
# strides: [1,stride_horizontal,,stride_vertices,1]
# padding='SAME'表示边界加上padding，使得卷积的输入和输出保持同样的尺寸
# padding='VALID' 表示不在边界上加padding


tf.nn.convolution(input,filter, padding,strides=None,dilation_rate=None,name=None,data_format=None)
# N维卷积的和

tf.nn.conv1d(value,filters,stride,padding,use_cudnn_on_gpu=None,data_format=None,name=None)
# 对三维数据进行一维卷积操作
# value:[batch,in_width,in_channels]
# filter:[filter_width,in_channels,out_channels]

tf.nn.conv3d(input, filter, strides, padding)
# input: [batch,in_depth,in_height,in_width,in_channels]
# filter: [filter_depth, filter_height, filter_width, in_channels, out_channels]
# strides: [1,stride1,stride2,stride3,1]

tf.nn.depthwise_conv2d(input, filter, strides, padding, rate=None, name=None, data_format=None)
# input :[batch,in_height,in_width,in_channels]
# filter :[filter_height,filter_width,in_channels,channel_multiplier]
# 输出是[batch, out_height, out_width, in_channels * channel_multiplier]

tf.nn.separable_conv2d(input,depthwise_filter,pointwise_filter,strides,padding,rat,name,data_format)

tf.nn.atrous_conv2d
tf.nn.conv2d_transpose
```

池化层
```py
tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME') #横竖步长为2，
# 每个输出元素是池化区域的最大值
tf.nn.avg_pool # 每个输出元素是池化区域的平均值
tf.nn.max_pool_with_argmax
# 返回最大值和最大值所在位置 (output,argmax),
# 其中output是每个池化区域的最大值。argmax是一个四维 <Targmax> 类型

tf.nn.avg_pool3d

```

### dropout
用来减轻overfitting
```py
keep_prob=tf.placeholder(tf.float32) # 训练时小于1，预测时等于1，所以使用placeholder
hidden1_drop=tf.nn.dropout(hidden1,keep_prob)
```



```py
tf.nn.lrn(pool1,4,bias=1,alpha=0.001/9.0,beta=0.75)
#LRN模仿生物的侧抑制机制，对局部神经元创建竞争环境，使其中响应大的值相对更大，并抑制其它反馈小的神经元。从而增强泛化能力
#可以用于Pooling之后，也可以用于conv之后、Pooling之前
#适用于ReLu这种没有上界的激活函数，不适合Sigmoid这种有固定边界，或者能抑制过大值得激活函数
```


## 参考文献
[TF官网-tf.nn](https://www.tensorflow.org/api_docs/python/tf/nn)  
《人工神经网络原理》马锐，机械工业出版社  
白话深度学习与TensorFlow，高扬，机械工业出版社  
《TensorFlow实战Google深度学习框架》，郑泽宇，中国工信出版集团  
