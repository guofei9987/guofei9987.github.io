---
layout: post
title: 【TensorFlow4】损失函数
categories:
tags: 2-3-神经网络与TF
keywords:
description:
order: 284
---


## 损失函数
#### 1.交叉熵
$H(p,q)=-\sum p(x)\log q(x)$

```py
cross_entropy=-tf.reduce_mean(y_*tf.log(tf.clip_by_value(y,1e-10,1.0)))
# y_代表正确结果，y代表预测结果
# tf.clip_by_value可以把张量中的数值限制在某个范围内,这里为了防止log0报错
# reduce_mean:大概是求均值？
```

TensorFlow对softmax+crossentropy进行了封装
```py
# cross_entropy=tf.nn.softmax_cross_entropy_with_logits(y,y_)

#  输入的 shape 应当是 (number of examples, num_classes)
logits = tf.transpose(Z3) # 注意，这里不是 y_hat
labels = tf.transpose(Y)

# 输出的(num_classes,)，你还要使用一次 tf.reduce_mean
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=labels))
```

#### 2.MSE
$MSE(y,y')=\dfrac{\sum (y-y')^2}{n}$
```py
mse=tf.reduce_mean(tf.square(y_-y))
```

#### 3.自定义
例如，预测销量时，多预测一个损失1元，少预测1个损失10元。  
$Loss(y,y')=\sum f(y_i,y_i')$,  
其中，$$f(x,y)=\left\{\begin{array}{ccc}a(x-y)&x>y\\
b(y-x)&x\leq y\end{array}\right.$$

```py
loss=tf.reduce_sum(tf.select(tf.greater(v1,v2),(v1-v2)*a,(v2-v1)*b))
```

#### 正则化项
写法1
```py
el_param1=tf.constant(1,dtype=tf.float32)
el_param2=tf.constant(1,dtype=tf.float32)
l1_term=el_param1*tf.reduce_sum(tf.abs(A))
l2_term=el_param2*tf.reduce_sum(tf.square(A))

loss=tf.reduce_sum(tf.square(y-y_hat))+l1_term+l2_term
```
写法2
```
tf.reduce_sum(tf.square(y-y_hat))\
+tf.contrib.layers.l1_regularizer(1.0)(A)\
+tf.contrib.layers.l2_regularizer(1.0)(A)
```

## 参考文献
[TF官网-tf.nn](https://www.tensorflow.org/api_docs/python/tf/nn)  
《人工神经网络原理》马锐，机械工业出版社  
白话深度学习与TensorFlow，高扬，机械工业出版社  
《TensorFlow实战Google深度学习框架》，郑泽宇，中国工信出版集团  
