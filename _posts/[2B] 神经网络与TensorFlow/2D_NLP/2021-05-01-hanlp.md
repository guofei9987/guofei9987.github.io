---
layout: post
title: 【NLP】hanlp
categories:
tags: 2-4-NLP
keywords:
description:
order: 341
---



## 介绍

相关项目
- HanLP：[https://github.com/hankcs/HanLP](https://github.com/hankcs/HanLP)
- 示例代码 Java：[https://github.com/hankcs/HanLP/tree/v1.7.5/src/test/java/com/hankcs/book](https://github.com/hankcs/HanLP/tree/v1.7.5/src/test/java/com/hankcs/book)
- 示例代码 Python：[https://github.com/hankcs/pyhanlp/tree/master/tests/book](https://github.com/hankcs/pyhanlp/tree/master/tests/book)


python版本安装
```sh
pip install pyhanlp

# 命令行试试（分词）
hanlp segment <<< '欢迎新老师生前来就餐'
```

Java
```xml
<dependency>
    <groupId>com.hankcs</groupId>
    <artifactId>hanlp</artifactId>
    <version>portable-1.7.5</version>
</dependency>
```






## 词典

加载词典
```Java
TreeMap<String, CoreDictionary.Attribute> dictionary =
    IOUtil.loadDictionary("data/dictionary/CoreNatureDictionary.mini.txt");

dictionary.size()
```

### 理论

给出了4种切分方法，[代码](https://github.com/hankcs/HanLP/blob/v1.7.5/src/test/java/com/hankcs/book/ch02/NaiveDictionaryBasedSegmentation.java)
- 完全切分：找到全部可能的词，保留在词典种的那些
- 正向最长匹配：从左到右做最长的匹配。bad case：商品, 和服, 务
- 反向最长匹配：从右往左。bad case：项, 目的, 研究
- 双向最长匹配。把正向和反向都做一遍，然后选出词数最少的（如果词数一样，选单字最少的）

上面的代码用的最粗暴的遍历，为了提升性能，显然有写做法：
- [BinTrie](https://github.com/hankcs/HanLP/blob/v1.7.5/src/test/java/com/hankcs/book/ch02/BinTrieBasedSegmentation.java)
- [DoubleArrayTrie](https://github.com/hankcs/HanLP/blob/v1.7.5/src/test/java/com/hankcs/book/ch02/DoubleArrayTrieBasedSegmentation.java)
- [AC自动机](https://github.com/hankcs/HanLP/blob/v1.7.5/src/test/java/com/hankcs/book/ch02/AhoCorasickSegmentation.java)
- [AC自动机+DoubleArrayTrie](https://github.com/hankcs/HanLP/blob/v1.7.5/src/test/java/com/hankcs/book/ch02/AhoCorasickDoubleArrayTrieSegmentation.java)


### ArrayTrie 分词

HanLP 对上面的算法做了封装，直接拿来用即可：**AC自动机+DoubleArrayTrie**
```Java
String dict1="src/test/CoreNatureDictionary.mini.txt"; // 用户可以自定义词典
HanLP.Config.ShowTermNature = false; //不显示词性
AhoCorasickDoubleArrayTrieSegment segment = new AhoCorasickDoubleArrayTrieSegment(dict1);
System.out.println(segment.seg("江西鄱阳湖干枯，中国最大淡水湖变成大草原"));
```
>[江西, 鄱阳湖, 干枯, ，, 中国, 最, 大, 淡水湖, 变成, 大, 草原]


还可以显示词性 **DoubleArrayTrie**
```Java
String dict1 = "src/main/data/dictionary/CoreNatureDictionary.mini.txt";
String dict2 = "src/main/data/dictionary/custom/上海地名.txt ns";
DoubleArrayTrieSegment segment = new DoubleArrayTrieSegment(dict1, dict2);

segment.enablePartOfSpeechTagging(true);    // 激活数词和英文识别
HanLP.Config.ShowTermNature = true;         // 显示词性


System.out.println(segment.seg("江西鄱阳湖干枯，中国最大淡水湖变成大草原"));
System.out.println(segment.seg("上海市虹口区大连西路550号SISU"));


// 显示每个词的词性：
for (Term term : segment.seg("上海市虹口区大连西路550号SISU")) {
    System.out.printf("单词:%s 词性:%s\n", term.word, term.nature);
}
```

### 停用词

[去除停用词](https://github.com/hankcs/HanLP/blob/v1.7.5/src/test/java/com/hankcs/book/ch02/DemoStopwords.java)


### 繁简体互转

https://github.com/hankcs/HanLP/blob/v1.7.5/src/test/java/com/hankcs/demo/DemoTraditionalChinese2SimplifiedChinese.java


```Java
System.out.println(HanLP.convertToTraditionalChinese("“以后等你当上皇后，就能买草莓庆祝了”。发现一根白头发"));
System.out.println(HanLP.convertToSimplifiedChinese("憑藉筆記簿型電腦寫程式HanLP"));
// 简体转台湾繁体
System.out.println(HanLP.s2tw("hankcs在台湾写代码"));
// 台湾繁体转简体
System.out.println(HanLP.tw2s("hankcs在臺灣寫程式碼"));
// 简体转香港繁体
System.out.println(HanLP.s2hk("hankcs在香港写代码"));
// 香港繁体转简体
System.out.println(HanLP.hk2s("hankcs在香港寫代碼"));
// 香港繁体转台湾繁体
System.out.println(HanLP.hk2tw("hankcs在臺灣寫代碼"));
// 台湾繁体转香港繁体
System.out.println(HanLP.tw2hk("hankcs在香港寫程式碼"));

// 香港/台湾繁体和HanLP标准繁体的互转
System.out.println(HanLP.t2tw("hankcs在臺灣寫代碼"));
System.out.println(HanLP.t2hk("hankcs在臺灣寫代碼"));

System.out.println(HanLP.tw2t("hankcs在臺灣寫程式碼"));
System.out.println(HanLP.hk2t("hankcs在台灣寫代碼"));
```



## 语言模型

一个句子出现的概率 $P(s) = p(w_1 w_2 ... w_n) = p(w_1\mid w_0) p(w_2\mid w_0 w_1)...p(w_{k+1}\mid w_0 w_1...w_k)$
- 其中 $w_0 = $ BOS （Begin Of Sentence）


上面的模型是极难计算的，因此做一些简化：
- 假设满足马尔可夫性，也就是说 $p(w_t\mid w_0 w_1...w_{t-1}) = P(w_t\mid w_{t-1})$ ，于是 $P(s) = p(w_1\mid w_0) p(w_2\mid w_1)...p(w_{k+1}\mid w_k)$，称为 **二元模型（bigram）**
- **n元语法（n-gram）** 指的是每个单词的概率取决于之前的 n-1 个单词
- **1元语法（unigram）** 其实就是记录每个单词的频率


平滑策略  
n 越大，越有可能稀疏，我们可以用线性插值来平滑它。  
例如 $p(w_t\mid w_{t-1})$ 很可能在语料中未出现，因此计算得到概率为0，但它的实际概率肯定不是0，这就是稀疏性。于是我们用一个平滑因子来平滑它 $p(w_t\mid w_{t-1}) := \lambda p(w_t\mid w_{t-1}) + (1-\lambda)p(w_t)$



### 分词语料库

- **PKU** 1998年《人民日报》语料库。误标率较高，不宜作为首选。
- **MSR** 微软亚洲研究院语料库。专有名词、姓名没有做切分，更符合认知



加载分词语料库
```Java
String MY_CWS_CORPUS_PATH = "src/main/data/test/my_cws_corpus.txt";

List<List<IWord>> sentenceList = CorpusLoader.convert2SentenceList(MY_CWS_CORPUS_PATH);
for (List<IWord> sentence : sentenceList) {
    System.out.println(sentence);
}
```

语料库的例子：
```txt
商品 和 服务
商品 和服 物美价廉
服务 和 货币
```

### Viterbi 分词训练和应用

```Java
public class DemoNgramSegment
{


    public static void main(String[] args)
    {
        String TRAIN_PATH = "src/main/data/test/icwb2-data/training/msr_training.utf8";
        String MSR_MODEL_PATH = "src/main/data/test/msr_cws_ngram";

//        训练模型
        trainBigram(TRAIN_PATH, MSR_MODEL_PATH);
//        加载训练好的模型
        Segment segment=loadBigram(MSR_MODEL_PATH);

//        模型的一些东西
        CoreDictionary.getTermFrequency("商品");
        CoreBiGramTableDictionary.getBiFrequency("商品", "和");


//        使用模型
        System.out.println(segment.seg("商品和服务"));


    }

    /**
     * 训练bigram模型
     *
     * @param corpusPath 语料库路径
     * @param modelPath  模型保存路径
     */
    public static void trainBigram(String corpusPath, String modelPath)
    {
        List<List<IWord>> sentenceList = CorpusLoader.convert2SentenceList(corpusPath);
        for (List<IWord> sentence : sentenceList)
            for (IWord word : sentence)
                if (word.getLabel() == null) word.setLabel("n"); // 赋予每个单词一个虚拟的名词词性
        final NatureDictionaryMaker dictionaryMaker = new NatureDictionaryMaker();
        dictionaryMaker.compute(sentenceList);
        dictionaryMaker.saveTxtTo(modelPath);
    }

    public static Segment loadBigram(String modelPath)
    {
        return loadBigram(modelPath, true);
    }

    /**
     * 加载bigram模型
     *
     * @param modelPath 模型路径
     * @param viterbi   是否创建viterbi分词器
     * @return 分词器
     */
    public static Segment loadBigram(String modelPath, boolean viterbi)
    {
        String MSR_MODEL_PATH = "data/test/msr_cws_ngram";

//        HanLP.Config.enableDebug();
        HanLP.Config.CoreDictionaryPath = modelPath + ".txt";
        HanLP.Config.BiGramDictionaryPath = modelPath + ".ngram.txt";

        return viterbi ? new ViterbiSegment().enableAllNamedEntityRecognize(false).enableCustomDictionary(false) :
            new DijkstraSegment().enableAllNamedEntityRecognize(false).enableCustomDictionary(false);
    }
}
```

训练过程：1）原理在上面贴了 2）保存下来的模型其实是如下的 2-gram 信息

```txt
橡胶@性能 1
橡胶@推进 2
...(几百万行)
```

预测过程：

![Viterbi](/pictures_for_blog/nlp/viterbi.jpeg)

算法
- 用正向贪心算法和负向贪心算法都不合适
- 用 Viterbi 算法。原理：（感觉就是动态规划）
    1. 正向，获取起点到 i 的最小花费
    2. 逆向，从终点向前回溯，得到最短路径

### 分词中集成用户词典



原因：实际应用中，往往遇到某一专业领域的词语，而对应的语料却不足。因此用户自己整理一个词典，来弥补通用语料的不足。

[代码](https://github.com/hankcs/HanLP/blob/v1.7.5/src/test/java/com/hankcs/book/ch03/DemoCustomDictionary.java)

```Java
// 关键点1：是否显示词性，默认为 true
HanLP.Config.ShowTermNature=true;
// 关键点2：加载用户词典（需要删除已缓存的同名的 bin 文件）
HanLP.Config.CustomDictionaryPath= new String[]{"src/main/data/dictionary/custom/CustomDictionaryF.txt"};  

Segment segment = new ViterbiSegment();
String sentence = "社会摇摆简称社会摇";
segment.enableCustomDictionary(false);
System.out.println("不挂载词典：" + segment.seg(sentence));

// 关键点3:动态增加用户词典
CustomDictionary.insert("社会摇", "nz 100");

// 低优先级：仍然考虑词图距离最短，也就是把用户词典也作为概率图的一部分
segment.enableCustomDictionary(true);
System.out.println("低优先级词典：" + segment.seg(sentence));

// 高优先级：强制优先考虑用户词典
segment.enableCustomDictionaryForcing(true);
System.out.println("高优先级词典：" + segment.seg(sentence));
```

>不挂载词典：[社会/n, 摇摆/vn, 简称/v, 社会/n, 摇/v]  
低优先级词典：[社会/n, 摇摆/vn, 简称/v, 社会摇/nz]  
高优先级词典：[社会摇/nz, 摆/v, 简称/v, 社会摇/nz]






额外:简单用法（一切用默认）
```java
System.out.println(HanLP.segment("王国维和服务员"));
# [王国维/nr, 和/c, 服务员/n]
```


### 模型调优

[混淆矩阵的计算](https://github.com/hankcs/HanLP/blob/v1.7.5/src/test/java/com/hankcs/book/ch03/EvaluateBigram.java)


[模型调优](https://github.com/hankcs/HanLP/blob/v1.7.5/src/test/java/com/hankcs/book/ch03/DemoAdjustModel.java)

- 增加关键词，例如姓名、机构名。加到 `data/dictionary/custom/CustomDictionary.txt` 会生成一个 bin 文件。 ？？？我在原项目中成功了，在独立项目中没成功








## 隐马尔可夫模型

问题：根据上面的用法，我们可以把新词作为词典放到模型中，但有时候新词是很难穷举的。

例如：在某个项目中，我们希望把 “北京大学口腔医院” 或 “北京大学口腔医学研究所” 之类作为组织机构名不切分。而这种词在语料中几乎不会出现，更不要说整理出一份列表。

我们还需要从统计学系的角度找解决方案。


“我参观了北京大学口腔医院。”
- 分词结果 “我, 参观, 了, 北京大学, 口腔, 医院”
- 期望结果 “我, 参观, 了, 北京大学口腔医院”


序列标注：
- 对一个序列上每一个位置，预测其标签。
- 词性标注：这个标签是 词性
- 命名实体识别：标签是 BMES（Begin，Middle，End，Single）

推导到这里，可以看出，此任务非常适合隐马尔可夫模型

![hmm](/pictures_for_blog/nlp/hmm.jpeg)

隐马尔可夫模型有三种用法：
1. 样本生成问题，给定模型 $\lambda = (\pi,A,B)$，生成一个样本序列 $(x_1, y_1),...(x_n,y_n)$
2. 模型训练问题，给定训练集，估计模型参数 $\lambda = (\pi,A,B)$
3. 序列预测问题，给定模型 $\lambda = (\pi,A,B)$，并给定观测序列 $x_1,..., x_n$，求出最可能的序列 $y_1,...,y_n$


[HanLP已有实现](https://github.com/hankcs/HanLP/blob/v1.7.5/src/main/java/com/hankcs/hanlp/model/hmm/FirstOrderHiddenMarkovModel.java)

使用 [例子](https://github.com/hankcs/HanLP/blob/v1.7.5/src/test/java/com/hankcs/hanlp/model/hmm/FirstOrderHiddenMarkovModelTest.java)


其父类实现了这些方法：
- generate：样本生成。用给定的 HMM，生成样本
- train：模型训练（因为是离散的，所以全都是用频率来估计概率）
    - estimateStartProbability
    - estimateTransitionProbability
    - estimateEmissionProbability
- similar：评估两个 HMM 模型是否相似。原理是判断是否每个参数都差不多。
- predict：序列预测，其实还是 Viterbi 算法

![hmm_viterbi](/pictures_for_blog/nlp/hmm_viterbi.jpeg)


### HMM的训练和预测

HanLP 在 `HMMSegmenter extends HMMTrainer` 这个类中已经整合了语料库格式转化、HMM训练、预测等，直接用就行了。

[原代码](https://github.com/hankcs/HanLP/tree/422077bddc59a66eaf9170198d5650ffc4686124/src/test/java/com/hankcs/book/ch04)


```java
import com.hankcs.hanlp.model.hmm.FirstOrderHiddenMarkovModel;
import com.hankcs.hanlp.model.hmm.HMMSegmenter;
import com.hankcs.hanlp.model.hmm.HiddenMarkovModel;
import com.hankcs.hanlp.model.hmm.SecondOrderHiddenMarkovModel;
import com.hankcs.hanlp.seg.common.CWSEvaluator;

import java.io.IOException;

public class CWS_HMM {
    public static void main(String[] args) throws IOException {
        trainAndEvaluate(new FirstOrderHiddenMarkovModel());
        trainAndEvaluate(new SecondOrderHiddenMarkovModel());
    }

    public static void trainAndEvaluate(HiddenMarkovModel model) throws IOException {

        // 训练
        String trainPath = "src/data/test/icwb2-data/training/msr_training.utf8";
        HMMSegmenter segmenter = new HMMSegmenter(model);
        segmenter.train(trainPath);

        // 使用
        System.out.println(segmenter.segment("商品和服务"));

        // 准召的评价
        String TEST_PATH = "src/data/test/icwb2-data/testing/msr_test.utf8";
        String OUTPUT_PATH = "src/data/test/msr_output.txt";
        String GOLD_PATH = "src/data/test/icwb2-data/gold/msr_test_gold.utf8";
        String TRAIN_WORDS = "src/data/test/icwb2-data/gold/msr_training_words.utf8";
        CWSEvaluator.Result result = CWSEvaluator.evaluate(segmenter.toSegment(), TEST_PATH, OUTPUT_PATH, GOLD_PATH, TRAIN_WORDS);
        System.out.println(result);
    }
}
```
>语料: 86k...[商品, 和, 服务]  
P:78.49 R:80.38 F1:79.42 OOV-R:41.11 IV-R:81.44  
语料: 86k...[商品, 和, 服务]  
P:78.34 R:80.01 F1:79.16 OOV-R:42.06 IV-R:81.04


准召的评价:
- HMM 的 OOV（Out Of Vocabulary ）大幅度提升到 40%，说明 40% 新词 被正确召回了
- 准召下降，说明出现了欠拟合





//        准召，可以看出，这个算法的 OOV（Out Of Vocabulary ）大幅度提升，说明









```python
from pyhanlp import HanLP

print(HanLP.segment("王国维和服务员"))
```
