---
layout: post
title: 【NLP】hanlp
categories:
tags: 2-4-NLP
keywords:
description:
order: 341
---



## 介绍

相关项目
- HanLP：[https://github.com/hankcs/HanLP](https://github.com/hankcs/HanLP)
- 示例代码 Java：[https://github.com/hankcs/HanLP/tree/v1.7.5/src/test/java/com/hankcs/book](https://github.com/hankcs/HanLP/tree/v1.7.5/src/test/java/com/hankcs/book)
- 示例代码 Python：[https://github.com/hankcs/pyhanlp/tree/master/tests/book](https://github.com/hankcs/pyhanlp/tree/master/tests/book)


python版本安装
```sh
pip install pyhanlp

# 命令行试试（分词）
hanlp segment <<< '欢迎新老师生前来就餐'
```

Java
```xml
<dependency>
    <groupId>com.hankcs</groupId>
    <artifactId>hanlp</artifactId>
    <version>portable-1.7.5</version>
</dependency>
```






## 词典

加载词典
```Java
TreeMap<String, CoreDictionary.Attribute> dictionary =
    IOUtil.loadDictionary("data/dictionary/CoreNatureDictionary.mini.txt");

dictionary.size()
```

### 理论

给出了4种切分方法，[代码](https://github.com/hankcs/HanLP/blob/v1.7.5/src/test/java/com/hankcs/book/ch02/NaiveDictionaryBasedSegmentation.java)
- 完全切分：找到全部可能的词，保留在词典种的那些
- 正向最长匹配：从左到右做最长的匹配。bad case：商品, 和服, 务
- 反向最长匹配：从右往左。bad case：项, 目的, 研究
- 双向最长匹配。把正向和反向都做一遍，然后选出词数最少的（如果词数一样，选单字最少的）

上面的代码用的最粗暴的遍历，为了提升性能，显然有写做法：
- [BinTrie](https://github.com/hankcs/HanLP/blob/v1.7.5/src/test/java/com/hankcs/book/ch02/BinTrieBasedSegmentation.java)
- [DoubleArrayTrie](https://github.com/hankcs/HanLP/blob/v1.7.5/src/test/java/com/hankcs/book/ch02/DoubleArrayTrieBasedSegmentation.java)
- [AC自动机](https://github.com/hankcs/HanLP/blob/v1.7.5/src/test/java/com/hankcs/book/ch02/AhoCorasickSegmentation.java)
- [AC自动机+DoubleArrayTrie](https://github.com/hankcs/HanLP/blob/v1.7.5/src/test/java/com/hankcs/book/ch02/AhoCorasickDoubleArrayTrieSegmentation.java)


### ArrayTrie 分词

HanLP 对上面的算法做了封装，直接拿来用即可：**AC自动机+DoubleArrayTrie**
```Java
String dict1="src/test/CoreNatureDictionary.mini.txt"; // 用户可以自定义词典
HanLP.Config.ShowTermNature = false; //不显示词性
AhoCorasickDoubleArrayTrieSegment segment = new AhoCorasickDoubleArrayTrieSegment(dict1);
System.out.println(segment.seg("江西鄱阳湖干枯，中国最大淡水湖变成大草原"));
```
>[江西, 鄱阳湖, 干枯, ，, 中国, 最, 大, 淡水湖, 变成, 大, 草原]


还可以显示词性 **DoubleArrayTrie**
```Java
String dict1 = "src/main/data/dictionary/CoreNatureDictionary.mini.txt";
String dict2 = "src/main/data/dictionary/custom/上海地名.txt ns";
DoubleArrayTrieSegment segment = new DoubleArrayTrieSegment(dict1, dict2);

segment.enablePartOfSpeechTagging(true);    // 激活数词和英文识别
HanLP.Config.ShowTermNature = true;         // 显示词性


System.out.println(segment.seg("江西鄱阳湖干枯，中国最大淡水湖变成大草原"));
System.out.println(segment.seg("上海市虹口区大连西路550号SISU"));


// 显示每个词的词性：
for (Term term : segment.seg("上海市虹口区大连西路550号SISU")) {
    System.out.printf("单词:%s 词性:%s\n", term.word, term.nature);
}
```

### 停用词

[去除停用词](https://github.com/hankcs/HanLP/blob/v1.7.5/src/test/java/com/hankcs/book/ch02/DemoStopwords.java)


### 繁简体互转

https://github.com/hankcs/HanLP/blob/v1.7.5/src/test/java/com/hankcs/demo/DemoTraditionalChinese2SimplifiedChinese.java


```Java
System.out.println(HanLP.convertToTraditionalChinese("“以后等你当上皇后，就能买草莓庆祝了”。发现一根白头发"));
System.out.println(HanLP.convertToSimplifiedChinese("憑藉筆記簿型電腦寫程式HanLP"));
// 简体转台湾繁体
System.out.println(HanLP.s2tw("hankcs在台湾写代码"));
// 台湾繁体转简体
System.out.println(HanLP.tw2s("hankcs在臺灣寫程式碼"));
// 简体转香港繁体
System.out.println(HanLP.s2hk("hankcs在香港写代码"));
// 香港繁体转简体
System.out.println(HanLP.hk2s("hankcs在香港寫代碼"));
// 香港繁体转台湾繁体
System.out.println(HanLP.hk2tw("hankcs在臺灣寫代碼"));
// 台湾繁体转香港繁体
System.out.println(HanLP.tw2hk("hankcs在香港寫程式碼"));

// 香港/台湾繁体和HanLP标准繁体的互转
System.out.println(HanLP.t2tw("hankcs在臺灣寫代碼"));
System.out.println(HanLP.t2hk("hankcs在臺灣寫代碼"));

System.out.println(HanLP.tw2t("hankcs在臺灣寫程式碼"));
System.out.println(HanLP.hk2t("hankcs在台灣寫代碼"));
```



## 语言模型

一个句子出现的概率 $P(s) = p(w_1 w_2 ... w_n) = p(w_1\mid w_0) p(w_2\mid w_0 w_1)...p(w_{k+1}\mid w_0 w_1...w_k)$
- 其中 $w_0 = $ BOS （Begin Of Sentence）


上面的模型是极难计算的，因此做一些简化：
- 假设满足马尔可夫性，也就是说 $p(w_t\mid w_0 w_1...w_{t-1}) = P(w_t\mid w_{t-1})$ ，于是 $P(s) = p(w_1\mid w_0) p(w_2\mid w_1)...p(w_{k+1}\mid w_k)$，称为 **二元模型（bigram）**
- **n元语法（n-gram）** 指的是每个单词的概率取决于之前的 n-1 个单词
- **1元语法（unigram）** 其实就是记录每个单词的频率


平滑策略  
n 越大，越有可能稀疏，我们可以用线性插值来平滑它。  
例如 $p(w_t\mid w_{t-1})$ 很可能在语料中未出现，因此计算得到概率为0，但它的实际概率肯定不是0，这就是稀疏性。于是我们用一个平滑因子来平滑它 $p(w_t\mid w_{t-1}) := \lambda p(w_t\mid w_{t-1}) + (1-\lambda)p(w_t)$



### 分词语料库

- **PKU** 1998年《人民日报》语料库。误标率较高，不宜作为首选。
- **MSR** 微软亚洲研究院语料库。专有名词、姓名没有做切分，更符合认知



加载分词语料库
```Java
String MY_CWS_CORPUS_PATH = "src/main/data/test/my_cws_corpus.txt";

List<List<IWord>> sentenceList = CorpusLoader.convert2SentenceList(MY_CWS_CORPUS_PATH);
for (List<IWord> sentence : sentenceList) {
    System.out.println(sentence);
}
```

语料库的例子：
```txt
商品 和 服务
商品 和服 物美价廉
服务 和 货币
```

### Viterbi 分词训练和应用

```Java
public class DemoNgramSegment
{


    public static void main(String[] args)
    {
        String TRAIN_PATH = "src/main/data/test/icwb2-data/training/msr_training.utf8";
        String MSR_MODEL_PATH = "src/main/data/test/msr_cws_ngram";

//        训练模型
        trainBigram(TRAIN_PATH, MSR_MODEL_PATH);
//        加载训练好的模型
        Segment segment=loadBigram(MSR_MODEL_PATH);

//        模型的一些东西
        CoreDictionary.getTermFrequency("商品");
        CoreBiGramTableDictionary.getBiFrequency("商品", "和");


//        使用模型
        System.out.println(segment.seg("商品和服务"));


    }

    /**
     * 训练bigram模型
     *
     * @param corpusPath 语料库路径
     * @param modelPath  模型保存路径
     */
    public static void trainBigram(String corpusPath, String modelPath)
    {
        List<List<IWord>> sentenceList = CorpusLoader.convert2SentenceList(corpusPath);
        for (List<IWord> sentence : sentenceList)
            for (IWord word : sentence)
                if (word.getLabel() == null) word.setLabel("n"); // 赋予每个单词一个虚拟的名词词性
        final NatureDictionaryMaker dictionaryMaker = new NatureDictionaryMaker();
        dictionaryMaker.compute(sentenceList);
        dictionaryMaker.saveTxtTo(modelPath);
    }

    public static Segment loadBigram(String modelPath)
    {
        return loadBigram(modelPath, true);
    }

    /**
     * 加载bigram模型
     *
     * @param modelPath 模型路径
     * @param viterbi   是否创建viterbi分词器
     * @return 分词器
     */
    public static Segment loadBigram(String modelPath, boolean viterbi)
    {
        String MSR_MODEL_PATH = "data/test/msr_cws_ngram";

//        HanLP.Config.enableDebug();
        HanLP.Config.CoreDictionaryPath = modelPath + ".txt";
        HanLP.Config.BiGramDictionaryPath = modelPath + ".ngram.txt";

        return viterbi ? new ViterbiSegment().enableAllNamedEntityRecognize(false).enableCustomDictionary(false) :
            new DijkstraSegment().enableAllNamedEntityRecognize(false).enableCustomDictionary(false);
    }
}
```

训练过程：1）原理在上面贴了 2）保存下来的模型其实是如下的 2-gram 信息

```txt
橡胶@性能 1
橡胶@推进 2
...(几百万行)
```

预测过程：

![Viterbi](/pictures_for_blog/nlp/viterbi.jpeg)

算法
- 用正向贪心算法和负向贪心算法都不合适
- 用 Viterbi 算法。原理：（感觉就是动态规划）
    1. 正向，获取起点到 i 的最小花费
    2. 逆向，从终点向前回溯，得到最短路径

### 分词中集成用户词典



原因：实际应用中，往往遇到某一专业领域的词语，而对应的语料却不足。因此用户自己整理一个词典，来弥补通用语料的不足。

[代码](https://github.com/hankcs/HanLP/blob/v1.7.5/src/test/java/com/hankcs/book/ch03/DemoCustomDictionary.java)

```Java
Segment segment = new ViterbiSegment();
final String sentence = "社会摇摆简称社会摇";
segment.enableCustomDictionary(false);
System.out.println("不挂载词典：" + segment.seg(sentence));
CustomDictionary.insert("社会摇", "nz 100");
segment.enableCustomDictionary(true);
System.out.println("低优先级词典：" + segment.seg(sentence));
segment.enableCustomDictionaryForcing(true);
System.out.println("高优先级词典：" + segment.seg(sentence));
```

>不挂载词典：[社会/n, 摇摆/vn, 简称/v, 社会/n, 摇/v]
低优先级词典：[社会/n, 摇摆/vn, 简称/v, 社会摇/nz]
高优先级词典：[社会摇/nz, 摆/v, 简称/v, 社会摇/nz]


额外1:加载一个自定义字典文件
```Java
HanLP.Config.CustomDictionaryPath= new String[]{"src/main/data/dictionary/custom/CustomDictionaryCopy.txt"};

// CustomDictionaryCopy.txt 内容如下
// 孙阳朝 nz 10
// 沈一石 nz 10
```


额外2:其它配置
```java
HanLP.Config.ShowTermNature=true; // 是否显示词性，默认为 true

System.out.println(HanLP.segment("王国维和服务员"));
# [王国维/nr, 和/c, 服务员/n]
```


### 模型调优

[混淆矩阵的计算](https://github.com/hankcs/HanLP/blob/v1.7.5/src/test/java/com/hankcs/book/ch03/EvaluateBigram.java)


[模型调优](https://github.com/hankcs/HanLP/blob/v1.7.5/src/test/java/com/hankcs/book/ch03/DemoAdjustModel.java)

- 增加关键词，例如姓名、机构名。加到 `data/dictionary/custom/CustomDictionary.txt` 会生成一个 bin 文件。 ？？？我在原项目中成功了，在独立项目中没成功








## 隐马尔可夫模型

我参观了北京大学口腔医院。







```python
from pyhanlp import HanLP

print(HanLP.segment("王国维和服务员"))
```
