---
layout: post
title: 【Application using NLP】
categories:
tags: 2-4-NLP
keywords:
description:
order: 321
---

## Sentiment Classification
### 难点
相对于单词数量来说，训练集太少，例如，你的训练集有10k，但涉及到的单词量也是10k  
难点解决：利用其它模型训练好的 word embedding 进行转化。  

### 简单版模型

假设共有5个分数（label有5种）。对于某条评论，每个单词用 embedding matrix 转化为向量后（假设转化后的向量是$300\times 1$的），把向量求平均，输入 softmax 模型进行回归（weights 形状是$5\times 300$）。

### RNN for sentiment classification
上面的方法有个缺陷，例如句子是 “Completely lacking in good taste, good service, and good ambience.”，这个句子会被判别为非常正向。  


这是一种 Many-to-one 的RNN  
这种模型可用性很好的原因是，你用近义词替换后，模型的输出结果会差不多。  

![](https://github.com/guofei9987/pictures_for_blog/blob/master/rnn/RNN_for_sentiment_classification.png?raw=true)


## 消除偏见

由于训练用的文本有偏见，embedding matrix 也会有偏见，例如性别偏见：  
Man: computer programmers as woman :Homemaker   
偏见会导致 embedding matrix 在使用时的问题。  


做法（以sexual bias为例）：  
step1. Identify bias direction $avg (e_{he}-e_{she},e_{male}-e_{female},...)$，这就找到表示性别的轴  
step2. Neutralize: For every word that is not definitional, project to get rid of bias.就是把那些性别无关的单词（如computer programmers），投影到轴上  
step3. Equalize pairs. 使得与性别有关的单词（如gril和boy）词组到computer programmers的距离相等。

## 生成模型

![](https://github.com/guofei9987/pictures_for_blog/blob/master/rnn/rnn2.png?raw=true)
（这个图里面，左边和右边是不同的RNN）


1. 翻译，Many-to-Many RNN model.  
2. 读图讲故事。模型的输入是一张图，输出是一句话，描述图中的内容。  
做法：一个训练好点的CNN，去除最后的 softmax 层，将这个输出作为RNN的输入。

2020年更新：  
这个模型叫做Seq2Seq，有这几种类型
- 图中的输出和输入不是同时，这个叫做 **Delayed Many2Many**，左半部分的神经元都一样，叫做 **Encoder**，右半部分的神经元都一样，叫做 **Decoder**。另外如果输入输出同时的话，叫做 Many-to-Many，也是最原始的那个RNN
- 每一块可以是 LSTM，GRU等。GRU更常见
- 针对右半部分，每个节点输入的y可以是上一个节点的输出（叫做AR，例如DeepAR），也可以是真实值（叫做MQRNN）

![](https://github.com/guofei9987/pictures_for_blog/blob/master/deep_learning/Seq2Seq.png?raw=true)

### DeepAR
DeepAR:（论文 https://arxiv.org/abs/1704.04110）
- 神经网络节点是GRU，
- 输出不是y，而是正态分布的$u,\sigma$，损失函数会相应调整。

![](https://github.com/guofei9987/pictures_for_blog/blob/master/deep_learning/deepar1.png?raw=true)

![](https://github.com/guofei9987/pictures_for_blog/blob/master/deep_learning/deepar2.png?raw=true)

### MQRNN
（论文 https://arxiv.org/abs/1711.11053）
- encoder用的是GRU，decoder用的是DNN
- 输出的是分位数。并且有对应的损失函数。

![](https://github.com/guofei9987/pictures_for_blog/blob/master/deep_learning/mqrnn1.png?raw=true)

![](https://github.com/guofei9987/pictures_for_blog/blob/master/deep_learning/mqrnn2.gif?raw=true)

WaveNet（https://arxiv.org/abs/1609.03499）

Attention（https://arxiv.org/abs/1706.03762）



### Picking the most likely sentence
缺点一样，在生成过程中，单词是依概率选取的。  
解决：Picking the most likely sentence  
我们想要找到$\arg\max P(y^{<1>},...,y^{<T_y>}\mid x)$  
- 不能用遍历，情况太多了。假设有$10k$个单词，要生成长度为10的句子，那么搜索空间是$10k^{10}$，非常巨大
- 不能用 greedy search （因为未必是全局最大）
- 用 beam search （未必能达到最优，但往往是最优）  
    1. 有一个 hyperparameter ，B，下面以$B=3$为例
    2. 每一步生成 top 3 （top B）个单词，放到下一步
    3. 如果要生成长度为10的句子，那么搜索空间变成$3^10$  
    （如果B=1，等价于 greedy search）

**beam search的改进**  
新问题，$\arg\max\limits_y P(y^{<1>},...,y^{<T_y>}\mid x) = \arg\max\limits_y \prod\limits_{t=1}^{T_y}P(y^{<t>}\mid x,y^{<1>},...,y^{<t-1>})$这个数字实际上非常小，很可能低于计算机所能表示的下界，尤其是要生成的句子很长的时候。
- 我们考虑用这个$\arg\max\limits_y \sum\limits_{t=1}^{T_y} \log P(y^{<t>}\mid x,y^{<1>},...,y^{<t-1>})$
- 进一步，为了将句子的长度也纳入最优化的考虑因素中，用这个，$\arg\max\limits_y \dfrac{1}{T_y^{\alpha}}\sum\limits_{t=1}^{T_y} \log P(y^{<t>}\mid x,y^{<1>},...,y^{<t-1>})$，这里，$\alpha$是一个 hyperparameter
    - $\alpha=0$,就是整个句子概率最大
    - $\alpha=1$,就是平均到每个单词，概率最大

### Error analysis in beam search
假设你有一个好的翻译（可能来自人的翻译），记为 $ y^* $  
算法用 beam search 计算出的句子，记为$\hat y$  

可以用算法计算 $P(y^* \mid x),P(\hat y \mid x)$  

- case1：$P(y^* \mid x)>P(\hat y \mid x)$，需要调整 beam search 算法
- case2：$P(y^* \mid x)\leq P(\hat y \mid x)$，需要调整 RNN


**Bleu Score**  （bilingual evaluation understudy）是一种给文本生成效果打分的方法。  
假如我们有人工翻译的标准答案，和机器翻译的结果:  
>Reference 1: The cat is on the mat.  
Reference 2: There is a cat on the mat.  
MT output: The cat the cat on the mat.  

**step1**:对于 MT output，用n-gram拆分，并计数  
这里假设用 2-gram，计数结果是{'the cat':2, 'cat the':1, 'cat on':1, 'on the': 1, 'the mat': 1}  
**step2**:对step1中拆分出的gram，找到标准答案的计数中最大的那个，结果是{'the cat':1, 'cat the':0, 'cat on':1, 'on the': 1, 'the mat': 1}  
**step3**: sum(step2)/sum(step1) 就是 Bleu Score

- 可以是 n-gram
- 如果 MT output 与某个标准答案完全一样，得分为1
- 假设 $p_n=$ Bleu score on n-grams，用$BP * \exp (1/4 \sum\limits_{n=1}^4 p_n)$作为总分，其中，$BP=\exp(1-\max(MTOutputLength/ReferenceOutputLength))$  
(BP 是 brevity penalty 的缩写)

## Attention Model Intuition
https://www.coursera.org/learn/nlp-sequence-models/lecture/lSwVa/attention-model
## Speech recognition
解决的问题是，输入的音频，$T_x>>T_y$

https://www.coursera.org/learn/nlp-sequence-models/lecture/sjiUm/speech-recognition

## Trigger Word Detection

https://www.coursera.org/learn/nlp-sequence-models/lecture/Li4ts/trigger-word-detection
