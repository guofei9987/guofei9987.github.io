---
layout: post
title: 【pytorch】BERT
categories:
tags: 0x26_torch
keywords:
description:
order: 274
---

## 为什么


RNN的问题：
- 不能并行计算
- 引入 Self-Attention 机制

word2vec 的问题
- 一个词在不同的句子中的语义不一样
- 预训练好的向量永久不变
