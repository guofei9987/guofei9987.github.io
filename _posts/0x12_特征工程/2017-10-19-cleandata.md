---
layout: post
title: ğŸ”¥ æ•°æ®æ¸…æ´—ä¸ç‰¹å¾å·¥ç¨‹
categories:
tags: 0x12_ç‰¹å¾å·¥ç¨‹
keywords:
description:
order: 100
---

## åˆæ­¥æ¸…æ´—

**ç¼ºå¤±å€¼**
1. ç›´æ¥ä½¿ç”¨ã€‚æœ‰äº›æ¨¡å‹æ”¯æŒç¼ºå¤±å€¼ï¼Œå¦‚å†³ç­–æ ‘
2. åˆ é™¤ç‰¹å¾ã€‚å¦‚æœæŸä¸ªç‰¹å¾å¤§å¤šæ•°éƒ½æ˜¯ç¼ºå¤±å€¼ï¼Œé‚£ä¹ˆå¯ä»¥åˆ é™¤è¿™ä¸ªç‰¹å¾
3. è¡¥å…¨
    - å‡å€¼å¡«å……ã€‚ç¼ºç‚¹æ˜¯å¡«å……çš„å€¼éƒ½ä¸€æ ·
    - ä¼—æ•°å¡«å……
    - æ’å€¼æ³•å¡«å……
    - èšç±»ï¼Œç„¶ååŒç±»å‡å€¼æ’è¡¥
    - å»ºæ¨¡é¢„æµ‹ã€‚ç¼ºç‚¹æ˜¯ï¼šå¦‚æœç¼ºå¤±å±æ€§ä¸å…¶ä»–å±æ€§æ— å…³ï¼Œé‚£ä¹ˆé¢„æµ‹ç»“æœæ— æ„ä¹‰ã€‚å¦‚æœé«˜åº¦ç›¸å…³ï¼Œé‚£ä¹ˆå¯ä»¥åˆ é™¤ç‰¹å¾ã€‚  
    - é«˜ç»´æ˜ å°„ã€‚ä¼˜ç‚¹ï¼šæœ€å‡†ç¡®çš„åšæ³•ï¼Œå› ä¸ºå®Œå…¨ä¿ç•™äº†ä¿¡æ¯ï¼Œä¹Ÿä¸å¢åŠ ä»»ä½•ä¿¡æ¯ã€‚  
    - ç¼ºå¤±æ ‡è®°ï¼šé€‚ç”¨äºæ ·æœ¬é‡éå¸¸å¤§ã€ç¼ºå¤±çš„å˜é‡æ˜¯ç¦»æ•£çš„ï¼Œä¸”éå¸¸ç¨€ç–ã€‚ç¼ºå¤±å€¼ä¹Ÿå½“åšæŸç±»å–å€¼å¤„ç†ã€‚å®ƒç±»ä¼¼One-hot Encode



**å¼‚å¸¸å€¼**
- **æç«¯å€¼**æŒ‡çš„æ˜¯5 Sigmaä¹‹å¤–çš„å€¼
- **ç¦»ç¾¤å€¼**æŒ‡çš„æ˜¯3 Sigmaä¹‹å¤–çš„å€¼  
- ç¡®è®¤çš„æ–¹æ³•
    - ç”»boxå›¾æ£€æŸ¥
    - ç”¨5å€stdæ£€æŸ¥
- è§£å†³æ–¹æ³•
    - ç›–å¸½æ³•ã€‚æŠŠ3sigmaä¹‹å¤–çš„æ•°æ®å®šä¸ºsigma
    - **å–ln**ã€‚æœ‰äº›å¼‚å¸¸å€¼æ˜¯å› ä¸ºæ­¤å˜é‡æœä»å¯¹æ•°æ­£æ€åˆ†å¸ƒï¼Œç›´æ¥å–å¯¹æ•°å°±èƒ½è§£å†³ã€‚
        - ç°å®ä¸­çš„å¯¹æ•°æ­£æ€åˆ†å¸ƒä¹Ÿå¾ˆå¹¿æ³›ï¼ˆå¯èƒ½ä»…æ¬¡äºæ­£æ€åˆ†å¸ƒï¼‰
    - åˆ†ç±»å»ºæ¨¡ã€‚æŠŠå¹²æ‰°å˜é‡å˜æˆåˆ†ç±»å˜é‡ï¼ˆå¼‚å¸¸ä¸º1ï¼Œä¸å¼‚å¸¸ä¸º0ï¼‰  
    - ç¦»æ•£åŒ–ã€‚ä¾‹å¦‚åšæˆ é«˜ã€ä¸­ã€ä½ï¼Œä¸‰ç§å­—æ®µã€‚  
    - å‰”é™¤
        - å‰”é™¤æ•´è¡Œ
        - å‰”é™¤æ•´åˆ—

**å†—ä½™å€¼**
- drop_duplicates



## æè¿°æ€§ç»Ÿè®¡

| åç§° | ç‰¹ç‚¹ |
|------|-----|
| ä¼—æ•° | ä¸å—æç«¯å€¼å½±å“ã€ä¸å”¯ä¸€ã€å¯ç”¨äºåˆ†å¸ƒæœ‰åçš„æƒ…å†µ |
| ä¸­ä½æ•° | ä¸å—æç«¯å€¼å½±å“ã€å¯ç”¨äºåˆ†å¸ƒæœ‰åçš„æƒ…å†µ |
| å¹³å‡å€¼ | æ˜“å—æç«¯å€¼å½±å“ã€æ•°å­¦æ€§è´¨è¾ƒå¥½ã€é€‚ç”¨äºæ— åçš„æƒ…å†µ |

<br>


| æ•°æ®ç±»å‹ | é€‚ç”¨å“ªäº› |
|---------|---------|
| åˆ†ç±»æ•°æ® | **ä¼—æ•°** |
| é¡ºåºæ•°æ® | **ä¸­ä½æ•°**ã€å››åˆ†ä½æ•°ã€ä¼—æ•°|
| é—´éš”æ•°æ® | **å¹³å‡æ•°**ã€å››åˆ†ä½æ•°ã€ä¼—æ•°|
| è¿ç»­æ•°æ® | **å¹³å‡æ•°**ã€è°ƒå’Œå¹³å‡ã€å‡ ä½•å¹³å‡ã€ä¸­ä½æ•°ã€å››åˆ†ä½æ•°ã€ä¼—æ•°|

å…¶å®ƒ
- å…¨è·
- å››åˆ†ä½è·
- æ–¹å·®
- æ ‡å‡†å·®
- ååº¦
- å³°åº¦
- å˜å¼‚ç³»æ•°


### æè¿°æ€§ç»˜å›¾

**å•å˜é‡**

| æ•°æ®ç±»å‹ | é€‚ç”¨å“ªäº› |
|---------|---------|
| åˆ†ç±»æ•°æ® | **é¥¼å›¾** |
| é¡ºåºæ•°æ® | **æ¡å½¢å›¾** |
| é—´éš”æ•°æ® | **æ¡å½¢å›¾** |
| è¿ç»­æ•°æ® | **ç›´æ–¹å›¾**ã€**ç®±å›¾**|


**åŒå˜é‡X-Y**

|X\Y|å¤šåˆ†ç±»|è¿ç»­ |
|---|-----|----|
|å¤šåˆ†ç±»|çŸ©å½¢å›¾|ç®±å›¾|
|è¿ç»­|ç®±å›¾|scatter|


## æ¨¡å‹åé¦ˆ

1. æ•°æ®æ¸…æ´—æœ‰æ²¡æœ‰é—®é¢˜
2. æ•°æ®æŠ½æ ·æœ‰æ²¡æœ‰é—®é¢˜
3. æ•°æ®ç†è§£æœ‰æ²¡æœ‰é—®é¢˜
    - ä¸»æˆåˆ†åˆ†æçœ‹ä¸€ä¸‹
    - èšç±»çœ‹ä¸€ä¸‹
4. æ¨¡å‹é€‰æ‹©æœ‰æ²¡æœ‰é—®é¢˜
5. å‚æ•°è°ƒæ•´æœ‰æ²¡æœ‰é—®é¢˜




## æ•°æ®ä¸å‡è¡¡


**Unbalanced Data** èƒŒæ™¯å°±ä¸è¯´äº†ï¼Œæ•°æ®ä¸å‡è¡¡æ˜¯å¸¸æ€ï¼Œåšåˆ†ç±»æ¨¡å‹æ—¶ä¸å¾—ä¸è®¤çœŸå¤„ç†ã€‚  
åŸºæœ¬ä¸Šæœ‰è¿™å‡ ä¸ªç­–ç•¥ï¼š
1. å¢åŠ æ•°æ®ã€‚å¾ˆå¤šå…¶å®ƒé—®é¢˜ä¹Ÿéƒ½èƒ½ç”¨è¿™ä¸ªæ–¹æ³•è§£å†³ï¼Œä½†æˆæœ¬å¤ªé«˜ï¼Œä¸å¤šæäº†ã€‚
2. Oversamplingï¼Œæœ‰å¾ˆå¤šå˜ç§
3. Undersamplingï¼Œä¹Ÿæœ‰å¾ˆå¤šå˜ç§
4. åˆ’åˆ†è®­ç»ƒé›†
5. ç®—æ³•å±‚é¢
    - æ”¹å˜ cost function. sklearn å¾ˆå¤šæ–¹æ³•å¯ä»¥è¾“å…¥ weights å‚æ•°ï¼Œå®é™…ä¸Šå°±æ˜¯æ”¹å˜ cost functionï¼Œè®©ç¨€æœ‰ç±»çš„æƒé‡å¢åŠ ã€‚
    - ä½¿ç”¨å¯ä»¥å¤„ç†ä¸å¹³è¡¡é—®é¢˜çš„æ¨¡å‹ï¼Œå¦‚ æœ´ç´ è´å¶æ–¯ã€‚æœ‰ææ–™è¯´ å†³ç­–æ ‘ ä¹Ÿå¯ä»¥ï¼Ÿæˆ‘è¡¨ç¤ºæ€€ç–‘ï¼Œå›å¤´è¯•è¯•ã€‚
    - å´æ©è¾¾çš„ç¥ç»ç½‘ç»œè¯¾ç¨‹ä¸Šè®²è¿‡ä¸€ä¸ªæ–¹æ³•ã€‚å¯¹äºå¯ä»¥é‡å¤è®­ç»ƒçš„æ¨¡å‹ï¼ŒæŠŠå¤šæ•°ç±»åˆ†å‰²æˆå¤šä¸ªéƒ¨åˆ†ï¼Œæ¯éƒ¨åˆ†éƒ½å’Œç¨€æœ‰ç±»ç»“åˆèµ·æ¥å»è®­ç»ƒæ¨¡å‹ã€‚
    - ã€TODO: è¿™ä¸€éƒ¨åˆ†ä»¥åè¡¥å……ï¼Œå› ä¸ºå¾ˆæœ‰ç”¨ä¸”é‡è¦ã€‘è€ƒè™‘ä¸åŒè¯¯åˆ†ç±»æƒ…å†µä»£ä»·çš„å·®å¼‚æ€§å¯¹ç®—æ³•è¿›è¡Œä¼˜åŒ–ï¼Œä¸»è¦æ˜¯åŸºäºä»£ä»·æ•æ„Ÿå­¦ä¹ ç®—æ³•(Cost-Sensitive Learning)ï¼Œä»£è¡¨çš„ç®—æ³•æœ‰adacostï¼›
    - ä¸å¹³è¡¡æ•°æ®é›†çš„é—®é¢˜è€ƒè™‘ä¸ºä¸€åˆ†ç±»ï¼ˆOne Class Learningï¼‰æˆ–è€…å¼‚å¸¸æ£€æµ‹ï¼ˆNovelty Detectionï¼‰é—®é¢˜ï¼Œä»£è¡¨çš„ç®—æ³•æœ‰One-class SVMã€‚ `sklearn.svm.OneClassSVM`


### 2. over sampling
https://github.com/scikit-learn-contrib/imbalanced-learn  
https://pypi.org/project/imbalanced-learn/

#### æ•°æ®å‡†å¤‡
```python
from sklearn.datasets import make_classification
from collections import Counter

X, y = make_classification(n_samples=100, weights=[0.2, 0.8])
```

#### RandomOverSampler
ç®€å•çš„å¤åˆ¶
```python
from imblearn import over_sampling
osamp = over_sampling.RandomOverSampler(random_state=0)
X_resampled, y_resampled = osamp.fit_sample(X, y)
Counter(y_resampled)
```

- ä¼˜ç‚¹ï¼šç®€å•
- ç¼ºç‚¹ï¼šå°ä¼—æ ·æœ¬å¤åˆ¶å¤šä»½ï¼Œä¸€ä¸ªç‚¹ä¼šåœ¨é«˜ç»´ç©ºé—´ä¸­åå¤å‡ºç°ã€‚å¯¼è‡´è¿‡æ‹Ÿåˆï¼Œæˆ–è€…è¿æ°”å¥½å°±èƒ½åˆ†å¯¹å¾ˆå¤šç‚¹ï¼Œå¦åˆ™åˆ†é”™å¾ˆå¤šç‚¹ã€‚

#### SMOTE
```python
from imblearn import over_sampling
osamp = over_sampling.SMOTE(kind='regular',k_neighbors=2)
X_resampled, y_resampled = osamp.fit_sample(X, y)
Counter(y_resampled)
```
éšæœºé€‰å–å°‘æ•°ç±»çš„æ ·æœ¬ï¼Œåœ¨è¯¥æ ·æœ¬ä¸æœ€é‚»è¿‘çš„æ ·æœ¬çš„è¿çº¿ä¸Šéšæœºå–ç‚¹ï¼Œç”Ÿæˆæ— é‡å¤çš„æ–°çš„ç¨€æœ‰ç±»æ ·æœ¬ã€‚

- kind='regular'
- kind='borderline1', kind='borderline2'å…³æ³¨åœ¨æœ€ä¼˜åŒ–å†³ç­–å‡½æ•°è¾¹ç•Œçš„ä¸€äº›å±é™©æ ·æœ¬
- kind='svm' ä½¿ç”¨æ”¯æŒå‘é‡æœºåˆ†ç±»å™¨äº§ç”Ÿæ”¯æŒå‘é‡ç„¶åå†ç”Ÿæˆæ–°çš„å°‘æ•°ç±»æ ·æœ¬ã€‚


ç¼ºç‚¹ï¼š
- å¢åŠ äº†ç±»ä¹‹é—´é‡å çš„å¯èƒ½æ€§ï¼ˆç”±äºå¯¹æ¯ä¸ªå°‘æ•°ç±»æ ·æœ¬éƒ½ç”Ÿæˆæ–°æ ·æœ¬ï¼Œå› æ­¤å®¹æ˜“å‘ç”Ÿç”Ÿæˆæ ·æœ¬é‡å (Overlapping)çš„é—®é¢˜ï¼‰ï¼Œ
- ç”Ÿæˆä¸€äº›æ²¡æœ‰æä¾›æœ‰ç›Šä¿¡æ¯çš„æ ·æœ¬ã€‚


#### ADASYN

å…³æ³¨çš„æ˜¯åœ¨é‚£äº›åŸºäºKNNåˆ†ç±»å™¨è¢«é”™è¯¯åˆ†ç±»çš„åŸå§‹æ ·æœ¬é™„è¿‘ç”Ÿæˆæ–°çš„å°‘æ•°ç±»æ ·æœ¬

```python
from imblearn import over_sampling
osamp = over_sampling.ADASYN()
X_resampled, y_resampled = osamp.fit_sample(X, y)
Counter(y_resampled)
```

### 3. under sampling

```python
usap = under_sampling.RandomUnderSampler()
X_resampled, y_resampled = usap.fit_sample(X, y)
Counter(y_resampled)
```
åŠŸèƒ½æŒºå¤šï¼Œ`replacement = True` æœ‰æ”¾å›æŠ½æ ·ï¼Œç»“æœä¼šæœ‰é‡å¤ï¼Œé»˜è®¤ `replacement = False`

- å¯å‘å¼è§„åˆ™åšé™é‡‡æ ·  `under_sampling.NearMiss(version=1)` version = 1, 2, 3 æœ‰3ç§å¯å‘å¼è§„åˆ™
- ç”¨è¿‘é‚»ç®—æ³•è¿›è¡Œé™é‡‡æ · `under_sampling.EditedNearestNeighbours()` ç»å¤§å¤šæ•°(kind_sel='mode')æˆ–è€…å…¨éƒ¨(kind_sel='all')çš„è¿‘é‚»æ ·æœ¬éƒ½å±äºåŒä¸€ä¸ªç±»ï¼Œè¿™äº›æ ·æœ¬ä¼šè¢«ä¿ç•™åœ¨æ•°æ®é›†ä¸­
- åœ¨EditedNearestNeighboursåŸºç¡€ä¸Š, å»¶ä¼¸å‡ºäº†RepeatedEditedNearestNeighboursç®—æ³•å’ŒALLKNNç®—æ³•ï¼Œå‰è€…é‡å¤åŸºç¡€çš„EditedNearestNeighboursç®—æ³•å¤šæ¬¡ï¼Œåè€…åœ¨è¿›è¡Œæ¯æ¬¡è¿­ä»£çš„æ—¶å€™ï¼Œæœ€è¿‘é‚»çš„æ•°é‡éƒ½åœ¨å¢åŠ ã€‚
- `under_sampling.ClusterCentroids()` ä¸æ˜¯ä»åŸå§‹æ•°æ®é›†ä¸­ç›´æ¥æŠ½å–æ•°æ®ï¼Œæ¯ä¸€ä¸ªç±»åˆ«çš„æ ·æœ¬éƒ½ä¼šç”¨K-Meansç®—æ³•çš„ä¸­å¿ƒç‚¹æ¥è¿›è¡Œåˆæˆã€‚

### over Sampling + under sampling



æ•°æ®ä¸å‡è¡¡å‚è€ƒèµ„æ–™ï¼š
- https://www.cnblogs.com/kamekin/p/9824294.html
- https://www.jianshu.com/p/2149d94963cc  
- https://github.com/scikit-learn-contrib/imbalanced-learn



## æ ‡å‡†åŒ–

æ ‡å‡†åŒ–æ˜¯å¯¹åˆ—æ“ä½œ  

æ•°æ®å‡†å¤‡ï¼š
```py
import pandas as pd
import numpy as np
df = pd.DataFrame(np.random.uniform(size=(20, 3)), columns=list('abc'))
```

**StandardScaler**
```py
# ï¼ˆZ-Scoreï¼‰
# å…¬å¼ä¸ºï¼š(X-mean)/std è®¡ç®—æ—¶å¯¹æ¯ä¸ªå±æ€§/æ¯åˆ—åˆ†åˆ«è¿›è¡Œã€‚
from sklearn import preprocessing as preprocessing

standard_scaler = preprocessing.StandardScaler()
standard_scaler.fit_transform(df)
standard_scaler.fit(df)
standard_scaler.transform(df)
```



**MinMaxScaler** 0-1æ ‡å‡†åŒ–  
- ä½¿ç”¨è¿™ç§æ–¹æ³•çš„ç›®çš„åŒ…æ‹¬ï¼š  
    1. å¯¹äºæ–¹å·®éå¸¸å°çš„ feature å¯ä»¥å¢å¼ºå…¶ç¨³å®šæ€§ã€‚
    2. å¦‚æœæ˜¯æ­£çš„ç¨€ç–çŸ©é˜µï¼Œå¯ä»¥ç»´æŒä¸º0çš„æ¡ç›®ã€‚


```py
from sklearn import preprocessing
min_max_scaler = preprocessing.MinMaxScaler()
min_max_scaler.fit_transform(df)
min_max_scaler.fit(df)
min_max_scaler.transform(df)
```

**MaxAbsScaler** åˆ—é™¤ä»¥è¯¥åˆ—çš„æœ€å¤§å€¼ã€‚ä¼˜ç‚¹æ˜¯å¯ä»¥ä¿æŒç¨€ç–çŸ©é˜µä¸­çš„0ä¸å˜ã€‚
```py
from sklearn import preprocessing
max_abs_scaler = preprocessing.MaxAbsScaler()
max_abs_scaler.fit_transform(df)
max_abs_scaler.fit(df)
max_abs_scaler.transform(df)
```

**RobustScaler** ä¸€ç§å¯ä»¥é˜²æ­¢å¼‚å¸¸å€¼çš„Scalerã€‚ç®—æ³•æ˜¯å‡å»0.5åˆ†ä½æ•°ï¼Œç„¶åé™¤ä»¥0.75-0.25æå·®ã€‚
```py
from sklearn import preprocessing
robust_scaler=preprocessing.RobustScaler(with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0))
robust_scaler.fit_transform(df)
robust_scaler.fit(df)
robust_scaler.transform(df)
```

**å…¶å®ƒ**
- ç§©æ ‡å‡†åŒ–ï¼šæŠŠæ•°æ®æ’åºï¼Œç„¶åè®°ä¸‹å…¶åºå·ï¼Œç„¶åè½¬åŒ–åˆ° `[0,1]` çš„æ•°æ®
- å‡½æ•°æ ‡å‡†åŒ–ï¼šç”¨ $x'=1/x$ã€$x=log(x)$ ç­‰è½¬åŒ–ã€‚é€‚ç”¨äºä½ çŸ¥é“å…¶åˆ†å¸ƒ/æ•°æ®æƒ…å†µ
- 


## æ­£åˆ™åŒ–ï¼ˆNormalizationï¼‰
æ¯ä¸ªæ ·æœ¬å˜æˆå•ä½èŒƒæ•°

```py
from sklearn import preprocessing
normalizer = preprocessing.Normalizer(norm='l2') # l1, l2, max

normalizer.fit(df)
normalizer.transform(df)
```


## OneHotEncoder
```py
from sklearn import preprocessing
import pandas as pd
df = pd.DataFrame(np.random.randint(low=0, high=3, size=(10, 3)), columns=list('abc'))

onehotencoder = preprocessing.OneHotEncoder()
onehotencoder.fit(df)

onehotencoder.n_values_  # æ¯ä¸ªfeatureæœ‰å¤šå°‘ç§valueï¼Œä¾‹å¦‚ array([3, 3, 3])
onehotencoder.feature_indices_  # ç¬¬iä¸ªfeatureè¢«æ˜ å°„åˆ° feature_indices_[i] to feature_indices_[i+1]
onehotencoder.transform(df).toarray()  # OneHotEncoder çš„ç»“æœ
```

## PolynomialFeatures
åœ¨è¾“å…¥æ•°æ®å­˜åœ¨éçº¿æ€§ç‰¹å¾æ—¶ï¼Œè¿™ä¸€æ“ä½œå¯¹å¢åŠ æ¨¡å‹çš„å¤æ‚åº¦ååˆ†æœ‰ç”¨ã€‚  
ä¾‹å¦‚ï¼Œç‰¹å¾å‘é‡X=(X1, X2)ï¼Œæœ€é«˜é¡¹æ¬¡æ•°ä¸º2ï¼Œè¢«è½¬åŒ–ä¸º(1, X1, X2, X1^2, X1X2, X2^2)  
å¦‚æœæœ€é«˜ä¸ºnæ¬¡æ–¹ï¼Œæœ‰mä¸ªfeatureï¼Œé‚£ä¹ˆè½¬åŒ–åå¾—åˆ°comb(n+m,m)ä¸ªfeature
ï¼ˆå› ä¸º$(\sum_{i=1}^m x_i)^n$å±•å¼€åæœ‰comb(m+n+1,n)é¡¹ï¼‰  

```py
from sklearn import preprocessing

polynomial_features = preprocessing.PolynomialFeatures(degree=2) # æœ€é«˜é¡¹çš„æ¬¡æ•°ä¸º2
# interaction_only=True åªäº§ç”Ÿç›¸äº’ä½œç”¨é¡¹
polynomial_features.fit_transform(df)
polynomial_features.fit(df)
polynomial_features.transform(df)
```
## FunctionTransformer
è‡ªå®šä¹‰é¢„å¤„ç†å™¨
```py
from sklearn import preprocessing
function_transformer=preprocessing.FunctionTransformer(lambda x:np.log(x+100))
function_transformer.fit_transform(df)
function_transformer.fit(df)
function_transformer.transform(df)
```


## ç»å…¸æ•°æ®é›†



### sklearnçš„éšæœºæ•°æ®


**sklearnéšæœºé€ çš„æ•°æ®** [samples-generator](https://scikit-learn.org/stable/modules/classes.html#samples-generator)

å›å½’ [sklearn.datasets.make_regression](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html)

```python
from sklearn import datasets

X, y, coef = \
    datasets.make_regression(n_samples=1000,
                             n_features=5,
                             n_informative=3,  # å…¶ä¸­ï¼Œ3ä¸ªfeatureæ˜¯æœ‰ä¿¡æ¯çš„
                             n_targets=1,  # å¤šå°‘ä¸ª target
                             bias=1,  # å°±æ˜¯ intercept
                             coef=True,  # ä¸ºTrueæ—¶ï¼Œä¼šè¿”å›çœŸå®çš„coefå€¼
                             noise=1,  # å™ªå£°çš„æ ‡å‡†å·®
                             )
```

åˆ†ç±» [sklearn.datasets.make_classification](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html)

```python
from sklearn import datasets

X, y = datasets. \
    make_classification(n_samples=1000,
                        n_features=10,
                        n_informative=2,
                        n_redundant=3,  # ç”¨ n_informative çº¿æ€§ç»„åˆå‡ºè¿™ä¹ˆå¤šä¸ªç‰¹å¾
                        n_repeated=3,  # ç”¨ n_informative+n_redundant çº¿æ€§ç»„åˆå‡ºè¿™ä¹ˆå¤šä¸ªç‰¹å¾
                        n_classes=2,
                        n_clusters_per_class=1,
                        weights=[0.2, 0.8],  # class æ•°é‡ä¸å‡è¡¡
                        scale=[5] + [1] * 8 + [3],  # feature çš„ scale
                        flip_y=0.1  # éšæœºäº¤æ¢è¿™ä¹ˆå¤šæ¯”ä¾‹çš„yï¼Œä»¥åˆ¶é€ å™ªå£°
                        )
```

å¦å¤–ï¼Œè¿˜æœ‰ä¸€ä¸ª [make_multilabel_classification](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_multilabel_classification.html) ç”¨æ¥ç”Ÿæˆå¤šæ ‡ç­¾ï¼ˆtargetæ˜¯å¤šä¸ªç»´åº¦ï¼‰

**èšç±»**

```python
from sklearn import datasets

X, y_true = datasets.make_blobs(
    n_samples=1000,
    n_features=2,
    centers=3,
    center_box=(-1, 1),  # å–å€¼èŒƒå›´
    cluster_std=0.05,
    random_state=23)
```





**å…¶å®ƒ**
```python
datasets.make_s_curve # Så‹
datasets.make_circles # åœ†ç¯
datasets.make_moons # æœˆäº®
...
```

### sklearnçœŸå®æ•°æ®
```py
import sklearn.datasets as datasets
dataset=datasets.load_iris()
dataset.data,dataset.target
dataset.feature_names,dataset.target_names
print(dataset.DESCR)
```


è½¬ä¸ºpandas  
```py
import pandas as pd
import sklearn.datasets as datasets
dataset=datasets.load_iris()
df_feature=pd.DataFrame(data=dataset.data,columns=dataset.feature_names)
df_target=pd.DataFrame(data=dataset.target,columns=['target'])
df=pd.concat([df_feature,df_target],axis=1)
```


**boston (regression)**  
Boston House Prices dataset  
:Number of Instances: 506  
:Number of Attributes: 13 numeric/categorical predictive  

**breast_cancer (classification)**  
:Number of Instances: 569  
:Number of Attributes: 30 numeric, predictive attributes and the class  

**diabetes (regression)**
:Number of Instances: 442  
:Number of Attributes: First 10 columns are numeric predictive values  

**digits (classification)**
:Number of Instances: 5620  
:Number of Attributes: 64 (8x8 image of integer pixels)  


**iris (classification)**  
:Number of Instances: 150 (50 in each of three classes)  
:Number of Attributes: 4 numeric, predictive attributes and the class  

**linnerud (multivariate regression)**

:Number of Instances: 20  
:Number of Attributes: 3  

**load_digits (classification)**  
æ‰‹å†™æ•°å­—è¯†åˆ«ï¼Œ8x8åƒç´ ï¼Œ16è‰²ï¼Œè‚‰çœ¼éƒ½ä¸å®¹æ˜“åˆ¤æ–­
```
=================   ==============
Classes                         10
Samples per class             ~180
Samples total                 1797
Dimensionality                  64
Features             integers 0-16
=================   ==============
```

**load_linnerud (multivariate regression)**
```
==============    ============================
Samples total     20
Dimensionality    3 (for both data and target)
Features          integer
Targets           integer
==============    ============================
```

**load_wine (classification)**
```
=================   ==============
Classes                          3
Samples per class        [59,71,48]
Samples total                  178
Dimensionality                  13
Features            real, positive
=================   ==============
```


### ç½‘ç»œèµ„æº
UCIï¼Œæœ‰å¾ˆå¤šç»å…¸æ•°æ®ï¼š  
http://archive.ics.uci.edu/ml/  
ï¼ˆPSï¼‰UCIæœ‰æ—¶å€™ä¼šå´©ï¼Œå¤ªä¸é è°±äº†ï¼Œgitä¸Šè‡ªå·±åšäº†ä¸ªåº“[datasets](https://github.com/guofei9987/datasets)  


åº”ç”¨ä¸¾ä¾‹ï¼š  
```py
df=pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data',sep='\s+',na_values='?',
               header=None,names=['mpg','cylinders','displacement','horsepower','weight','acceleration','model_year','origin','car_name'])
df.head()
```

sogouæ•°æ®å®éªŒå®¤ï¼š  
http://www.sogou.com/labs/resource/list_pingce.php  

https://www.quandl.com/search   

### MNIST
1. [lecunå®˜ç½‘](http://yann.lecun.com/exdb/mnist/)
2. æˆ‘ç»™è½¬æˆäº†csvæ ¼å¼ï¼Œæ”¾åˆ°äº†[githubä¸Š](https://www.guofei.site/datasets/cv/MNIST_data_csv.zip),ç”¨ä»£ç è¯»ä¹Ÿæ–¹ä¾¿  
```py
# step1:ä¸‹è½½
import requests
url = 'https://www.guofei.site/datasets/cv/MNIST_data_csv.zip'
r = requests.get(url)
with open('MNIST_data_csv.zip', 'wb') as f:
    f.write(r.content)
# step2ï¼šè§£å‹
import zipfile
azip = zipfile.ZipFile('MNIST_data_csv.zip')
azip.extractall()
# step3ï¼šè¯»å–
import pandas as pd
mnist_train_images = pd.read_csv('mnist_train_images.csv', header=None).values
mnist_test_images = pd.read_csv('mnist_test_images.csv', header=None).values
mnist_train_labels = pd.read_csv('mnist_train_labels.csv', header=None).values
mnist_test_labels = pd.read_csv('mnist_test_labels.csv', header=None).values
```
3. tensorflow
```py
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)
mnist.train.images,mnist.train.labels
rand_test_indices = np.random.choice(len(mnist.test.images), test_size)
mnist.test.images[rand_test_indices],mnist.test.labels[rand_test_indices]
```

### å›¾ç‰‡ç±»

**Cifar-10**ï¼Œç”±Hintonç­‰æ•´ç†å‡ºï¼Œæœé›†äº†10ç§ç±»å‹60000å¼ å›¾ç‰‡ï¼Œå…¨éƒ¨æ˜¯$32\times 32$å½©å›¾ï¼Œäººå·¥æ ‡æ³¨æ­£ç¡®ç‡ä¸º94%  
http://www.cs.toronto.edu/~kriz/cifar.html


**ImageNet** æ˜¯åŸºäºWordNetçš„å¤§å‹å›¾ç‰‡æ•°æ®åº“ï¼Œç”±æé£é£å¸¦å¤´æ•´ç†ï¼Œ1500ä¸‡å¼ å›¾ç‰‡å…³è”åˆ°20000ä¸ªåè¯åŒä¹‰è¯ä¸Šã€‚  
å›¾ç‰‡ä»äº’è”ç½‘ä¸Šçˆ¬è™«ï¼Œé€šè¿‡äºšé©¬é€Šäººå·¥æ ‡æ³¨æœåŠ¡åˆ†ç±»ã€‚  
æ¯å¼ å›¾ç‰‡ä¸Šç”±å¤šä¸ªå®ä½“ã€‚  
http://www.image-net.org/challenges/LSVRC/  


## å‚è€ƒèµ„æ–™
- [æèˆªï¼šã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹](https://www.weibo.com/u/2060750830?refer_flag=1005055013_)  
- [å…³äºä½¿ç”¨sklearnè¿›è¡Œæ•°æ®é¢„å¤„ç†](http://www.cnblogs.com/chaosimple/p/4153167.html)  
