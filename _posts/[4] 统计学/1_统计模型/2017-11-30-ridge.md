---
layout: post
title: 【ridge&lasso】理论与实现
categories:
tags: 4-1-统计模型
keywords:
description:
order: 409
---

- lasso 给损失函数添加了一个l1正则化项，ridge给损失函数添加了一个l2正则化项
- lasso 可以用来剔除变量，适合变量少、维度高的情况
- ridge 可以用来解决多重共线性问题


## ridge
岭回归(ridge regression)是一种为解决多重共线性问题而提出的一种 **有偏估计** 回归方法。  


对于这个问题：  
$y=X\beta+\varepsilon$  
传统的OLS对应的损失函数是这样的：  
$\mid\mid X\beta -y\mid\mid^2$  
上述优化问题可以采用梯度下降法进行求解，也可以采用如下公式进行直接求解:  
$\beta=(X^T X)^{-1}X^T y$  


多重共线性意味着X不满秩，所以不可求逆，或者求逆误差会很大。  
ridge的损失函数写成这样：  
$\mid\mid X\beta -y\mid\mid^2+\mid\mid \Gamma \beta\mid\mid^2$  
其中，定义$\Gamma=\alpha I$  
解变成:  
$\beta(k)=(X^T X+\alpha I)^{-1}X^T y$  


## 模型性质

### 性质1
$\hat \beta (k)$ 是$\beta$的有偏估计  


证明：  
$E \beta(k)=E (X^T X+k I)^{-1}X^T y = (X^T X+k I)^{-1}X^T Ey=(X^T X+k I)^{-1}X^T X\beta$  


显然, $k=0$时，才有$E \beta(k)=\beta$  


### 性质2
$\hat \beta(k)$是$\hat\beta$的线性变换


证明：  
$\beta(k)=(X^T X+k I)^{-1}X^T y = (X^T X+k I)^{-1} (X^T X (X^T X)^{-1}) X^T y$  
$=(X^T X+k I)^{-1} (X^T X) (X^T X)^{-1}) X^T y = (X^T X+k I)^{-1} (X^T X) \hat\beta$  


### 性质3
$\forall k>0, \mid\mid \beta \mid\mid \neq 0$  
$\forall k>0, \mid\mid \beta \mid\mid < \mid\mid\hat\beta\mid\mid$  
$if \space k\to\infty, \hat\beta(k)\to 0$  


### 性质4
$MSE(\hat\beta (k)) <MSE(\hat\beta)$  

## alpha的选择
### 1. 岭迹法
- 岭迹到k很稳定了
- 没有不合业务的回归系数
- 残差平方和增加不太多


### 2. VIF
$D\hat \beta (k)= \sigma^2 c(k)$


## Python实现

<iframe src="https://www.guofei.site/StatisticsBlog/ridge_lasso.html" width="100%" height="3600em" marginwidth="10%"></iframe>

## 参考资料
[^lihang]: [李航：《统计学习方法》](https://www.weibo.com/u/2060750830?refer_flag=1005055013_)  
[^wangxiaochuan]: [王小川授课内容](https://weibo.com/hgsz2003)  
[^EM]: 我的另一篇博客[EM算法理论篇](http://www.guofei.site/2017/11/09/em.html)  
[^AppliedRegression]: 《应用回归分析》，人民大学出版社  
