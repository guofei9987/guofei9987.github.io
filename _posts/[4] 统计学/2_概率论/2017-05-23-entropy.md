---
layout: post
title: 信息熵
categories:
tags: 4-2-概率论
keywords: entropy, conditional entropy
description:
order: 9550
---
## 信息熵的定义与性质[^lihang]

信息熵
: 如果X是离散分布，$P(X=x_i)=p_i$，那么信息熵定义为$H(X)=-\sum\limits_{i=1}^n p_i \log p_i$

(定义 $0\log0=0$)

性质  
1. 单位是bit(对数底是2)或nat(对数底是e)
1. 信息熵用来表示随机变量的不确定性，不确定性越大，信息熵越大
2. $0 \leq H(X) \leq \log n$(用Lagrange证明)

## 条件熵

条件熵(conditional entropy)
: 已知X的条件下，Y的不确定性。X对Y的条件分布的熵，对X的数学期望  
$H(Y \mid X)=\sum\limits_{i=1}^n P(X=x_i)H(Y \mid X=x_i)$

性质:  
1. 等价写法$H(Y\mid X)=-\sum\limits_{x,y}P(x,y)log(P(Y\mid X))$
1. $H(Y \mid X)=H(X,Y)-H(X)$(用条件熵定义和条件概率定义容易证明)
2. $H(Y \mid X) \leq H(Y)$(???不会证)

## empirical

大部分时候，概率是不知道的，是从数据中估计出来的，用估计出来的概率计算熵时，得到的结果是经验熵或条件经验熵  

经验熵(empirical entropy)
: 计算熵时，用的概率是从数据估计出来的。

条件经验熵(empirical entropy)
: 计算条件熵时，用的概率是从数据估计出来的

## 信息益增(information gain)

信息益增(information gain)
: 得知特征X的信息而使得Y的信息不确定度减少的程度

计算方法：  
A分类方法下，数据集D的经验熵变化  
$g(D,A)=H(D)-H(D \mid A)$  

## 信息益增比(information gain ratio)

$g_R(D,A)=\dfrac{g(D,A)}{H(D)}$

## KL散度
定义为$D_{KL}(P\mid\mid Q)=\int_{-\infty}^{\infty}\ln\dfrac{p(x)}{q(x)} dx$


## 参考文献

[^lihang]: [李航：《统计学习方法》](https://www.weibo.com/u/2060750830?refer_flag=1005055013_)
