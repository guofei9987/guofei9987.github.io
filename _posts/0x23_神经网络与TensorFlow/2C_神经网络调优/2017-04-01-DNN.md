---
layout: post
title: 【DNN】Structuring DNN Projects
categories:
tags: 0x23_神经网络与TF
keywords:
description:
order: 250
---

## 简介

### 优点

1. 自学习和自适应  
2. 非线性：现实世界是一个非线性的复杂系统，人脑也是.  
3. 鲁棒性:局部损坏只会削弱神经网络，而不会产生灾难性错误  
4. 计算的并行性：每个神经元独立处理信息  
5. 存储的分布性：知识不是存储于某一处，而是分布在所有连接权值中  


### 按结构分类

- 前向神经网络（或前馈神经网络， feedforward neural network）
    - 单层感知机
    - 线性神经网络
    - BP神经网络
    - 径向基(RBF)
    - 卷积神经网络(CNN)
    - ...
- 循环神经网络(RNN,recurrent nerual networks)
    - Hopfield网络
    - Elman网络
    - CG网络模型
    - BSB（盒中脑）模型
    - BAM（双向联想记忆）
    - ...


### 按功能分类
- 有监督学习
    - BP，径向基，Hopfield
- 无监督学习
    - 自组织神经网络
    - 竞争神经网络


此外，还有：
Hebb学习规则  
纠错学习规则（用的多）  
随机学习规则（Boltzmann机）  
竞争学习规则  


## 激活函数
（$z=b+\sum\limits_ix_iw_i$）

**Linear neurons**

$y=z$

**Binary Threshold neuros**


$$y=
\begin{cases}
1&   &if z>=0\\  
0&   & otherwize
\end{cases}$$



**ReLu(Rectified Linear Unit)**

$y=\max(z,0)$

**Sigmoid**


$y=\dfrac{1}{1+e^{-z}}$

- S-function
- 优点是连续、可微，而且微分是$y(1-y)$
- Sigmoid函数还有一种是tanh(z)

**Stochastic binary neurons**


$p(s=1)=\dfrac{1}{1+e^{-z}}$
- 这种神经元依概率输出0或1

**tanh**  
- $g(z)=\dfrac{e^z-e^{-z}}{e^z+e^{-z}}$
- $g'(z)=1-(g(z))^2$


**Leaky ReLU**


$max(0.01x,x)$

**ELU**  

$$y=
\begin{cases}
z&   &if z>=0\\  
\alpha(e^x-1)&   & otherwize
\end{cases}$$


**SELU**  
一种自带batch normalization 的ReLU

$$y=
\begin{cases}
\lambda z&   &if z>=0\\  
\lambda\alpha(e^x-1)&   & otherwize
\end{cases}$$

这里的 $\lambda, \alpha$ 是两个手工计算出来的常数。使得当输入值mean=0，var=1，那么输出值也是mean=0，var=1


### sigmoid的理论基础

**从exponential family角度看**  
关于exponential family, 参见我的另一篇文章<a href='/2017/05/26/distribution1.html#title12'>常见统计分布(2).</a>  

Bernoulli distribution的概率分布函数可以写为：  
$f(x)=p^x(1-p)^{1-x}$  
进一步写为，  
$f(x)=exp[ln\dfrac{p}{1-p} x+ln(1-p)]$  

比较exponential family的定义：  
$P(y;\eta)=b(y)exp(\eta^T T(y)-a(\eta))$    

得到,
$\eta=\dfrac{p}{1-p}$


注：从normal distribution推导exponential family，对应的是线性回归。

**从logistic regression角度看**  
用sigmoid function后，logistic regression的参数，MLE方法等价于OLS方法  

**从BP神经网络角度看**  
误差反向传播过程中，sigmoid function有以下好处：
- 光滑：处处可导，导数连续
- 压缩：把R压缩到[-1,1],并且接近0更敏感。这是BP的需求。
- 导数容易求。y'=y(1-y). 误差反向传播算法中，大大减少了运算量


ReLU出现后，sigmoid就不太用来作为隐含层节点了。因为 sigmoid 在接近0的时候才有强烈的敏感。在大多数区域内饱和（饱和的意思是梯度接近0，导致迭代优化很慢）

RuLU 也有缺点，如果激活值是负数，那么梯度也是负数，就得不到迭代变化的机会。针对这一点有一些改进方案。


### 平方差损失的理论基础

以线性回归为例，  
$y_i=\theta^T x_i+\varepsilon_i$，其中，$\varepsilon_i\sim N(0,\sigma^2)$  
就有这个结论$P(y=y_i\mid x_i;\theta)=\dfrac{1}{\sqrt{2\pi}\sigma}\exp (\dfrac{-(y_i-\theta^T)^2}{2\sigma^2})$  

然后，使用我们喜闻乐见的MLE方法，得到优化对象：平方差损失函数  
（式子就不写了）

## 结构

### Feed-forward neural networks

### Recurrent networks

- 同一个layer之间互相关联
- They can have very complicated dynamics, and this can make them very difficult to train.

### Symmetrically connected networks

- Hopfield：无hidden units
- Boltzmann machines:有hidden units


### 关于Linear

纯linear 也服从误差向前传播，并且注意多层linear网络与单层linear网络没有区别（证明）

### softmax layer

#### 结构

$y_i=\dfrac{e^{z_i}}{\sum\limits_{j \in group}e^{z_j}}$  
于是  
$\dfrac{\partial y_i}{\partial z_i}=y_i(1-y_i)$  

**作为最后一层的原因**
1. 非负、和总是1，符合对概率的定义  
2. softmax regression 的二元情况与 logistics regression 等价
3. 与最大熵模型等价
4. 总和为1，使得具有某种竞争的概念，是一种赢者通吃的形式。
5. 相比 argmax，可微
6. 套一个交叉熵（也就是最大似然），可以大大避免优化过程中出现“平原”

#### cost function

这时，cost function 不应当是误差平方和了，而是交叉熵损失函数  
$C=-\sum\limits_j t_j log y_j$  
此时$\dfrac{\partial C}{\partial z_i}=y_i-t_i$  

**原因**  
如果残差平方和作为 Cost Function ，误差前向传播算法中，最后一项开始便有$\sigma'(v)$这一项，神经元的输入值较为极端时，$\sigma'(v)=\sigma(v) (1-\sigma(v))$ 接近0.  
用交叉熵作为 Cost Function，最后一层不存在这个问题  
公式自己推导，很简单。  



以二值为例：  
$y=\dfrac{1}{1+e^{-z}}$  
如果cost function是误差平方和$E=0.5(y-t)^2$  
那么$\dfrac{dE}{dz}=(y-t)y(1-y)$  
这时，如果y接近0或接近1，那么学习速度将会非常小  
如果是交叉熵损失函数，$E=-tlog(y)-(1-t)log(1-y)$，这时$\dfrac{dE}{dz}=y-t$  

**模型对照**  
1. 二值时，模型就是logistics回归  
2. 多值时，等价于最大熵模型  


## why deep?
Informally: Thera are functions you can compute with a "small" L-layer deep neural network that shallower networks require exponentially more hidden units to comput.  

这里有一个比喻性的说明：  
你的真实函数是$y=X_1 \mathbf{XOR} X_2 \mathbf{XOR} X_3 \mathbf{XOR} ... \mathbf{XOR} X_n$  
- 深度方案：类似一个二叉树，深度为$\log_2 n+1$，节点个数为$2n-1$（可以自己画一下）  
- 浅层方案：如果限定最多2层，那么hidden layer 需要 $2^{n-1}$ 个节点。



## 神经网络可以拟合任意函数

神经网络可以拟合任意函数，意味着神经网络有某种普遍性。  
例如，你可以把图像识别看成是某个函数，把翻译看成是某个函数。  

这里做一些说明，而非证明。  
1. 拟合指的是某种 **近似**，通过增加隐藏神经元的数目，提高精度  
$\forall \varepsilon>0$（$\varepsilon$是精度）,可以使用足够多的神经元，使得神经元的输出$g(x)$满足$\mid g(x)-f(x)\mid <\varepsilon$,  
2. 对于跳跃函数不能拟合，这是因为神经网络的输出必然是连续的，但是仍然可以用连续函数去近似拟合。


具体过程见于[神经网络与深度学习-chapter4](http://neuralnetworksanddeeplearning.com/chap4.html)

### 1. 分析单输入单输出
1. 单个sigmoid函数可以模拟阶跃函数
2. 多个阶跃函数叠加，可以模拟 if-else 结构
3. 两个阶跃函数可以模拟一个凸起（相反的权重，下一层求和）
4. 多个凸起


### 2. 分析多输入多输出
以2个输入为例，类似地，可以组合一个“塔型凸起”。  
多个凸起可以近似任意函数


### 讨论激活函数
把激活函数改成与sigmoid相似、但中间有很多波动，整个推理仍然成立（仍然可以拟合任意函数）  
思考一下激活函数改成 ReLU 的情况，这种情况仍然可以拟合任意函数  
思考一下激活函数改成线性的情况，这种情况不可以拟合任意函数  
