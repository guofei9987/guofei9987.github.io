---
layout: post
title: 【NLP】主题模型/关键词提取
categories:
tags: 0x24_NLP
keywords:
description:
order: 341
---

任务：给出一段文字，提取其关键字。

## 基于 TF-IDF 的关键词提取

一个很直观的判断：一段文字中，哪个词越多，这个词就是关键词。

问题：高频词不等于关键词
1. 很可能 “的” 是高频的
2. 既是去除这些无意义词，也可能有问题。例如某个体育网站在奥运会期间，可能所有的文章中，“奥运会”都是高频词。

以上两个问题用 TF-IDF 解决。

## TextRank 关键词提取

解决问题：我们没有海量语料如何办？

1. 我们知道 PageRank，根据链接，形成一个图。然后根据这个图，计算每个页面的重要程度。
2. 文本复用类似的思路。一个词是一个“页面”，一个 2-gram 构成一个 “链接”。然后得到一个 Graph，进而计算得到每个词的重要程度。

[代码](https://github.com/hankcs/HanLP/blob/v1.7.5/src/test/java/com/hankcs/book/ch08/DemoPlane.java)


## LDA 主题模型


LDA（Latent Dirichlet Allocation）是一个关于NLP的模型。
区别于另一个LDA(Linear Discrimination Analysis) 是一种有监督降维模型。

先做 CountVectorizer
```python
count_vectorizer = CountVectorizer(max_df=0.95, min_df=2,
                                max_features=n_features,
                                stop_words='english')

X_cnt_vec = count_vectorizer.fit_transform(data_samples)
```

然后lda
```python
lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,
                                learning_method='online',
                                learning_offset=50.,
                                random_state=0)
lda.fit(X_cnt_vec)
```


```python
lda.components_array # array, [n_components, n_features]，意义：components_[i, j]值得是 topic i和 word j 的关系强度。
```

1. n_components: 主题个数
2. doc_topic_prior:先验Dirichlet分布的参数$\theta$，默认1/n_components
3. topic_word_prior:先验Dirichlet分布的参数$\beta$，默认1/n_components
4. learning_method: 即LDA的求解算法。有 ‘batch’ 和 ‘online’两种选择。 ‘batch’即我们在原理篇讲的变分推断EM算法，而"online"即在线变分推断EM算法，在"batch"的基础上引入了分步训练，将训练样本分批，逐步一批批的用样本更新主题词分布的算法。默认是"batch"。如果数据量少，用"batch"，需要调参少。如果数据量多，用 "online" ，速度较快。
5. learning_decay：仅仅在算法使用"online"时有意义，取值最好在(0.5, 1.0]
6. learning_offset：仅仅在算法使用"online"时有意义，取值要大于1。用来减小前面训练样本批次对最终模型的影响。
7. max_iter ：EM算法的最大迭代次数。
8. batch_size: 仅仅在算法使用"online"时有意义， 即每次EM算法迭代时使用的文档样本的数量。
evaluate_every




代码官方文档。 https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html  

理论篇 https://zhuanlan.zhihu.com/p/31470216
