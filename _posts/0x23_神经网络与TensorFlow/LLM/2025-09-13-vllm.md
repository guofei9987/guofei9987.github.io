---
layout: post
title: 【vLLM】学习笔记
categories:
tags: 0x23_深度学习
keywords:
description:
order: 250
---


## KV Cache

是什么？
1. 大模型一个一个吐出 Token，然后把已经输出的所有的 Token 放一起继续计算下一个。
2. 这个过程中，前面的 Token 之间的 K 和 V 是不变的。
3. 如果每次都重新计算它们，就太浪费了。
4. 因此把前面的 K 和 V 存下来，就节省了大量计算资源
5. 也正是因为这个机制，在 LLM 输出时，你不会感觉到越来越慢

但 KV Cache 也是有浪费的
1. 大模型一开始不知道要生成多少 Token，因此按最大预分配
2. 大模型预测第一个 Token 时，也要预分配全部 Cache，随着时间才逐渐用完，这也浪费
3. 显存之间有碎片


vLLM 解决了以上问题，用了若干个方法：
1. 参考操作系统对内存的管理，用 页 来管理 KV Cache 集合，每页 N 个。也引入了逻辑显存、物理显存的概念
2. 在 Prompt 相同时，只存放一份关于 Prompt 的 KV Cache，做法如下
    - 对 Prompt 对应的 KV 块计数，计数
    - Beam Search 



![:caption vllm1](/a/nn/vllm/vllm1.png)



![:caption vllm2](/a/nn/vllm/vllm2.png)


例子

```python
# 安装，要注意 CUDA 版本
# pip install vllm

from vllm import LLM, SamplingParams
prompts = [
    "用 Python 写一个快排函数",
    "描述 AI 的未来"
]

sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_token=10)
outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    print(output.promt)
    print(output.outputs[0].text)
```
