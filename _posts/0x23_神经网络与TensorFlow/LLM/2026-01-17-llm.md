---
layout: post
title: 🔥【Agent】应用全流程
categories:
tags: 0x23_深度学习
keywords:
description:
order: 250
---



## 前置知识


相关资料
- [【LLM】提问的艺术：如何写好prompt](https://www.guofei.site/2023/05/13/prompt.html)
- [【Transformer】Bert和GPT](https://www.guofei.site/2022/10/29/transformer.html)
- DNN训练
    - [https://www.guofei.site/2019/05/04/dnn_train.html](https://www.guofei.site/2019/05/04/dnn_train.html)
    - [https://www.guofei.site/2017/04/01/DNN.html](https://www.guofei.site/2017/04/01/DNN.html)
    - [https://www.guofei.site/2019/05/12/structuring_ml_projects.html](https://www.guofei.site/2019/05/12/structuring_ml_projects.html)



tokenizer 和 embedding，与传统类似

token：文本切分后的“最小单位”，
- 中文：平均一个汉字一个 token，有多个汉字合并为一个 token 的情况，也有1个生僻字拆分成多个 token 的情况
- 英文：1个token=3～4个字符，因此一个单词平均 1～2个 token（长词会更多）

一个展示token的网站：http://tiktokenizer.vercel.app/


embedding：每个token对应的向量，可能是 768、2048、4096 等，取决于模型，也是模型训练时，需要更新的参数。


一个 GPT 模型可视化的网站：https://bbycroft.net/llm

## Prompt


好的问题才有好的答案（另一篇博客）

实际上，RAG、Zero Shot、One Shot、Few Shot 都属于 Prompt


## MCP

LLM 调用工具，有一段演化历史
- 第一代，由 OpenAI 调用各种工具。具体做法是：工具开发者上传一个 yml 文件，yml写明 description、调用方法等，用户使用的时候最多手动选择3个 tool
- 第二代，由应用调用 LLM，然后 LLM 决定调用哪个函数，应用去调用这个函数
- 第三代，模版生成 prompt，对 LLM 做 SFT，使其有调用能力

MCP（Model Context Protocol） 协议是 Claude 母公司 Anthropic 于 2024年11月开源发布，它提供了一种标准化方式连接不同的 LLM、工具


一些 MCP 市场
- mcp.so
- mcpmarket.com
- smithery.ai


MCP 原理


<div class="mermaid">
sequenceDiagram
    autonumber
    participant 用户
    participant Cline
    participant MCP Server
    participant LLM

    rect rgb(240, 248, 255)
    Note over Cline,MCP Server: 启动（只运行一次）
    Cline ->> MCP Server: 启动 MCP Server
    Cline ->> MCP Server: 你好，我是 Cline
    MCP Server -->> Cline: 你好，我是 get_info
    Cline ->> MCP Server: 你有哪些工具？
    MCP Server -->> Cline: 我有 get_my_info,...
    end

    loop 每次用户提问（可反复运行）
    用户 ->> Cline: Guo Fei 的 GitHub 地址是？
    Cline ->> LLM: Guo Fei 的 GitHub 地址是？我还有一些工具...
    LLM -->> Cline: 我要调用 get_my_info，参数是...
    Cline ->> MCP Server: 我要调用 get_my_info，参数是...
    MCP Server -->> Cline: 调用完毕，结果是xxx
    Cline ->> LLM: 调用完毕，结果是xxx
    LLM -->> Cline: Guofei 的 Github 地址是xxx
    Cline -->> 用户: Guofei 的 Github 地址是xxx
    end
</div>


注
1. Cline 可以换成别的，例如 Codex
2. Cline 和 MCP Server 之间的对话都是 XML/Json 格式的，上面的图为了好理解才写成自然语言
3. MCP 协议本身并没有规定如何与 LLM 交互，与 LLM 交互是由 Cline 决定的。Cline 调用 LLM 的 Prompt 极其复杂（几万字），包含 tool 用法、各种指令（ XML、Function Calling 之类的）
    - Cline 与 LLM 的交互可能为多轮，直到得到最终结果
    - 这个多轮交互，可以抽象为 **思考-行动-观察**（Think-Action-Observation）的循环，并最终用一个结束标志（本身是一个 Action）来结束整个流程。这个思想出自 **ReAct**（2022年的一篇论文）
    - 每次交互，本身也是 stream 模型，LLM 会连续吐出 Token



### MCP 实操


**step1：环境配置**
```bash
# 创建 MCP 项目
mkdir my_mcp
cd my_mcp

# 建立虚拟环境
python3 -m venv .venv
source .venv/bin/activate
python -m pip install -U pip
pip install "mcp[cli]"

# 不要手动运行，仅作为测试使用
# python server.py
```

**step2:写代码** 保存为 `server.py`（别的名字也可以，之后的配置按照实际名字更改）

```python
from mcp.server.fastmcp import FastMCP

mcp = FastMCP("get_info", log_level="ERROR")


@mcp.tool()
async def get_my_info(channel: str) -> str:
    # 下面的函数注释会先被 LLM 拿到
    """
    Get Guo Fei's public information by channel.

    Available channels (must match exactly):
    - "个人网站": personal website
    - "GitHub": GitHub profile
    - "知乎": Zhihu profile

    Args:
        channel: One of the predefined channel names above.

    Returns:
        A URL string corresponding to the requested channel.

    If an invalid channel is provided, a readable error message is returned.
    """

    info = {
        "个人网站": "https://www.guofei.site/"
        , "GitHub": "https://github.com/guofei9987/"
        , "知乎": "https://www.zhihu.com/people/guofei9987/answers/by_votes"
    }

    return info.get(channel, f"无效的渠道名称：{channel}。请使用以下之一：{', '.join(info.keys())}")


if __name__ == "__main__":
    mcp.run(transport='stdio')
```



**step3:配置工具**：（用 CLINE）。
- 在 VSCode 中，安装 CLINE
- 在 CLINE 中，配置 MCP
- 配置文件如下：（注意，地址按照实际地址更改，最好是绝对路径）
```json
"get_my_info": {
      "autoApprove": [],
      "disabled": false,
      "timeout": 60,
      "command": "～/my_mcp/.venv/bin/python",
      "args": [
        "～/my_mcp/server.py"
      ],
      "type": "stdio"
    }
```


**step4：使用 agent**
- 输入测试语句：`调用 get-my-info 工具，channel = github`
- agent 返回的输出：`成功调用 get-my-info 工具，channel = github，返回结果为：[](https://github.com/guofei9987/)<https://github.com/guofei9987/>`



**step3-1:别的工具也可以**
- Codex：配置文件是这样的：
```
[mcp_servers.get_my_info]
command = "~/my_mcp/.venv/bin/python"
args = ["~/my_mcp/server.py"]
```


**step4_1:使用**
- 输入：`Guo Fei 的 GitHub 地址是？`
- 输出：`Guo Fei 的 GitHub 地址是 https://github.com/guofei9987/`







## RAG

**为什么需要 RAG？** Prompt 解决不了的问题：
- 缺乏知识：涉及最新的知识（例如某个新上映的电影），或者特定专业知识（例如内网的某个办公应用如何配置），大模型没有学习过，所以表现不好
- 幻觉问题：大模型不了解自己的知识边界，对于欠缺知识的领域仍然尝试回答，错误的回答内容具有迷惑性


解决方案：**RAG**（Retrieval-Augmented Generation，检索增强生成）
- 本质上就是把搜索到的资料（例如企业信息、知识库等）作为提示词的一部分发给 LLM，让 LLM 有根据地输出内容，从而提高回答的准确性、相关性、新鲜度，并解决幻觉问题。
    - OpenAI：RAG 可以将回答准确率从 45% 提升到了 98%
- 相当于给大模型装上了“知识外挂”，基础大模型不用重新训练即可随时调用特定领域知识。省下了重新训练的成本。
- 技术实现上，涉及数据检索、信息增强、AI 生成等多个过程


**RAG步骤**
- 准备阶段
    - **文档结构化**，把 json、pdf、html、图片等等，转化为 txt
    - **分块**（Chunking），把文档切分为小块
        - 常见方法
            - 固定分块：每个分块512个，要有重叠（overlapping）
            - 固定规则：例如用标点、段落分块
            - 语义分块：使用文本类模型做区块识别
        - 分块原则
            - 尽可能保留关键信息
            - 效果评估：后面
    - **Embedding**，把小块映射为向量，可以提高检索性能
    - **构建索引**（Indexing） ，把这些向量存到向量数据库/索引。一些优化策略
        - 根据扩展信息，如标签、摘要、关联问题
        - 链式索引、树索引、图索引
- 调用阶段
    - **Query Rewrite**：把用户问题改写为合适检索的问题
        - 例如，用户问“iPhone15 Pro 和 iPhone16 哪个续航长”，就需要拆分成两个问题去检索
        - 又例如，错别字、不通顺的表达、指代不明、玩梗等
        - *可选*：语义校验模块，用来判断改写的质量
    - **检索**（Retrieval）：各种检索
    - **重排**（Rerank）：目的是提升检索精度、精简后续对模型的输入
        - 常见方法：用模型重排、结合业务策略（时间、用户属性、标签等）
    - **Prompt工程**（核心技术），如何把检索结果很好的放入 Prompt
    - 大模型根据 Prompt 做出回答


加入到 Prompt 是这样的：

```text
"""
基于提供的【相关材料】，回答最后的问题
## 【相关材料】
{context}

## 最后的问题
问题：{input}
"""
```

**RAG 的效果评估**，评估工具：[RAGAS](https://zhuanlan.zhihu.com/p/1897294443570246229)

- 检索的评估
    - 召回率：是否检索出了 Top N 内容
    - 相关性：召回的内容与 query 的相关性
    - 业务维度：例如地域、时效等
- 生成的评估
    - **faithful** 忠实性：答案是否是从召回的内容中推断出来的
    - **answer relevance** 答案相关性：回答和召回内容，与问题的相关性
    - **answer similarity** 答案相似性：回答和标准答案的相似性
    - 事实准确性：回答是否匹配用户问题
    - ...
- 业务指标：用户满意率、首轮成功率


**RAG优化**
1. Chunking 阶段：用模型基于语义做切分
2. rewrite


### RAG 实操

准备工作与说明：
- 一个可以调用大模型的函数 `call_model(payload)`
- 一个原始文档 `价格表.txt`，内容类似（你可以用LLM补充到几百个）：
```
iPhone 15 Pro Max: 7999元
儿童安全座椅：699元
...
```
- 相关的包安装
```sh
pip install -U transformers sentence-transformers huggingface_hub
pip install -U langchain langchain-community langchain-huggingface
pip install -U chromadb # 向量数据库
```
- 模型下载（用一些轻量级的模型）
    - 召回：https://huggingface.co/BAAI/bge-small-zh-v1.5
    - 重排：https://huggingface.co/cross-encoder/mmarco-mMiniLMv2-L12-H384-v1/



说明：各个环节都比较粗暴，实际应用可以按照上面的理论篇对每个环节优化
- Chunking，简单用换行符分割
- 召回模型、重排模型，都用的很小的模型
- 没有做 Query Rewrite


**先不用 RAG 试一下**
```python
query = "iPhone 15 Pro Max 多少钱"


payload = {
    'model': 'DeepSeek-V3.2',
    'messages': [
        {
            'role': 'system',
            'content': '你是一个助手'
        },
        {
            'role': 'user',
            'content': query
        }
    ],
}

res = call_model(payload)

print("未加入 RAG的结果：")
print(res)
```

结果就不贴了，又长又假


**使用 RAG**

```python
from typing import List

query = "iPhone 15 Pro Max 多少钱"


# %% Chunking
def split_into_chunks(doc_file: str) -> List[str]:
    with open(doc_file, 'r') as f:
        content = f.read()
    return content.split('\n')


chunks = split_into_chunks("价格表.txt")

# %% Embedding

from sentence_transformers import SentenceTransformer

embed_model = SentenceTransformer("./bge-small-zh-v1.5")



def embed_chunk(chunk: str) -> List[float]:
    embedding = embed_model.encode(chunk)
    return embedding.tolist()


test_embedding = embed_chunk("测试")
print("embedding 的维度", len(test_embedding))

embeddings = [embed_chunk(chunk) for chunk in chunks]

print("embedding 数量（片段个数）", len(embeddings))

# %% embedding 存入向量数据库中

import chromadb

# 数据存在内存中
chromadb_client = chromadb.EphemeralClient()
chromadb_collection = chromadb_client.get_or_create_collection(name="default")


def save_embedding(chunks: List[str], embeddings: List[List[float]]) -> None:
    ids = [str(i) for i in range(len(embeddings))]
    chromadb_collection.add(
        documents=chunks,
        embeddings=embeddings,
        ids=ids
    )


save_embedding(chunks, embeddings)


# %% 召回

def retrieve(query: str, top_k: int) -> List[str]:
    query_embedding = embed_chunk(query)
    results = chromadb_collection.query(
        query_embeddings=query_embedding,
        n_results=top_k
    )
    return results['documents'][0]


retrieved_chunks = retrieve(query, 10)

print("召回：", retrieved_chunks)

# %% 重排
from sentence_transformers import CrossEncoder

cross_encoder = CrossEncoder("./mmarco-mMiniLMv2-L12-H384-v1")


def rerank(query: str, retrieved_chunks: List[str], top_k: int = 3) -> List[str]:
    pairs = [(query, chunk) for chunk in retrieved_chunks]
    scores = cross_encoder.predict(pairs)

    chunk_with_scores = [(chunk, score)
                         for chunk, score in zip(retrieved_chunks, scores)]

    chunk_with_scores.sort(key=lambda x: x[1], reverse=True)

    return [chunk for chunk, _ in chunk_with_scores][:top_k]


reranked_chunks = rerank(query, retrieved_chunks)

print("重排后：", reranked_chunks)

# %% 测试结果：

query = "皮鞋多少钱"

retrieved_chunks = retrieve(query, top_k=10)
reranked_chunks = rerank(query, retrieved_chunks, top_k=3)

print("query:", query)
print("retrieved_chunks:", retrieved_chunks)
print("reranked_chunks:", reranked_chunks)

payload = {
    'model': 'DeepSeek-V3.2',
    'messages': [
        {
            'role': 'system',
            'content': f'''你是一个助手，请给予【相关材料】回答问题
# 【相关材料】
{"\n".join(reranked_chunks)}
            '''
        },
        {
            'role': 'user',
            'content': query
        }
    ],
}

res = call_model(payload)

print("加入 RAG的结果：")
print(res)
```


<!-- 参考资料：https://www.douyin.com/video/7523955114718088487 -->

### RAG 工程选型

embedding
- 有哪些
- 榜单
- xxx





向量数据库
- FAISS、Elasticsearch、Milvus、Pinecon





## Agent

Agent 是包含了 RAG、提示词、Function calling、Re-Act等一系列元素的集合
- API 调用，例如查询天气、发送邮件
- Re-Act


**自主Agent**
- 由 Planner 模块（也是个LLM），与其它 LLM、工具调用的配合



## Fine-tuning


**Scaling Law** LLaMA:即使是7B模型，训练数据继续扩展到 1T token，其性能仍然会上升


**为什么需要微调？**
- 更改提示词方便快捷，但是有上限：
- 领域模型经验难以描述
- prompt 太长，遵循程度低。
- prompt 遵循有幻觉
- 哪些任务适合 SFT？
- Prompt 放不下，例如某个系统有数万条规则。
- 一些新的思考方式，例如日志分析大模型，可以让它学会沿着报错 stack 分析
- 某些文风（例如角色扮演）、风险偏好对齐等等


**有哪些思路**
- 大模型（如 DeepSeek-671B）在特定领域的知识转移到小模型如（Qwen-0.6B）。如此，可以在推理时获得双方的优点：小模型的QPS、小资源消耗，接近大模型准确率。
    - 方法和流程：大模型 -> 获得COT数据 -> 蒸馏小模型
- 大模型->专家经验（Ground truth）->蒸馏小模型
    - 方法和流程：专家用人工的方式提供答案/推理（可以借助大模型，或者不借助大模型）。成本较高。
- **RSFT**（拒绝采样 SFT）：
    - 传统 SFT 数据是用大模型（如 DeepSeek-R1）生成 CoT 和答案。然后剔除低质量的，用于训练小模型。
    - RSFT 是让小模型A产生 CoT 和答案，用大模型判断质量并筛选，然后训练小模型A。
    - 因为训练数据来自 A 自己，它是会这些的，只是思路不能收敛于正确答案。这样可以帮助其思路收敛，因此效果很好。
- **RFT**：RL+SFT，击溃是 RSFT 的基础上，用高质量的一组问题做回答和奖惩。




**什么时候需要？**
- 如果是通识场景，不需要 SFT
- 如果场景比较冷僻，就需要 SFT

**有哪些方法**
- 全量微调（Full Finetune）：所有参数都更新
- 参数高效微调（PEFT，Parameter-Efficient-Finetune），仅调整部分参数
    - LoRA：通过低秩矩阵分解模拟权重更新
    - 还有 Adapter、Prefix Tuning 等
    - QLoRA：是量化 + LoRA，冻结的基座模型用 4bit 放显存
- 强化学习（Reinforcement Learning）
    - 人类反馈强化学习（RLHF）：人类对模型输出评价，使其符合人类价值观和任务需求
    - 近端策略优化（PPO）：通过策略梯度更新
    - 直接偏好优化（DPO）：无需显式训练奖励，是PPO的简化
    - 组相对策略优化（GRPO）




**base model 选择**
- 复杂SFT任务：选 7B 以上
- 高性能任务：选 1B 以下



### 获取数据

**方法1直接用大模型生成COT**。例如，直接用基座模型（例如 DeepSeek-R1/V3， ChatGPT）生成COT数据

**方法2设计多智能体** 例如，设计2个智能体。一个扮演客服专家，一个扮演客户
- 客服专家产生话术
- 客户提出意见，例如，“增加友好和关心的语气”、“这段话术稍显冗长”
- 客服专家根据意见做优化
- 数轮之后（3-5轮），得到优化后的话术。
- 用另一个 LLM 做一轮过滤，得到最终的数据集


**方法3:传统方法**
- 业务系统收集（预训练一般不用这个，因为有数据泄露风险，但 SFT 经常需要）
- 开源数据集
    - 开源数据集
    - HuggingFace、GitHub
- 爬虫。爬到的数据，解析阶段也可以用 LLM 了
- 采买：题库、书籍、教材
- ...


### 高质量训练数据

数据不需要太多：*Meta《LIMA》：1k优质样本即可打败海量普通数据*。因此，获取 **高质量训练数据** 是关键

**数据质量评估**
- **准确**
    - **无事实错误**
    - **一致性**：内部处理逻辑统一，数据之间没有互相矛盾
    - 其它问题，拼写检查与纠正、文本规范化（如有必要的话，例如小写化、去除特殊字符）、确定适当的最大文本长度。
    - 方法和工具：
        - 人工质量抽检
        - rule-based：格式准确性等多重规则
        - LLM as judge
        - 构建静态指标：例如，输出长度/PPL等
            - **PPL** （困惑度，perplexity）的计算：预测下一个词时的“平均交叉熵”的指数。它大于1，并且越大代表越不确定。
            - **PPL** 过高和过低都要剔除。过低代表不必再学了，过高的也是学不会。
        - 借用开源 reward model/微调若干大模型来实现数据质量奖励/训练小模型
- **完整性** （覆盖任务要点）
    - 覆盖全部需要的 **任务类型**
    - **难度梯度** 简单/中等/复杂任务比例合理
    - 覆盖全部需要的 **文本主题** 
    - **数据洁净度**：剔除重复、噪声、错误的样本
    - 去除冗余数据
    - 正负样本均衡
    - 样本相关性，捕捉数据集的上下文和语义多样性。（生成的指令和其最相似的种子指令之间的ROUGE-L分数分布。）
    - kmeans/k-center（计算复杂度较低）， 挑选出多样性的数据，即Seed Instruction Data。这步关注的是 coverage。
- **简洁性**：（无冗余信息）
    - 高质量的重复数据，可以提升模型效果。但中等质量会浪费计算资源、降低泛化性。
    

实操流程
- 去重：**md5**、**MinHash**
- 低质量过滤
    - 关键词过滤、规则过滤
    - 用”小“模型（例如 Qwen-7B）计算 PPL（PPL太低、太高都不适合训练）
    - ...
- 去毒：黄、赌、毒、涉政、隐私去除
- 数据集整体评估：消融实验。
    - 是什么：把新的数据集，在之前模型的 checkpoint 上继续训练，观察其在各个验证集（榜单）上的表现
    - 目的：评估新数据集是否：1）提供增量知识，2）不损害已训练的效果
    - 实验结果：效果提升，说明有提升；效果下降，说明损害了已训练的效果；效果不变，说明新数据集没有带来提升
    - 难点：多个验证集，有的提升，有的下降，怎么办？





**业务知识如何融入**
- 专家的感性知识。典型历史案例 -> （大模型）抽取专家知识 -> 沉淀为业务知识
- 策略/正则表达式/规则 -> 翻译成人话 -> 沉淀为专家经验
- 业务专有名称 -> 整理出业务“词典”







## SFT


如何训练
- loss. SFT 时，一般会 mask 掉 prompt 部分的 loss，只学习输出部分的 loss
- 学习率. 一般是预训练的学习率的 0.1 倍
- epoch. 小样本（<1万）5个，大样本（>1万）2个
- 混入 30%-50% 高质量预训练数据（通用数据），防止灾难性遗忘，保持泛化能力

关键指标
- 业务合格率：从业务出发，人工对结果打标，给出 good/ok/bad，并计算指标
- 提升率：指标比上一个版本提升了多少


LoRA（Low-Rank Adaptation of Large Language Models）
- 核心原理是矩阵分解 $\Delta W_{d\times k} = B_{d\times r} A_{r\times k}$，当 r 较小时，A 和 B 的参数量就很小
    - 训练时冻结原来的模型参数 $W$，
    - 随机初始化 $A$ 和 $B$，训练时更新的是  $A$ 和 $B$
    - 最后按照 $W + \Delta W = W + AB$ 来更新参数
- 需要调整的参数少
- 还有利于多卡并行训练
- 因此很快、显存占用也低
    - 具体来说，全参训练相比需要计算全部参数的梯度（显存角度：需要存储全部梯度以及优化器状态）；而 LoRA 不需要这些，但是额外需要计算 LoRA 部分的前向、梯度，但这个占比只有1%-3%
- 推理时，有 sLoRA 这样的技术，使其共享预训练模型参数，进一步降低推理成本。


LoRA 评价
- 最终效果低于全量微调
- 如果基座模型表现不好，LoRA 大概率不好（因为只改动了一部分参数）
- LoRA 记忆力不如全参微调

一个专门解数学题的7B模型训练步骤：
- step1: 用 Math-500（开源数据集）做 SFT
- step2: 现价一个难度过滤器，保证每次训练正好是有些难度的
- step3: 蒸馏超大模型的 CoT
- step4: 改善 CoT，这个例子的实验表示，CoT 要简繁适中、给出多个思路。

数据量经验值：最少100条，以 5000条为佳



## GRPO

区别：
- SFT：用大模型准备数据，给小模型来训练
- RL：问题描述（Initial State 无Output），只需要足够多样的问题，无需答案

为什么：“SFT负责记忆，RL负责泛化”
- SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training，https://arxiv.org/abs/2501.17161


大模型相关的 RL 发展历史：
1. PPO：用了4个模型：Actor Model（大模型本身），Ref Model（冻结的模型），Reward Model，Critic Model（对每个状态的评估，预测未来的潜在回报）
2. DPO：用一些数学技巧，省去了 Critic Model、Reward Model、Ref Model，只需要 Actor Model 和 accept/reject 。性能很好。Reward 很难写。
3. GRPO，用组内相对结果来计算奖励，无需 Critic（节省内存）



GRPO（Group Relative Policy Optimization）包含3个模型
- Actor Model：是训练过程中需要优化的核心策略模型，负责根据输入生成候选输出。通过生成多个输出（如数学题的多解方案）进行群体内对比。
- Reward Model：用于评估Actor生成输出的质量，为每个输出分配奖励值。通过群体归一化（如减去均值、除以标准差）将绝对奖励转换为相对优势，替代传统优势函数计算。
    - **这个部分也可以是个规则，而不是 Model**，例如：
    - 格式奖励（例如是否有think 和 answer，例如格式是否符合预期）
    - 结果正确性奖励。有些问题可以用规则判断
    - 其它奖励：
        - 语言一致性
        - 长度奖励
        - 重复性惩罚
        - 思考过程奖励，条理性、逻辑性
- Reference Model：是一个冻结（参数不更新）的基准模型，用于计算KL散度（Kullback-Leibler Divergence），约束Actor Model的策略更新幅度，防止其过度偏离初始策略。


GRPO的算法流程
1. 分组采样响应。对同一个 Prompt 生成一组响应（4-8个）
2. 组内评分语排名。根据Reward Model（或规则），对每个响应评分，然后，相对优势=个体得分-组平均分
3. 策略优化方向。强化学习得分相对优势高的，抑制相对优势低的。


## Post-Pretrain

使用大量数据，在一个已经训练好的模型上继续训练。


举例：有一个英文的大模型，想做一个中文+金融大模型。做法：


![:caption Post-Pretrain](/a/nn/llm/post_pretrain.svg)


Post-Pretrain 和 SFT 的区别
- SFT 主要是问答格式的语料，Post-Pretrain 是普通的语料
- loss 有对应的调整


## 性能与资源

一般结论：推理需要的显存为模型大小的2～3倍；后训练需要8～10倍；LoRA 后训要20%这个量级。

影响因素
- 模型本身的变量个数，例如 Qwen-14B 有 14Billion 个参数；Qwen-7B 有 7Billion 个参数
- 变量类型：Int8 需要 1个字节；FP16/BF16 需要2个字节；FP32 需要4个字节
    - 推理时用小精度：叫做 **量化**
    - 最常用的：BF16（范围大、精度小）
    - 游戏卡可以跑大模型，不能做工业渲染
- **推理** 假设参数量为 X，用 BF16，全部参数占用 2X，KV缓存留 20%，其它损耗留 10%，共 `2 * 1.3 = 2.6`（经验值是 2～3 倍）
- **训练** 参数 2X，梯度 2X，优化器 Adam 4X、激活和损耗 30%，`2X(1 + 1 + 2 + 0.3) = 8.6X`


推理性能指标
- 首字延迟（TFFT）。模型开始计算，到产生第一个 Token 消耗的时间。
    - 这个时间内，大模型会把输入的 Prompt 逐 Token 计算一边，并建立 KV Cache
    - 通常是秒级
- 生成速度（TPOT）。之后的 Token 生成速度。
    - 通常是几十毫秒级
- QPS

性能提高技术
- 并行。这里有很多技术
    - 并行训练，多个batch并行处理，然后汇聚梯度
    - 把大矩阵乘法分解到多个卡上并行，计算速度提升，但通信开销变大。
    - 基础工具：PyTorch Distributed、DeepSpeed、通信协议 NCLL（NVIDIA Collective Communications Library，为 GPU 优化的通信库，是 Pytorch 的基础之一）
- 用 BP16，而不是 FP32
- prefix cache
    - 适用于 query 有共同的前缀。例如系统提示词都一样。
- continuous batch：按 batch，把一批需要预测的放到一起。（类似向量化运算）
- 量化（Quantization） 学术界非常多的方法提出
    - 例如，FP32 转 INT8
- 剪枝（Pruning）：去除接近0的权重
- 稀疏化。
    - 压缩输入，只选取重要的 Token（gemfilter）；
    - 压缩 MLP 计算（对 FNN 模块做剪枝）
    - 压缩 attention （SeerAttention）
- 一些简单的任务，路由到更小的模型
- 如果模型只处理特点领域的任务，考虑 distill
- 大小模型配合（相关技术复杂）



## 大模型评估



测评大模型（*A Survey on Evaluation of Language Models*）https://arxiv.org/abs/2307.03109
- 验证性能和能力
- 评估泛化能力：在未见过的数据上也保持良好
- 明确不同的大模型差异：了解其优势领域
- 防止大模型风险：安全漏洞之类的
- 知道大模型改进：识别模型弱点、提供优化方向

一些知识点
- 有时候，多一个换行符，可能对结果产生较大影响

测评方法
- 在有标准答案的数据集上评测：如何提取模型的答案
- 针对选择题，计算每个选项的 PPL
- 对抗测评：在另一个模型上得到回答，比较 A 回答 和 B 回答的好坏。解决的问题：有时候单纯看答案，不能很好的标注回答的好坏。
    - 与基线模型对抗
    - 多个模型两两对抗


评估问题
- 封闭式问题：使用预先已经有的答案完成测评
- 开放式问题：
    - 人工测评
    - 大模型测评



----------------

**安全评估**
- 内生安全：在数据和模型层面的安全
    - 数据投毒：操纵训练样本，使 LLM 在特定触发条件下输出有害结果
    - 模型幻觉。诸如医疗、金融、法律场景可能造成严重后果。主要靠 RAG、MCP 
        - 引证生成：要求模型标记来源
            - 方式1:inline citations，用 prompt 约束：“每句必须带引用”、“只允许引用给定 sources 列表”、“引用必须来自支持该句的原文，不支持就说不知道”。缺点是仍然会有幻觉
            - 方式2:post-hoc attribution，先让模型生成不带引用的答案，对每个 claim 做证据匹配（1. 用 embedding 来匹配，2. 用 LLM 来匹配）。更可控，但工程复杂、耗时高
        - self-RAG：由 LLM 自动决定是否检索、检索什么、如何使用证据、何时停止
    - 价值观问题
- 服务安全：AI+、智能体、各种服务的漏洞
    - 数据泄露（用户隐私、提示词之类的）
    - 安全攻击（类比传统的互联网攻击）
    - 供应链风险
    - ...
- 使用上的安全、未来的安全问题
    - 伪造
    - 虚假新闻
    - 错误的重大建议（医疗、金融）

评估的板块
- 内容安全：黄赌毒、迷信、恐怖暴力
- 意识形态
- 数据安全
- 伦理：身心健康、公序良俗、礼貌、防歧视
- 合规：符合国家的部门规章（金融、医疗、政务等）
- 自我认知：xx公司开发的xx模型
- 真实性


攻击手法
- 指令劫持
    - 口令复述
    - 正反介绍
    - 循序渐进
    - 情景带入
    - ...
- 绕过检测
    - 内涵
    - 藏头诗
    - 嵌套
- ...


模型水印
- 模型文件水印、AIGC标识
- 参数级水印
- 行为级水印：特定输入，使其呈现特殊的输出
- 数据水印：“善意”版本的数据投毒



## 一些 AI 产品架构


**一、转发回复架构**

![:caption 转发回复架构](/a/nn/llm/agent_structure1.svg)


这个是最简单的架构，常用于聊天 app


**分拆任务架构**


<div class="mermaid">
graph TB
A[用户请求]
B[前端]

subgraph S[ ]
direction LR
C[后端]
E1((模型1))
E2((模型2))
E3((模型3))
D[数据库]
end

A --> B
B --> A
B --> C
C --> B
C --> D
D --> C
C --> E1
E1 --> C
C --> E2
E2 --> C
C --> E3
E3 --> C
</div>

举例：
- 论文阅读助手，会有多个模型，做不同的任务
- AI PPT，有的模型规划大纲，有的模型画结构，有的模型填内容


**三、事件触发**


<div class="mermaid">
graph TB
A[其它系统]
D[动作]

A --->|触发（事件、定时）| B

subgraph S[ ]
direction LR
B[后端系统]
C[Agent]
end

B --> C
C --> D
</div>

例子：
- 新闻订阅
- 金融事件响应系统
- 市场调研助手：问卷收集到一定数量后，启动流程，最终给用户输出报告


**感知启动**


![:caption 感知启动](/a/nn/llm/agent_structure4.svg)

例子：法律咨询助手
- Agent：观察对话、理解要点、判断潜在需求、匹配类似案件等


**其它：以上架构的组合**


## 多Agent

整体思路
- 角色分解：
    - supervisor：只负责需求拆解、分发任务，没有执行权限
    - 执行者：负责执行具体的任务
    - SOP标准化
- 人设隔离，划分不同的人设，不同的人可使用不同的工具集
    - 人设：“你是一个资深Java开发，只负责开发后端代码”
    - 工具：例如检索、Python解释器、数据库读写
- 通信机制
    - 点对点传递：适合线形任务
    - 共享黑板：全局共享当前状态，适合复杂任务
- 协作模式
    - **层级式**：Boss -> Leader -> Worker
        - 中心化调度
        - 效率高
    - **辩论式**： Agent A VS Agent B
        - 多轮互斥讨论和 review，准确率高
    - **流水线**：固定顺序执行，适合 SOP 明确的任务







问题
- 错误叠加，整体出错概率极高
- 蝴蝶效应，错误传导
- 解决方案：
    - 断点续传
    - agent判断调用错误状态，并自动行动




工程经验
- 多 Agent 不能互相耦合。
    - 否则：风格（思路）不统一，即使它们共享上下文；
    - 决策与执行尽量用同一个模型。
    - 可以拆出来的：例如一个子Agent去查询、学习一个生僻的库，而绝不能用它来写代码。






## 一些方向

### text2sql

流程
1. 用户问题
2. 构建 prompt
    - 选择 schema
    - 构建的 Prompt 要求：简洁、严格规定不准自由发挥、不准解释，可以添加 few shot
3. 生成 SQL
    - 用一个单独的 LLM 来执行
    - 可以生成 N 份，然后让模型选择最优的
4. 校验
    - 规则校验
    - 模型校验
5. 执行
    - 如有报错，则返回给大模型
6. （如有必要）解释结果，结构化数据转文本
