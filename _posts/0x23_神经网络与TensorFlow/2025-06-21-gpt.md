---
layout: post
title: 【LLM】GPT 模型
categories:
tags: 0x23_深度学习
keywords:
description:
order: 250
---




## 历史

Bert vs GPT

**共同点**：GPT（Generative Pre-trained Transformer）和 BERT（Bidirectional Encoder Representations from Transformers） 都是基于 Transformer 的预训练语言模型

差别：

| 方面 | Bert | GPT |
|-----|-------|-----|
|模型结构 | Encoder-only | Decoder-only |
|  训练目标   |  去噪自编码（Denoising Autoencoder），主要 MLM，把句子里一部分词遮住再预测；常配合 NSP/替代任务   | 自回归（Autoregressive），按顺序预测下一个词（Next Token Prediction）
|  注意力方向   | 双向注意力（同时看左右上下文），更擅长“理解/表征” | 因果/单向注意力（只能看左边上下文），更适合“续写/生成”
| 核心能力倾向    |  强在理解（分类、实体识别、检索排序、语义匹配等判别式任务）   | 强在生成（对话、写作、代码、摘要、翻译等生成式任务）
|  下游使用方式   |  通常在 [CLS] 向量等表征上接一个分类/标注头进行微调   | 常用“提示词/指令 + 解码生成”，也可微调做特定生成任务
|  输出形式   |   输出每个 token 的上下文化向量表示，再用于判别或填空  | 输出一段可继续生成的文本序列
| 典型应用举例 | 情感分类、意图识别、命名实体识别、搜索/匹配 | 聊天机器人、内容创作、代码助手、长文总结



**Scaling Law** LLaMA:即使是7B模型，训练数据继续扩展到 1T token，其性能仍然会上升




GPT-1 到 ChatGPT 的演化
- GPT1: 预训练——微调
- GPT2: 引入 prompt，拥有 ZeroShot learning 能力
- GPT3: 有 FewShot learning 能力
- InstructGPT

ChatGPT
- 基于大规模预训练语言模型（GPT-3，一般也称作 base 模型）进一步训练
- 通过在人工标注和反馈的数据上进行指令对齐，从而有更好的对话能力（ChatGPT，一般也称为 chat 模型）


阶段一：SFT（supervised fine-tuning）
- 人工收集、标注训练样本，有监督地微调 GPT-3.5 模型
    - 问题来源：1）标注员 2）ChatGPT的用户
    - 答案（demonstration）有标注员填写，标准：有用、无害、真实
    - 此阶段 1.3万+ 样本量
- 通过有监督微调，赋予 GPT 模型能理解人类的复杂指令
    - Plain：标注人员构思问题和答案
    - Few-shot
    - User-based：根据用户提交的问题改写
    - API prompt：从 OpenAI的 API调用种抽取的 Prompt
阶段二：RM（Reward Model）
- 人工对模型输出候选结果进行排序，训练奖励模型
    - 同一个问题，模型输出多个回答，标注员对其排序
    - RLHF（Reinforcement Learning from Human Feedback）
- 奖励模型以人类的视角来衡量模型的表现

阶段三：PPO（Proximal Policy Optimization）
- 通过 RM，利用强化学习 PPO 算法对模型进一步训练
- 重复阶段二和阶段三，从而使得模型不断增强

## Prompt


好的问题才有好的答案（另一篇博客）

实际上，RAG、Zero Shot、One Shot、Few Shot 都属于 Prompt


## MCP

LLM 调用工具，有一段演化历史
- 第一代，由 OpenAI 调用各种工具。具体做法是：工具开发者上传一个 yml 文件，yml写明 description、调用方法等，用户使用的时候最多手动选择3个 tool
- 第二代，由应用调用 LLM，然后 LLM 决定调用哪个函数，应用去调用这个函数
- 第三代，模版生成 prompt，对 LLM 做 SFT，使其有调用能力

MCP 协议是 Claude 母公司 Anthropic 于 2024年11月开源发布，它提供了一种标准化方式连接不同的 LLM、工具



## RAG

**为什么需要 RAG？** Prompt 解决不了的问题：
- 缺乏知识：涉及最新的知识（例如某个新上映的电影），或者特定专业知识（例如内网的某个办公应用如何配置），大模型没有学习过，所以表现不好
- 幻觉问题：大模型不了解自己的知识边界，对于欠缺知识的领域仍然尝试回答，错误的回答内容具有迷惑性


解决方案：**RAG**（Retrieval-Augmented Generation，检索增强生成）
- 本质上就是把搜索到的资料（例如企业信息、知识库等）作为提示词的一部分发给 LLM，让 LLM 有根据地输出内容，从而提高回答的准确性、相关性、新鲜度，并解决幻觉问题。
    - OpenAI：RAG 可以将回答准确率从 45% 提升到了 98%
- 相当于给大模型装上了“知识外挂”，基础大模型不用重新训练即可随时调用特定领域知识。省下了重新训练的成本。
- 技术实现上，涉及数据检索、信息增强、AI 生成等多个过程


**RAG步骤**
- 准备阶段
    - **文档结构化**，把 json、pdf、html、图片等等，转化为 txt
    - **分块**（Chunking），把文档切分为小块
        - 常见方法
            - 固定分块：每个分块512个，要有重叠（overlapping）
            - 固定规则：例如用标点、段落分块
            - 语义分块：使用文本类模型做区块识别
        - 分块原则
            - 尽可能保留关键信息
            - 效果评估：后面
    - **Embedding**，把小块映射为向量，可以提高检索性能
    - **构建索引**（Indexing） ，把这些向量存到向量数据库/索引。一些优化策略
        - 根据扩展信息，如标签、摘要、关联问题
        - 链式索引、树索引、图索引
- 调用阶段
    - **Query Rewrite**：把用户问题改写为合适检索的问题
        - 例如，用户问“iPhone15 Pro 和 iPhone16 哪个续航长”，就需要拆分成两个问题去检索
    - **检索**（Retrieval）：各种检索
    - **重排**（Rerank）：目的是提升检索精度、精简后续对模型的输入
        - 常见方法：用模型重排、结合业务策略（时间、用户属性、标签等）
    - **Prompt工程**（核心技术），如何把检索结果很好的放入 Prompt
    - 大模型根据 Prompt 做出回答


加入到 Prompt 是这样的：

```text
"""
基于提供的【相关材料】，回答最后的问题
## 【相关材料】
{context}

## 最后的问题
问题：{input}
"""
```

RAG 的效果评估

检索的评估
- 召回率：是否检索出了 Top N 内容
- 相关性：召回的内容与 query 的相关性
- 业务维度：例如地域、时效等

生成的评估
- **faithful** 忠实性：答案是否是从召回的内容中推断出来的
- **answer relevance** 答案相关性：回答和召回内容，与问题的相关性
- **answer similarity** 答案相似性：回答和标准答案的相似性
- 事实准确性：回答是否匹配用户问题
- ...

RAG 评估工具：[RAGAS](https://zhuanlan.zhihu.com/p/1897294443570246229)





## Agent

Agent 是包含了 RAG、提示词、Function calling、Re-Act等一系列元素的集合
- API 调用，例如查询天气、发送邮件
- Re-Act


## 模型评估


两种
- 封闭式问题：使用预先已经有的答案完成测评
- 开放式问题：
    - 人工测评
    - 大模型测评

## 微调

**两个思路**
- 大模型（如 DeepSeek-671B）在特定领域的知识转移到小模型如（Qwen-0.8B）。如此，可以在推理时获得双方的优点：小模型的QPS、小资源消耗，接近大模型准确率。
    - 方法和流程：大模型 -> 获得COT数据 -> 蒸馏小模型
- 大模型->专家经验（Ground truth）->蒸馏小模型
    - 方法和流程：专家用人工的方式提供答案/推理（可以借助大模型，或者不借助大模型）。成本较高。


**什么时候需要？**
- 如果是通识场景，不需要 SFT
- 如果场景比较冷僻，就需要 SFT

**有哪些方法**
- 全参数微调：更新所有参数，需要大量高质量领域数据
- 部分参数微调：只调整关键参数
    - LoRA
    - QLoRA

**base model 选择**
- 复杂SFT任务：选 7B 以上
- 高性能任务：选 1B 以下



### 获取数据

**方法1直接用大模型生成COT**。例如，直接用基座模型（例如 DeepSeek-R1/V3， ChatGPT）生成COT数据

**方法2设计多智能体** 例如，设计2个智能体。一个扮演客服专家，一个扮演客户
- 客服专家产生话术
- 客户提出意见，例如，“增加友好和关心的语气”、“这段话术稍显冗长”
- 客服专家根据意见做优化
- 数轮之后（3-5轮），得到优化后的话术。
- 用另一个 LLM 做一轮过滤，得到最终的数据集


**方法3:传统方法**
- 业务系统收集（预训练一般不用这个，因为有数据泄露风险，但 SFT 经常需要）
- 开源数据集
    - 开源数据集
    - HuggingFace、GitHub
- 爬虫。爬到的数据，解析阶段也可以用 LLM 了
- 采买：题库、书籍、教材
- ...


### 高质量训练数据

数据不需要太多：*Meta《LIMA》：1k优质样本即可打败海量普通数据*。因此，获取 **高质量训练数据** 是关键

**数据质量评估**
- **准确**
    - **无事实错误**
    - **一致性**：内部处理逻辑统一，数据之间没有互相矛盾
    - 其它问题，拼写检查与纠正、文本规范化（如有必要的话，例如小写化、去除特殊字符）、确定适当的最大文本长度。
    - 方法和工具：
        - 人工质量抽检
        - rule-based：格式准确性等多重规则
        - LLM as judge
        - 构建静态指标：例如，输出长度/PPL等
            - **PPL** （困惑度，perplexity）的计算：预测下一个词时的“平均交叉熵”的指数。它大于1，并且越大代表越不确定。
            - **PPL** 过高和过低都要剔除。过低代表不必再学了，过高的也是学不会。
        - 借用开源 reward model/微调若干大模型来实现数据质量奖励/训练小模型
- **完整性** （覆盖任务要点）
    - 覆盖全部需要的 **任务类型**
    - **难度梯度** 简单/中等/复杂任务比例合理
    - 覆盖全部需要的 **文本主题** 
    - **数据洁净度**：剔除重复、噪声、错误的样本
    - 去除冗余数据
    - 正负样本均衡
    - 样本相关性，捕捉数据集的上下文和语义多样性。（生成的指令和其最相似的种子指令之间的ROUGE-L分数分布。）
    - kmeans/k-center（计算复杂度较低）， 挑选出多样性的数据，即Seed Instruction Data。这步关注的是 coverage。
- **简洁性**：（无冗余信息）
    - 高质量的重复数据，可以提升模型效果。但中等质量会浪费计算资源、降低泛化性。
    

一些实操方法
- 去重：**md5**、**MinHash**
- 低质量过滤：用”小“模型（例如 Qwen-7B）计算 PPL（PPL较低可能是低质量数据？）
- 数据集整体评估：消融实验。
    - 是什么：把新的数据集，在之前模型的 checkpoint 上继续训练，观察其在各个验证集（榜单）上的表现
    - 目的：评估新数据集是否：1）提供增量知识，2）不损害已训练的效果
    - 实验结果：效果提升，说明有提升；效果下降，说明损害了已训练的效果；效果不变，说明新数据集没有带来提升
    - 难点：多个验证集，有的提升，有的下降，怎么办？





**业务知识如何融入**
- 专家的感性知识。典型历史案例 -> （大模型）抽取专家知识 -> 沉淀为业务知识
- 策略/正则表达式/规则 -> 翻译成人话 -> 沉淀为专家经验
- 业务专有名称 -> 整理出业务“词典”


### 资源评估

一般结论：推理需要的显存为模型大小的2～3倍；后训练需要8～10倍；LoRA 后训要20%这个量级。

影响因素
- 模型本身的变量个数，例如 Qwen-14B 有 14Billion 个参数；Qwen-7B 有 7Billion 个参数
- 变量类型：Int8 需要 1个字节；FP16/BF16 需要2个字节；FP32 需要4个字节
    - 推理时用小精度：叫做 **量化**
    - 最常用的：BF16（范围大、精度小）
    - 游戏卡可以跑大模型，不能做工业渲染
- **推理** 假设参数量为 X，用 BF16，全部参数占用 2X，KV缓存留 20%，其它损耗留 10%，共 `2 * 1.3 = 2.6`（经验值是 2～3 倍）
- **训练** 参数 2X，梯度 2X，优化器 Adam 4X、激活和损耗 30%，`2X(1 + 1 + 2 + 0.3) = 8.6X`


推理性能指标
- 首字延迟（TFFT）。模型开始计算，到产生第一个 Token 消耗的时间。
    - 这个时间内，大模型会把输入的 Prompt 逐 Token 计算一边，并建立 KV Cache
    - 通常是秒级
- 生成速度（TPOT）。之后的 Token 生成速度。
    - 通常是几十毫秒级
- QPS

性能提高技术
- 并行
    - 很多技术。例如把大矩阵乘法分解到多个卡上并行，计算速度提升，通信开销变大。
- prefix cache
    - 适用于 query 有共同的前缀。例如系统提示词都一样。
- continuous batch：按 batch，把一批需要预测的放到一起。
- 大小模型配合（相关技术复杂）
- 有损优化
    - 量化（学术界非常多的方法提出）
    - 稀疏化。
        - 压缩输入，只选取重要的 Token（gemfilter）；
        - 压缩 MLP 计算（对 FNN 模块做剪枝）
        - 压缩 attention （SeerAttention）


## SFT



有哪些：
- 全量微调（Full Finetune）：所有阐述都更新
- 参数高效为套（PEFT，Parameter-Efficient-Finetune），仅调整部分参数
    - LoRA：通过低秩矩阵分解模拟权重更新
    - 还有 Adapter、Prefix Tuning 等
- 强化学习（Reinforcement Learning）
    - 人类反抗强化学习（RLHF）：人类对模型输出评价，使其符合人类价值观和任务需求
    - 近端策略优化（PPO）：通过策略梯度更新
    - 直接偏好优化（DPO）：无需显式训练奖励，是PPO的简化
    - 组相对策略优化（GRPO）



为什么？
- 更改提示词方便快捷，但是有上限：
- 领域模型经验难以描述
- prompt 太长，遵循程度低。
- prompt 遵循有幻觉
- 哪些任务适合 SFT？
- Prompt 放不下，例如某个系统有数万条规则。
- 一些新的思考方式，例如日志分析大模型，可以让它学会沿着报错 stack 分析
- 某些文风、风险偏好对齐等等

如何训练
- loss. SFT 时，一般会 mask 掉 prompt 部分的 loss，只学习输出部分的 loss
- 学习率. 一般是预训练的学习率的 0.1 倍
- epoch. 小样本（<1万）5个，大样本（>1万）2个
- 混入 30%-50% 高质量预训练数据（通用数据），防止灾难性遗忘，保持泛化能力

关键指标
- 业务合格率：从业务出发，人工对结果打标，给出 good/ok/bad，并计算指标
- 提升率：指标比上一个版本提升了多少


LoRA：
- 核心原理是矩阵分解 $\Delta w_{d\times k} = B_{d\times r} A_{r\times k}$，当 r 较小时，A 和 B 的参数量就很小
- 需要调整的参数少
- 还有利于多卡并行训练
- 因此很快
- 最终效果略低于全量微调


一个专门解数学题的7B模型训练步骤：
- step1: 用 Math-500（开源数据集）做 SFT
- step2: 现价一个难度过滤器，保证每次训练正好是有些难度的
- step3: 蒸馏超大模型的 CoT
- step4: 改善 CoT，这个例子的实验表示，CoT 要简繁适中、给出多个思路。





## GROP

区别：
- SFT：用大模型准备数据，给小模型来训练
- RL：问题描述（Initial State 无Output），只需要足够多样的问题，无需答案

为什么：“SFT负责记忆，RL负责泛化”
- SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training，https://arxiv.org/abs/2501.17161


大模型相关的 RL 发展历史：
1. PPO：用了4个模型：Actor Model（大模型本身），Ref Model（冻结的模型），Reward Model，Critic Model（对每个状态的评估，预测未来的潜在回报）
2. DPO：用一些数学技巧，省去了 Critic Model、Reward Model、Ref Model，只需要 Actor Model 和 accept/reject 。性能很好。Reward 很难写。
3. GROP，用组内相对结果来计算奖励，无需 Critic（节省内存）



GROP（Group Relative Policy Optimization）包含3个模型
- Actor Model：是训练过程中需要优化的核心策略模型，负责根据输入生成候选输出。通过生成多个输出（如数学题的多解方案）进行群体内对比。
- Reward Model：用于评估Actor生成输出的质量，为每个输出分配奖励值。通过群体归一化（如减去均值、除以标准差）将绝对奖励转换为相对优势，替代传统优势函数计算。
    - **这个部分也可以是个规则，而不是 Model**，例如：
    - 格式奖励（例如是否有think 和 answer，例如格式是否符合预期）
    - 结果正确性奖励。有些问题可以用规则判断
    - 其它奖励：
        - 语言一致性
        - 长度奖励
        - 重复性惩罚
        - 思考过程奖励，条理性、逻辑性
- Reference Model：是一个冻结（参数不更新）的基准模型，用于计算KL散度（Kullback-Leibler Divergence），约束Actor Model的策略更新幅度，防止其过度偏离初始策略。


GROP的算法流程
1. 分组采样响应。对同一个 Prompt 生成一组响应（4-8个）
2. 组内评分语排名。根据Reward Model（或规则），对每个响应评分，然后，相对优势=个体得分-组平均分
3. 策略优化方向。强化学习得分相对优势高的，抑制相对优势低的。


## 大模型评估



测评大模型（*A Survey on Evaluation of Language Models*）https://arxiv.org/abs/2307.03109
- 验证性能和能力
- 评估泛化能力：在未见过的数据上也保持良好
- 明确不同的大模型差异：了解其优势领域
- 防止大模型风险：安全漏洞之类的
- 知道大模型改进：识别模型弱点、提供优化方向

一些知识点
- 有时候，多一个换行符，可能对结果产生较大影响

测评方法
- 在有标准答案的数据集上评测：如何提取模型的答案
- 针对选择题，计算每个选项的 PPL
- 对抗测评：在另一个模型上得到回答，比较 A 回答 和 B 回答的好坏。

