---
layout: post
title: 【LLM】GPT 模型
categories:
tags: 0x23_深度学习
keywords:
description:
order: 250
---




## GPT-1 到 ChatGPT

Bert vs GPT
- 模型结构：
    - Bert：Encoder-only
    - GPT：Decoder-only
- 预训练
    - Bert 采用 MLM（Masked Language Model）模型知道过去和未来的词，预测中间的词
    - GPT 采用传统 LM 模式，只知道过去的词，预测未来的词
- 下游任务微调
    - Bert：多种任务微调，如文本分类、情感分析等
    - GPT：指定输入输出


**Scaling Law** LLaMA:即使是7B模型，训练数据继续扩展到 1T token，其性能仍然会上升




历史迭代
- GPT1: 预训练——微调
- GPT2: 引入 prompt，拥有 ZeroShot learning 能力
- GPT3: 有 FewShot learning 能力
- InstructGPT

ChatGPT
- 基于大规模预训练语言模型（GPT-3，一般也称作 base 模型）进一步训练
- 通过在人工标注和反馈的数据上进行指令对齐，从而有更好的对话能力（ChatGPT，一般也称为 chat 模型）


阶段一：SFT（supervised fine-tuning）
- 人工收集、标注训练样本，有监督地微调 GPT-3.5 模型
    - 问题来源：1）标注员 2）ChatGPT的用户
    - 答案（demonstration）有标注员填写，标准：有用、无害、真实
    - 此阶段 1.3万+ 样本量
- 通过有监督微调，赋予 GPT 模型能理解人类的复杂指令
    - Plain：标注人员构思问题和答案
    - Few-shot
    - User-based：根据用户提交的问题改写
    - API prompt：从 OpenAI的 API调用种抽取的 Prompt
阶段二：RM（Reward Model）
- 人工对模型输出候选结果进行排序，训练奖励模型
    - 同一个问题，模型输出多个回答，标注员对其排序
    - RLHF（Reinforcement Learning from Human Feedback）
- 奖励模型以人类的视角来衡量模型的表现

阶段三：PPO（Proximal Policy Optimization）
- 通过 RM，利用强化学习 PPO 算法对模型进一步训练
- 重复阶段二和阶段三，从而使得模型不断增强

## Prompt


好的问题才有好的答案（另一篇博客）




## RAG

Prompt 解决不了的问题：
- 缺乏知识：涉及最新的知识，或者特定专业知识，大模型没有学习过，所以表现不好
- 幻觉问题：大模型不了解自己的知识边界，对于欠缺知识的领域仍然尝试回答，错误的回答内容具有迷惑性


解决方案：**RAG**（Retrieval-Augmented Generation，检索增强生成）
- 本质上就是把搜索到的资料（例如企业信息、知识库等）作为提示词的一部分发给 LLM，让 LLM 有根据地输出内容，从而提高回答的准确性、相关性、新鲜度，并解决幻觉问题。
    - OpenAI：RAG 可以将回答准确率从 45% 提升到了 98%
- 相当于给大模型装上了“知识外挂”，基础大模型不用重新训练即可随时调用特定领域知识。省下了重新训练的成本。
- 技术实现上，涉及数据检索、信息增强、AI 生成等多个过程


RAG步骤
- 准备阶段
    - Load 阶段，把 json、pdf、html、图片等等，转化为 txt
    - Split 阶段，把文档切分为小块
    - Embed 阶段，把小块映射为一个向量
    - Store 阶段，把这些向量存下来
- 调用阶段
    - 检索片段，并加入到 Prompt
    - 大模型根据 Prompt 做出回答


加入到 Prompt 是这样的：

```text
"""
基于提供的【相关材料】，回答最后的问题
## 【相关材料】
{context}

## 最后的问题
问题：{input}
"""
```


## Agent

Agent 是包含了 RAG、提示词、Function calling、Re-Act等一系列元素的集合
- API 调用，例如查询天气、发送邮件
- Re-Act


## 模型评估


两种
- 封闭式问题：使用预先已经有的答案完成测评
- 开放式问题：
    - 人工测评
    - 大模型测评

## SFT

**两个思路**
- 大模型（如 DeepSeek-671B）在特定领域的知识转移到小模型如（Qwen-0.8B）。如此，可以在推理时获得双方的优点：小模型的QPS、小资源消耗，接近大模型准确率。
    - 方法和流程：大模型 -> 获得COT数据 -> 蒸馏小模型
- 大模型->专家经验（Ground truth）->蒸馏小模型
    - 方法和流程：专家用人工的方式提供答案/推理（可以借助大模型，或者不借助大模型）。成本较高。


**什么时候需要？**
- 如果是通识场景，不需要 SFT
- 如果场景比较冷僻，就需要微调

**有哪些方法**
- 全参数微调：更新所有参数，需要大量高质量领域数据
- 部分参数微调：只调整关键参数
    - LoRA
    - QLoRA

**base model 选择**
- 复杂SFT任务：选 7B 以上
- 高性能任务：选 1B 以下



### 合成数据

**方法1直接用大模型生成COT**。例如，直接用基座模型（例如 DeepSeek-R1/V3， ChatGPT）生成COT数据

**方法2设计多智能体** 例如，设计2个智能体。一个扮演客服专家，一个扮演客户
- 客服专家产生话术
- 客户提出意见，例如，“增加友好和关心的语气”、“这段话术稍显冗长”
- 客服专家根据意见做优化
- 数轮之后（3-5轮），得到优化后的话术。
- 用另一个 LLM 做一轮过滤，得到最终的数据集

### 高质量SFT训练数据

数据不需要太多：*Meta《LIMA》：1k优质样本即可打败海量普通数据*

因此，获取 **高质量训练数据** 是关键

**数据质量评估**
- **准确**
    - **无事实错误**
    - **一致性**：内部处理逻辑统一，数据之间没有互相矛盾
    - 方法和工具：
        - 人工质量抽检
        - rule-based：格式准确性等多重规则
        - LLM as judge
        - 构建静态指标：例如，输出长度/PPL等
            - **PPL** （困惑度，perplexity）的计算：预测下一个词时的“平均交叉熵”的指数。它大于1，并且越大代表越不确定。
            - **PPL** 过高和过低都要剔除。过低代表不必再学了，过高的也是学不会。
        - 借用开源 reward model/微调若干大模型来实现数据质量奖励/训练小模型
- **完整性** （覆盖任务要点）
    - 覆盖全部需要的 **任务类型**
    - **难度梯度** 简单/中等/复杂任务比例合理
    - 覆盖全部需要的 **文本主题** 
    - **数据洁净度**：剔除重复、噪声、错误的样本
    - 去除冗余数据
    - 正负样本均衡
    - 样本相关性，捕捉数据集的上下文和语义多样性。（生成的指令和其最相似的种子指令之间的ROUGE-L分数分布。）
    - kmeans/k-center（计算复杂度较低）， 挑选出多样性的数据，即Seed Instruction Data。这步关注的是 coverage。
- **简洁性**：（无冗余信息）


**业务知识如何融入**
- 专家的感性知识。典型历史案例 -> （大模型）抽取专家知识 -> 沉淀为业务知识
- 策略/正则表达式/规则 -> 翻译成人话 -> 沉淀为专家经验
- 业务专有名称 -> 整理出业务“词典”


如何训练
- loss. SFT 时，一般会 mask 掉 prompt 部分的 loss，只学习输出部分的 loss
- 学习率. 一般是预训练的学习率的 0.1 倍
- epoch. 小样本（<1万）5个，大样本（>1万）2个
- 混入 30%-50% 高质量预训练数据（通用数据），防止灾难性遗忘，保持泛化能力

关键指标
- 业务合格率：从业务出发，人工对结果打标，给出 good/ok/bad，并计算指标
- 提升率：指标比上一个版本提升了多少





## GROP

区别：
- SFT：用大模型准备数据，给小模型来训练
- RL：问题描述（Initial State 无Output），只需要足够多样的问题，无需答案


GROP（Group Relative Policy Optimization）包含3个模型
- Actor Model：是训练过程中需要优化的核心策略模型，负责根据输入生成候选输出。通过生成多个输出（如数学题的多解方案）进行群体内对比。
- Reward Model：用于评估Actor生成输出的质量，为每个输出分配奖励值。通过群体归一化（如减去均值、除以标准差）将绝对奖励转换为相对优势，替代传统优势函数计算。
    - **这个部分也可以是个规则，而不是 Model**，例如：
    - 格式奖励（例如是否有think 和 answer，例如格式是否符合预期）
    - 结果正确性奖励。有些问题可以用规则判断
    - 其它奖励：
        - 语言一致性
        - 长度奖励
        - 重复性惩罚
        - 思考过程奖励，条理性、逻辑性
- Reference Model：是一个冻结（参数不更新）的基准模型，用于计算KL散度（Kullback-Leibler Divergence），约束Actor Model的策略更新幅度，防止其过度偏离初始策略。


GROP的算法流程
1. 分组采样响应。对同一个 Prompt 生成一组响应（4-8个）
2. 组内评分语排名。根据Reward Model（或规则），对每个响应评分，然后，相对优势=个体得分-组平均分
3. 策略优化方向。强化学习得分相对优势高的，抑制相对优势低的。
