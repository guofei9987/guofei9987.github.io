---
layout: post
title: 【有监督降维】LDA
categories:
tags: 3-1-降维
keywords:
description:
order: 351
---


## 介绍
LDA(Linear Discrimination Analysis) 是一种有监督降维模型。  
区别于另一种LDA（Latent Dirichlet Allocation），这是一个关于NLP的模型。  

### 1. LDA vs PCA
相同点：
1. 两者均可以对数据进行降维。
2. 两者在降维时均使用了矩阵特征分解的思想。
3. 两者都假设数据符合高斯分布。都不适合非高斯分布样本。

不同点：
1. LDA是有监督的降维方法，而PCA是无监督的降维方法
2. LDA降维最多降到类别数k-1的维数，而PCA没有这个限制。
3. LDA除了可以用于降维，还可以用于分类。
4. LDA选择分类性能最好的投影方向，而PCA选择样本点投影具有最大方差的方向。
5. LDA在样本分类信息依赖均值而不是方差的时候，比PCA之类的算法较优。LDA在样本分类信息依赖方差而不是均值的时候，降维效果不好。
LDA可能过度拟合数据。

LDA算法既可以用来降维，又可以用来分类，但是目前来说，主要还是用于降维。在我们进行图像识别图像识别相关的数据分析时，LDA是一个有力的工具

## 原理
$x \to w^Tx$ 向低维空间投影时，我们希望尽量让类间的距离尽量大（类间的距离用各类的中心点的距离来定义），同时希望让类内的距离尽量小  


假设共有N个类，第$i$个类有$m_i$个样本，这个类的样本集合是$X_i$，这个类样本的均值是$u_i$  

定义 **类间散度矩阵** $S_b=\sum\limits_{i=1}^N m_i(u_i-u)(u_i-u)^T$  
定义 **每个类的散度矩阵** $S_{w_i}=\sum\limits_{x\in X_i}(x-u_i)(x-u_i)^T$  
定义 **类内散度矩阵** $S_w=\sum\limits_{i=1}^N S_{w_i}$  

我们希望 $w^T S_b w$ 尽量小，同时 $w^T S_b w$ 尽量大（这两个都是矩阵）  
常见的一个目标函数$\arg\min\limits_w J(w)=\dfrac{tr(w^T S_b w)}{tr(w^T S_w w)}$

## 算法流程
（略）
## 实现
```py
# 载入数据
import sklearn.datasets as datasets

dataset = datasets.load_iris()
X, Y = dataset.data, dataset.target

# 构造模型
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
lda = LinearDiscriminantAnalysis()
lda.fit(X, Y) # 训练
lda.transform(X) # 输出降维后的数据 X-1维
lda.fit_transform(X,Y) # fit+transform

lda.predict(X) # LDA可以用来做预测
lda.predict_proba(X) # 也能计算概率
lda.score(X,Y)
```

## 参考文献
周志华：《机器学习》  
http://www.cnblogs.com/pinard/p/6244265.html
