---
layout: post
title: 【PCA】理论与实现
categories:
tags: 3-1-降维
keywords:
description:
order: 320
---


## 降维

### 降维的必要性

1. 多重共线性--预测变量之间相互关联。多重共线性会导致解空间的不稳定，从而可能导致结果的不连贯。
2. 高维空间本身具有稀疏性。一维正态分布有68%的值落于正负标准差之间，而在十维空间上只有0.02%。
3. 过多的变量会妨碍查找规律的建立。
4. 仅在变量层面上分析可能会忽略变量之间的潜在联系。例如几个预测变量可能落入仅反映数据某一方面特征的一个组内。
5. **维度灾难** 指的是维度很高的情况下，模型更有可能通过检验，但模型的预测能力实际上会变得很弱，多元回归就是典型代表。  


### 降维的目标

1. 减少预测变量的个数
2. 确保这些变量是相互独立的
3. 提供一个框架来解释结果


## PCA介绍


PCA（Principal Component Analysis）不仅仅是对高维数据进行降维，更重要的是经过降维去除了噪声，发现了数据中的模式。

PCA把原先的n个特征用数目更少的m个特征取代，新特征是旧特征的线性组合，这些线性组合最大化样本方差，尽量使新的m个特征互不相关。从旧特征到新特征的映射捕获数据中的固有变异性。  

PCA在 **有噪声** 或者存在 **多重相关性** 的场景下很好用。

### 数学表达

原始数据是这样的：  

|原始数据|X1|X2|...|Xp|
|--|--|--|--|--|
|sample1|XX|XX|...|XX|
|sample2|XX|XX|...|XX|
|sample3|XX|XX|...|XX|
|...|...|...|...|...|


我们想要的计算这样的结果：

|原始数据+PCA|X1|X2|...|Xp|Y_1|Y_2|...|Y_p|
|--|--|--|--|--|--|--|--|--|
|sample1|XX|XX|...|XX|?|?|?|?|
|sample2|XX|XX|...|XX|?|?|?|?|
|sample3|XX|XX|...|XX|?|?|?|?|
|...|...|...|...|...|
|sample n|XX|XX|...|XX|?|?|?|?|

如果 X1 和 X2 这两列完全相同，那么其中一列是不必要存在的。同样道理如果 $X_1, ..., X_p$ 有多重共线性，那么仅使用 $Y_i$ 的前几个，就可以代表 X
 的 p 维数据了。

## 模型的数学形式

我们想做这件事：
$$\left \{ \begin{array}{ccc}
Y_1=l_{11}X_1+l_{12}X_2+...+l_{1p}Xp\\
Y_2=l_{21}X_1+l_{22}X_2+...+l_{2p}Xp\\
...\\
Y_p=l_{p1}X_1+l_{p2}X_2+...+l_{pp}Xp
\end{array}\right.$$  


其中，
$$\left \{ \begin{array}{ccc}
Var(Y_1) \geq Var(Y_2) \geq ...Var(Y_p) \geq 0\\
COV(Y_i,Y_j)=0&i \neq j\\
l_{i1}^2+l_{i2}^2+...+l_{ip}^2=1&i=1,2,...p
\end{array}\right.$$  


解释一下上面的限制条件：  
1. 这是为了提取后的因子重要程度排序：$Var(Y_1) \geq Var(Y_2) \geq ...Var(Y_p) \geq 0$
2. 这是为了提取后的因子不再有相关性：$COV(Y_i,Y_j)=0$
3. 为了保持信息不丢失


### 模型的矩阵形式
以矩阵形式表示：  

$X=(X_1,X_2,...X_p)'$  
$Y=T'X$  

## 模型的解

这里先写出模型的解，然后写出这个解的证明过程  
$COV(X)$是协方差矩阵，这是一个p×p矩阵，而且是一个实对称阵，  
**对于p×p实对称矩阵，存在p个正交的特征向量**。  
按照特征值$\lambda_i$从大到小排序，对应特征向量记为$e_1,e_2,...e_p$(列向量)  

模型如下：  
$$\left ( \begin{array}{ccc}
Y_1\\Y_2\\...\\Y_p
\end{array}\right )=\left ( \begin{array}{ccc}
e_1'\\e_2'\\...\\e_p'
\end{array}\right )\left ( \begin{array}{ccc}
X_1\\X_2\\...\\X_p
\end{array}\right )$$  


### 解的证明

引理：（**很重要**，后面反复用到）  
$COV(Y_i,Y_j)=COV(e_i'X,e_j'X)=e_i' COV(X) e_j$  

因为$COV(X)$是实对称矩阵，$e_i,e_j(i\neq j)$是特征向量并且正交，所以：

$$COV(Y_i,Y_j)=\left \{ \begin{array}{ccc}
0&i\neq j\\
\lambda_i& i=j&
\end{array}\right.$$  


(证明完毕)  

### 解的性质

#### 性质1

$Cov(Y)$是对角阵，对角线上的元素是$Cov(x)$的特征值

#### 性质2  

$\sum Var(Y_i)=\sum Var(X_i)$  

#### 性质3

$Cov(X_i,Y_j)=Cov(E_iX_i,e_j'X)=E_i Cov(X) e_j=\lambda_i e_{ij}$  
$\rho(X_i,Y_j)=\dfrac{Cov(X_i,Y_j)}{\sqrt{DX_i DY_j}}=\dfrac{\lambda_i e_{ij}}{\sqrt{\lambda_i \sigma_{ii}}}$  
这个相关系数叫做 **因子载荷量**  

#### 贡献率

$\lambda_k/\sum \lambda$叫做 **贡献率**   

贡献率的cumsum叫做 **累计贡献率**  
## Python实现

大图见于<a href='https://www.guofei.site/StatisticsBlog/PCA.html' target="HypothesisTesting">这里</a>  


<iframe src="https://www.guofei.site/StatisticsBlog/PCA.html" width="100%" height="3600em" marginwidth="10%"></iframe>

大图见于<a href='https://www.guofei.site/StatisticsBlog/PCA.html' target="HypothesisTesting">这里</a>  


## 更一般化的推导

我们希望找到一种降维的编码方案，这种方案记为函数 $f$，对应一个还原方案，记为函数 $g$  
- 这套编码方案首先要合理，也就是 $x \approx g(f(x))$
- 进一步，希望找到一组编码 $c=f(x)$，使得 $x$ 和 $g(c)$ 尽量接近
- 我们用 L2 范数来定义“接近”，问题转化为一个最优化问题 $c^\star =\argmin\limits_c \mid\mid x-g(c) \mid\mid_2$


继续添加一些前提条件：
- 把映射限定为线性映射，即 $g(c)=Dc$
- 最优化有无穷多解，因为按比例扩大D，都可以满足条件。所以继续限定 D 是正交的。


接下来就是推导过程，只写思路
- 把最优化问题化简
- 用迹的一些定理转化一下。
- 优化问题最终转化到特征分解。
